<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>multimodal-neural-translation  3</title>
      <link href="/2019/09/19/multimodal-neural-translation3/"/>
      <url>/2019/09/19/multimodal-neural-translation3/</url>
      
        <content type="html"><![CDATA[<hr><h2 id="本文主要介绍多模态神经翻译的几个任务："><a href="#本文主要介绍多模态神经翻译的几个任务：" class="headerlink" title="本文主要介绍多模态神经翻译的几个任务："></a>本文主要介绍<strong>多模态神经翻译</strong>的几个任务：</h2><h2 id="Image-pivoted-Zero-resource-Translation-Bilingual-lexicon-induction"><a href="#Image-pivoted-Zero-resource-Translation-Bilingual-lexicon-induction" class="headerlink" title="Image-pivoted Zero-resource Translation (Bilingual lexicon induction)"></a>Image-pivoted Zero-resource Translation (Bilingual lexicon induction)</h2><h3 id="Unsupervised-Bilingual-Lexicon-Induction-from-Mono-lingual-Multimodal-Data"><a href="#Unsupervised-Bilingual-Lexicon-Induction-from-Mono-lingual-Multimodal-Data" class="headerlink" title="Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data"></a>Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data</h3><p><strong>双语词汇归纳</strong>是一项长期存在的自然语言处理任务。近年来的研究表明，在不依赖<strong>平行语料库</strong>的情况下，以图像为中心学习词汇归纳是一种很有前景的方法。然而，这些基于视觉的方法只是简单地将单词与整个图像关联起来，这就限制了翻译具体的单词，并且需要以对象为中心的图像。</p><p>当一个单词出现在一个句子的上下文中时，我们人类可以很好地理解单词。因此，在本文中，我们建议利用图像及其相关的字幕(caption)来解决以前方法的局限性。提出了一种使用不同<strong>单语多模态数据</strong>训练的<strong>多语多模态数据</strong>字幕模型，将不同语言的单词映射到联合空间。从多语言描述模型中归纳出两种类型的词表示:<code>语言特征</code>和<code>局部视觉特征</code>。<code>语言特征</code>是在具有视觉语义约束的句子语境中习得的，这有利于学习与视觉无关的词的翻译。<code>局部化的视觉特征</code>(localized visual feature)关注图像中与词相关的区域，从而减轻了图像对突出视觉表征(一张图片中只有一个单词表示的物体)的限制。</p><h3 id="基于图片的零样本翻译任务的研究意义"><a href="#基于图片的零样本翻译任务的研究意义" class="headerlink" title="基于图片的零样本翻译任务的研究意义"></a>基于图片的零样本翻译任务的研究意义</h3><p>现如今，机器翻译任务已经深入渗透在我们的日常生活中，“百度翻译”，“谷歌翻译”等API为我们的生活带来了许多便利条件。而这些表现优异的机器翻译模型背后，离不开海量的成对双语语料来支撑模型的训练。但是<font color="red">小语种的成对双语语料是十分匮乏的。</font>在这种情况下，无监督翻译模型的设计显得尤为重要。</p><p>那么，在没有双语对照关系的数据储备下，机器该如何学到翻译知识呢？回想一下我们还是孩童时，是如何学习外语的。我想大部分人通常都躲不过看图识字这一阶段。即便我们不会英语，看到一张苹果的图片，并配上“apple”，我们也会知道这个单词是“苹果”的含义。因为我们默认有着这样的认知——描述同一幅图片的词语或句子往往表达着相似的语义含义，因此我们可以使用图片这样的多媒体资源作为媒介，学习到两种语言的对应关系。人是如此，机器亦然。我们可以将图片作为媒介，<strong>利用图片对应起两种语言的单词或句子，从而完成词语，甚至是句子的翻译任务</strong>。</p><p>但是这种做法会存在两点缺陷：</p><ul><li>仅根据图片翻译词语，翻译模型只会对具有视觉意义的词语有很好的翻译能力，如<code>名词</code>，<code>形容词</code>，而对于图片无法明确表示的词语，翻译质量则会很差。</li><li>这种方法只能使用包含单一目标词语的图片，而现实生活中的图片往往都是包含众多物体的复杂图片。</li></ul><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>针对上述的两点缺陷，我们分别设计了两种词语特征表示——<code>语言特征表示</code>和<code>局部视觉特征</code>表示，用于之后的对应词语匹配。这两种特征表示的抽取均通过一个“代理”任务——多语言图片描述生成来实现。</p><ul><li><p>使用<code>源语言</code>和<code>目标语言</code>的（图片，内容描述）数据训练一个双语图片描述生成器，通过共享同一个<font color="red">图片编码器和描述生成器`</font>来迫使不同语言的词语被嵌入到同一个隐含空间内，含义相近的词语特征向量距离更近，而含义相远的词语特征向量距离更远。得到的这一特征就是词语的语言特征表示。在这种情况下，模型学习到的词语特征并不限制与图片内容一一对应，而是可以从其共存的句子环境中学到所有词语的语言信息，其中就包含“a”这类非视觉性词语。</p></li><li><p>为了不限制图片只包含单一物体，我们设计将图片的部分区域与词语相对应，而不是拿整张图片与词语对应。这就是词语的局部视觉特征表示。将图片通过卷积神经网络得到的以空间区域划分的图片特征依次输入到图片描述生成模型中，<strong>用生成相应词语的概率表示该词语与这一图片局部区域的匹配程度</strong>。根据匹配程度对这些区域图片特征做加权和，便得到了这一词语的一个局部视觉特征。当然，同一个词语所出现的图片有很多个。因此，最终每个词语都会对应一个局部视觉特征集合。</p></li><li><p>在获得这两种词语的特征表示（语言表示+局部视觉表示）后，便可以进行两种语言之间的词语翻译。将两个词语的语言特征向量的余弦相似度和局部视觉特征向量的余弦相似度之和作为这两个词语是否符合同一语义的分值，再以词汇表中与待翻译词匹配分值最高的词语作为翻译后的单词。</p><center><img src="/images/comparsion.png"></center>实验证明这两种特征的结合，可以有效地彼此互补，从而明显地提升词语翻译的准确度。</li></ul><h3 id="From-Words-to-Sentences-A-Progressive-Learning-Approach-for-Zero-resource-Machine-Translation-with-Visual-Pivots"><a href="#From-Words-to-Sentences-A-Progressive-Learning-Approach-for-Zero-resource-Machine-Translation-with-Visual-Pivots" class="headerlink" title="From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots"></a>From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots</h3><p>上一篇论文仅仅专注于词语翻译，那么这篇论文则专注于更复杂也更有研究需求的句子翻译。这篇文章的巧妙点在于利用预训练的多语言图片描述生成模型生成双语的图片内容描述，以此来作为成对数据训练机器翻译模型，从而解决数据匮乏的问题。当然，由于图片描述生成模型并不是完美的，而且“一图胜千言”，描述同一张图片的句子可能是很多样的。这样生成的训练数据往往存在很多噪声。因此，这篇论文着重研究并提出了两个降噪方案，来提升无监督机器翻译的质量。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>神经机器翻译模型<strong>缺乏大规模的并行语料库</strong>。相反，我们人类可以学习多语言翻译，即使没有平行文本，通过把我们的语言引用到外部世界。为了模拟人类的学习行为，我们以图像为中心，实现了零资源翻译学习。然而，<strong>一图胜千言</strong>，描述同一张图片的句子可能是很多样的，这样生成的训练数据往往存在很多噪声，从而阻碍了翻译模型的学习。<font color="green">However, since captions related to images are not necessarily good mutual translations, such learning approaches also suffer from noisy rewards.</font>与图像相关的字幕不一定是很好的相互翻译。</p><p>在这项工作中，我们为基于图像的零资源机器翻译提出了一个渐进式的学习方法。由于基于图像的词的多样性较低，我们首先学习基于图像的词级翻译，然后利用学习得到的词级翻译来抑制以图像为中心的多语言句子中的噪声，进而学习句子级翻译。</p><h2 id="第一种降噪方法"><a href="#第一种降噪方法" class="headerlink" title="第一种降噪方法"></a>第一种降噪方法</h2><p>第一种是对有噪声的句子根据其噪声程度赋予不同的权重，使得有噪声的数据对翻译模型训练优化的影响较小，优质的训练数据对翻译模型优化的影响更大。主要做法是计算源语言和目标语言句子的最小EMD（Earth Mover’s Distance）距离。其中，词与词之间的距离用多语言图片描述生成模型中计算得到的词语嵌入特征向量的余弦距离来表示。由于这一预训练好的图片描述生成模型针对不同的语言共享图片编码器和描述生成器，因此不同语言的词语会被嵌入到同一个隐含空间中。在这一空间中，语义相近的词语的特征向量更近。根据句子层级和词语层级的EMD距离，便可以对损失函数中每个训练样例及每个样例中的词语的贡献度加以权重，从而减轻噪声对NMT模型性能的影响。</p><h2 id="第二种降噪方法"><a href="#第二种降噪方法" class="headerlink" title="第二种降噪方法"></a>第二种降噪方法</h2><p>第二种降噪方法是让NMT模型对“源语言的句子”进行重建。这里的“源语言的句子”实际是经过噪声化的句子。我们对句子随机进行词语调位、词语增添，词语丢弃等操作，训练NMT模型重建出原来完好的句子。这一自我编码解码的方式可以提高模型的鲁棒性。</p><p>最终的实验结果表明，通过以上两种降噪方式，可以很大程度上提高无监督机器翻译的质量。</p><p><img src="/images/multilingual.png" alt="img"></p><h1 id="Language-Pivoted-zero-source-caption"><a href="#Language-Pivoted-zero-source-caption" class="headerlink" title="Language Pivoted zero-source caption"></a>Language Pivoted zero-source caption</h1><p>既然图片等视觉信息可以对无监督的机器翻译任务提供很大帮助，那么反过来，机器翻译任务是否可以为计算机视觉相关任务，如图片描述自动生成带来帮助呢？答案是肯定的。</p><p>图片描述生成任务是对于给定的一张图片，自动生成描述图片内容的自然语言描述。这一任务从提出至今已有较长的研究历史，然而大部分工作都只专注于英文图片描述生成模型的研究。为了满足世界各地人们的需要，尤其是非母语为英语的人们，训练其他语种的图片描述生成模型十分必要。然而，由于用来训练图片描述生成模型的训练数据全部需要人工标注，为每一种语言，尤其是小语种标注大量的（图片，内容描述）数据几乎是不可能的。因此，我们希望可以利用已有的图片描述数据集来训练任一语言的图片描述生成模型，这也被称作跨语言图片描述生成。那么，如何在只了解一种语言的图片描述知识的情况下，自动为图片生成另一种语言的内容描述呢？或许机器翻译可以帮这个忙。</p><p>2018年，Jiuxiang Gu等人提出先在数据上训练这种语言的图片描述生成模型，随后通过机器翻译模型将所生成的描述翻译到目标语言。然而，由于机器翻译模型并不是完美的，在翻译的过程中，种种翻译错误严重影响了最终生成的描述质量。我们在Unpaired Cross-lingual Image Caption Generation with Self-Supervised Rewards这篇工作中总结了两种翻译错误，分别是语言表达不通顺和语义翻译不正确。前者会导致翻译得到的目标语言图片描述存在语法错误，后者则可能出现句子所描述的内容与图片不符的情况。针对这两种错误，我们设计了两种奖励函数来解决问题。下面，我们来详细了解这篇文章。</p><h3 id="Unpaired-Cross-lingual-Image-Caption-Generation-with-Self-Supervised-Rewards"><a href="#Unpaired-Cross-lingual-Image-Caption-Generation-with-Self-Supervised-Rewards" class="headerlink" title="Unpaired Cross-lingual Image Caption Generation with Self-Supervised Rewards"></a>Unpaired Cross-lingual Image Caption Generation with Self-Supervised Rewards</h3><p>这篇文章利用机器翻译对跨语言的图片描述生成任务进行了探索。讨论如何在某种语言的图片描述数据匮乏的情况下，训练该语言的图片描述生成模型。这对于除英语外的其他语种的图片描述生成模型的训练提供了思路。</p><p>我们以语言作为支点，使用资源充沛的英文数据来训练其他语言的图片描述生成模型。首先，通过机器翻译将英文的训练数据翻译到目标语言，随后以（图片，翻译得到的目标语言内容描述）作为训练数据来训练目标语言的图片描述生成模型。由于机器翻译模型并不完美，种种翻译错误使得这样的训练数据中存在很多噪声。我们分析总结了两种错误，分别是语言表达不通顺和语义翻译不正确。<br><img src="/images/image-pivot.png" alt="lauage-pivot"></p><h2 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h2><p>生成不同语言的图像描述对于满足全球用户的需求是至关重要的。然而，为每一种目标语言收集大规模的成对图像标题数据集是非常昂贵的，这对于训练下降图像描述模型至关重要。以往的工作都是通过一个主语言，借助主语言中的成对图像标题数据和pivot-to-target机器翻译模型，来解决跨语言图像标题的非成对问题。然而，这种以语言为中心的翻译方法存在着由中心到目标(pivot-to-target)的翻译带来的不准确性，包括不流畅性和视觉无关性错误。在本文中，我们提出在强化学习框架中生成具有自监督奖励的跨语言图像描述，以减少这两类的错误。我们利用目标语单语语料库的自监督来提供流畅性奖励，并提出了一个多层次的视觉语义匹配模型来提供句子级和概念级的视觉关联奖励。<br><img src="/images/ssr_model.png" alt="rl"></p><h1 id="Multimodal-Enhanced-Translation"><a href="#Multimodal-Enhanced-Translation" class="headerlink" title="Multimodal Enhanced Translation"></a>Multimodal Enhanced Translation</h1><p>以上两个任务证明了跨模态信息对于数据匮乏情况下的无监督学习任务可以给予更多的指导信息，对无监督模型的训练效果有很大提升。那么，在数据充裕的情况下，如存在大量成对数据的机器翻译，跨模态信息的加入是否还能进一步提升模型质量呢？以下两篇论文就对这一问题进行了探索。</p><h2 id="VATEX-A-Large-Scale-High-Quality-Multilingual-Dataset-for-Video-and-Language-Research"><a href="#VATEX-A-Large-Scale-High-Quality-Multilingual-Dataset-for-Video-and-Language-Research" class="headerlink" title="VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"></a>VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research</h2><p>这篇论文通过收集并标注视频和中英文的视频内容描述数据，证明了在存在成对翻译数据的情况下，引入跨模态信息（这里是视频），会进一步提升机器翻译模型的质量。其中，跨模态信息的加入主要从以下两个方面对机器翻译带来更多增益。</p><ul><li><p>消除歧义。我们都知道“一词多义”，在句子上下文信息不充裕的情况下，我们可能无法确定句子中某些词语的含义，自然更不能准确翻译到另一种语言。在这种情况下，视频信息便可以帮助消除语言上的歧义。</p><center><img src="/images/vatex.png"></center></li><li><p>减小噪声。 在训练数据存在少量噪声时，跨模态信息的加入也会提供更多信息，增强模型的鲁棒性。</p></li></ul><p>NAACL 2019的最佳短文<a href="#Bibtex">this</a>就着重对减少噪声方面跨模态信息的作用进行了实验和探索。它通过设计三种弱化源语言句子信息完整度的方法，探索跨模态信息对机器翻译模型的帮助会有多大。这三种加噪方式分别是隐藏句子中表示颜色的词语、隐藏实体词、逐级隐藏句子中的后$k$个词。实验证明，任一种方式下，多模态机器翻译模型都要明显优于纯文本的机器翻译模型，而且信息损失越严重，跨模态信息的作用越明显。这说明在训练数据复杂，存在噪声的情况下，跨模态信息的确会对机器翻译模型带来更多助益。</p><h1 id="Bibtex"><a href="#Bibtex" class="headerlink" title="Bibtex"></a>Bibtex</h1><pre><code>@article{caglayan2019probing,  title={Probing the Need for Visual Context in Multimodal Machine Translation},  author={Caglayan, Ozan and Madhyastha, Pranava and Specia, Lucia and Barrault, Lo{\&quot;\i}c},  journal={arXiv preprint arXiv:1903.08678},  year={2019}}</code></pre>]]></content>
      
      
      <categories>
          
          <category> multimodal learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> multimodal learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dataset pipeline</title>
      <link href="/2019/09/18/dataset-pipeline/"/>
      <url>/2019/09/18/dataset-pipeline/</url>
      
        <content type="html"><![CDATA[<h1 id="本教程旨在："><a href="#本教程旨在：" class="headerlink" title="本教程旨在："></a>本教程旨在：</h1><ul><li>说明 TensorFlow 输入流水线本质上是一个 <code>ETL</code> 流程。</li><li>介绍围绕 <code>tf.data</code> API 的常见性能优化。</li><li>讨论转换的应用顺序对性能的影响。</li><li>总结设计高性能 <code>TensorFlow</code> 输入流水线的最佳做法。</li></ul><h1 id="输入流水线结构"><a href="#输入流水线结构" class="headerlink" title="输入流水线结构"></a>输入流水线结构</h1><p>我们可以将典型的 TensorFlow 训练输入流水线视为 ETL 流程：</p><ul><li><strong>提取</strong>：从永久性存储（可以是 HDD 或 SSD 等本地存储或 GCS 或 HDFS 等远程存储）读取数据。</li><li><strong>转换</strong>：使用 CPU 核心解析数据并对其执行预处理操作，例如图像解压缩、数据增强转换（例如随机裁剪、翻转和颜色失真）、重排和批处理。</li><li><strong>加载</strong>：将转换后的数据加载到执行机器学习模型的加速器设备（例如，GPU 或 TPU）上。<br>这种模式可高效利用 CPU，同时预留加速器来完成对模型进行训练的繁重工作。此外，将输入流水线视为 <code>ETL</code> 流程可提供便于应用性能优化的结构。</li></ul><p>使用 <code>tf.estimator.Estimator</code>API时，前两个阶段（提取和转换）是在 <code>input_fn</code>(传递给 <code>tf.estimator.Estimator.train</code>) 中捕获的。代码可能如下（简单序列）实现所示：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">parse_fn</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token string">"Parse TFExample records and perform simple data augmentation."</span>  example_fmt <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">"image"</span><span class="token punctuation">:</span> tf<span class="token punctuation">.</span>FixedLengthFeature<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>string<span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token string">"label"</span><span class="token punctuation">:</span> tf<span class="token punctuation">.</span>FixedLengthFeature<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>int64<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span>  parsed <span class="token operator">=</span> tf<span class="token punctuation">.</span>parse_single_example<span class="token punctuation">(</span>example<span class="token punctuation">,</span> example_fmt<span class="token punctuation">)</span>  image <span class="token operator">=</span> tf<span class="token punctuation">.</span>image<span class="token punctuation">.</span>decode_image<span class="token punctuation">(</span>parsed<span class="token punctuation">[</span><span class="token string">"image"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  image <span class="token operator">=</span> _augment_helper<span class="token punctuation">(</span>image<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># augments image using slice, reshape, resize_bilinear</span>  <span class="token keyword">return</span> image<span class="token punctuation">,</span> parsed<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token keyword">def</span> <span class="token function">input_fn</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  files <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>list_files<span class="token punctuation">(</span><span class="token string">"/path/to/dataset/train-*.tfrecord"</span><span class="token punctuation">)</span>  dataset <span class="token operator">=</span> files<span class="token punctuation">.</span>interleave<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>TFRecordDataset<span class="token punctuation">)</span>  dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>buffer_size<span class="token operator">=</span>FLAGS<span class="token punctuation">.</span>shuffle_buffer_size<span class="token punctuation">)</span>  dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span>map_func<span class="token operator">=</span>parse_fn<span class="token punctuation">)</span>  dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span>batch_size<span class="token operator">=</span>FLAGS<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>  <span class="token keyword">return</span> dataset</code></pre><p>下一部分以此输入流水线为基础，并添加了性能优化。</p><h1 id="优化性能"><a href="#优化性能" class="headerlink" title="优化性能"></a>优化性能</h1><p>由于新型计算设备（例如 GPU 和 TPU）可以不断提高神经网络的训练速度，因此，CPU 处理很容易成为瓶颈。tf.data API 为用户提供构建块来设计可高效利用 CPU 的输入流水线，并优化 ETL 流程的每个步骤。</p><h2 id="流水线"><a href="#流水线" class="headerlink" title="流水线"></a>流水线</h2><p>要执行训练步骤，您必须首先提取并转换训练数据，然后将其提供给在加速器上运行的模型。但是，在一个简单的同步实现中，当 CPU 正在准备数据时，加速器处于空闲状态。相反，当加速器正在训练模型时，CPU 处于空闲状态。因此，训练步的用时是 CPU 预处理时间和加速器训练时间的总和。</p><p>流水线将训练步骤的预处理和模型执行过程重叠到一起。当加速器正在执行第 N 个训练步时，CPU 正在准备第 N+1 步的数据。这样做不仅可以最大限度地缩短训练的单步用时（而不是总用时），而且可以缩短提取和转换数据所需的时间</p><p>如果不使用流水线，CPU 和 GPU/TPU 在大部分时间都处于空闲状态：<br><img src="img/datasets_without_pipelining.png" alt="dataset_withoutpipeline"></p><p>使用流水线可以显著减少空闲时间：</p><p><img src="img/datasets_with_pipelining.png" alt="datasets_with_pipelining"></p><p><code>tf.data</code> API 通过 <code>tf.data.Dataset.prefetch</code> 转换提供了一种软件流水线机制，该机制可用于将生成数据的时间和使用数据的时间分离开。具体而言，该转换使用后台线程和内部缓冲区，以便在请求元素之前从输入数据集中预取这些元素。因此，为了实现上图所示的流水线效果，您可以将 <code>prefetch(1)</code> 作为最终转换添加到数据集流水线中（如果单步训练使用 n 个元素，则添加 <code>prefetch(n)</code>）。</p><p>要将此项更改应用于我们正在运行的示例，请将：</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span>batch_size<span class="token operator">=</span>FLAGS<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span><span class="token keyword">return</span> dataset</code></pre><p>更改为：</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span>batch_size<span class="token operator">=</span>FLAGS<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>prefetch<span class="token punctuation">(</span>buffer_size<span class="token operator">=</span>FLAGS<span class="token punctuation">.</span>prefetch_buffer_size<span class="token punctuation">)</span><span class="token keyword">return</span> dataset</code></pre><p>请注意，只要可以将“提供方”的工作与“使用方”的工作重叠，预取转换就会发挥作用。</p><h1 id="并行处理数据提取"><a href="#并行处理数据提取" class="headerlink" title="并行处理数据提取"></a>并行处理数据提取</h1><p>在实际设置中，输入数据可能会远程存储（例如，GCS 或 HDFS），这是因为输入数据不适合本地存储，或因为训练是分布式训练，因此在每台机器上复制输入数据没有意义。非常适合在本地读取数据的数据集流水线在远程读取数据时可能会遇到 I/O 瓶颈，这是因为本地存储和远程存储之间存在以下差异：</p><ul><li><strong>首字节时间</strong>：与本地存储相比，从远程存储读取文件的首字节所用时间可能要多出几个数量级。</li><li><strong>读取吞吐量</strong>：虽然远程存储可提供较大的聚合宽带，但读取单个文件可能只能利用此宽带的一小部分。</li></ul><p>此外，将原始字节读入内存中后，可能还需要对数据进行反序列化或解密（例如，protobuf），这会带来额外的开销。无论数据是在本地还是远程存储，都存在这种开销，但如果未有效预取数据，则在远程存储的情况下可能更糟。</p><p>为了降低各种数据提取开销的影响，<code>tf.data</code> API 提供了 <code>tf.contrib.data.parallel_interleave</code> 转换。使用此转换可以并行执行其他数据集（例如数据文件读取器）并交错这些数据集的内容。可以通过 <code>cycle_length</code> 参数指定要重叠的数据集的数量。</p><p>下图说明了为 parallel_interleave 转换提供 cycle_length=2 的效果：<br><img src="img/datasets_parallel_io.png" alt="parallel_interleave"></p><p>如将</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> files<span class="token punctuation">.</span>interleave<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>TFRecordDataset<span class="token punctuation">)</span></code></pre><p>改为：</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> files<span class="token punctuation">.</span>apply<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>data<span class="token punctuation">.</span>parallel_interleave<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>TFRecordDataset<span class="token punctuation">,</span>cycle_length<span class="token operator">=</span>FLAGS<span class="token punctuation">.</span>num_parallel_readers<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>由于负载或网络事件，远程存储系统的吞吐量可能会随时间而变化。鉴于这种差异，<code>parallel_interleave</code> 转换可以选择使用预取（如需了解详情，请参阅 <code>tf.contrib.data.parallel_interleave</code>）。</p><p>默认情况下，<code>parallel_interleave</code> 转换可提供元素的确定性排序以帮助实现可再现性。作为预取的替代方案（在某些情况下可能效率低下），<code>parallel_interleave</code> 转换还提供了一个可提升性能但无法保证排序的选项。特别是，如果 <code>sloppy</code> 参数设为 <code>true</code>，则该转换可在系统请求下一个元素时暂时跳过其元素不可用的文件，从而放弃该转换的确定性排序</p><h1 id="并行处理数据转换"><a href="#并行处理数据转换" class="headerlink" title="并行处理数据转换"></a>并行处理数据转换</h1><p>准备批次数据时，可能需要预处理输入元素。为此，<code>tf.data</code> API 提供了 <code>tf.data.Dataset.map</code> 转换，以将用户定义的函数（例如，正在运行的示例的 <code>parse_fn</code>）应用于输入数据集的每个元素。由于输入元素彼此独立，因此可以跨多个 CPU 核心并行执行预处理。为实现这一点，<code>map</code> 转换提供了 <code>num_parallel_calls</code> 参数来指定并行处理级别。例如，下图说明了将 <code>num_parallel_calls=2</code> 设置为 <code>map</code> 转换的效果：</p><p>如何为 <code>num_parallel_calls</code> 参数选择最佳值取决于硬件、训练数据的特征（例如其大小和形状）、映射函数的成本以及同时在 CPU 上进行的其他处理；一个简单的启发法是<code>设为可用 CPU 核心的数量</code>。例如，如果执行以上示例的机器有 4 个核心，则设置 <code>num_parallel_calls=4</code> 会更高效。另一方面，将 num_parallel_calls 设置为远大于可用 CPU 数量的值可能会导致调度效率低下，进而减慢速度。</p><p>如将</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span>  dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span>map_func<span class="token operator">=</span>parse_fn<span class="token punctuation">)</span></code></pre><p>改为：</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span>map_func<span class="token operator">=</span>parse_fn<span class="token punctuation">,</span>num_parallel_calls<span class="token operator">=</span>FLAGS<span class="token punctuation">.</span>num_parallel_calls<span class="token punctuation">)</span></code></pre><p>此外，如果批次大小为数百或数千，那么并行处理批次创建过程还可能给流水线带来更大的优势。为此，<code>tf.data</code> API 提供了 <code>tf.contrib.data.map_and_batch</code>转换，它可以将映射和批次转换“混合”在一起。<br>如将：</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span>map_func<span class="token operator">=</span>parse_fn<span class="token punctuation">,</span>num_parallel_calls<span class="token operator">=</span>FLAGS<span class="token punctuation">.</span>num_parallel_calls<span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span>batch_size<span class="token operator">=</span>FLAGS<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span></code></pre><p>改为：</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>apply<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>data<span class="token punctuation">.</span>map_and_batch<span class="token punctuation">(</span>map_func<span class="token operator">=</span>parse_fn<span class="token punctuation">,</span>batch_size<span class="token operator">=</span>FLAGS<span class="token punctuation">.</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h1 id="性能考虑因素"><a href="#性能考虑因素" class="headerlink" title="性能考虑因素"></a>性能考虑因素</h1><h2 id="重复和重排"><a href="#重复和重排" class="headerlink" title="重复和重排"></a>重复和重排</h2><p><code>tf.data.Dataset.repeat</code> 转换会将输入数据重复有限（或无限）次；每次数据重复通常称为一个周期。<code>tf.data.Dataset.shuffle</code> 转换会随机化数据集样本的顺序。</p><p>如果在 <code>shuffle</code> 转换之前应用 <code>repeat</code> 转换，则系统会对周期边界进行模糊处理。也就是说，某些元素可以在其他元素出现之前重复出现。另一方面，如果在重复转换之前应用 <code>shuffle</code> 转换，那么在每个周期开始时性能可能会降低，因为需要初始化 <code>shuffle</code> 转换的内部状态。换言之，前者（<code>repeat</code> 在 <code>shuffle</code> 之前）可提供更好的性能，而后者（<code>repeat</code> 在 <code>shuffle</code> 之前）可提供更强的排序保证。</p><p>如果可能，建议您使用 <code>tf.contrib.data.shuffle_and_repeat</code> 混合转换，这样可以达到两全其美的效果（良好的性能和强大的排序保证）。否则，我们建议在重复之前进行重排。</p><h1 id="最佳做法摘要"><a href="#最佳做法摘要" class="headerlink" title="最佳做法摘要"></a>最佳做法摘要</h1><p>使用 <strong>prefetch</strong> 转换可将提供方和使用方的工作重叠。我们特别建议将 prefetch(n)（<strong>其中 n 是单步训练使用的元素数/批次数</strong>）添加到输入流水线的末尾，以便将在 CPU 上执行的转换与在加速器上执行的训练重叠。</p><p>通过设置 <code>num_parallel_calls</code> 参数并行处理 <code>map</code> 转换。建议您将其值设为可用 CPU 核心的数量.</p><ul><li><p>如果您使用 batch 转换将预处理元素组合到一个批次中，建议您使用 map_and_batch 混合转换；特别是在您使用的批次较大时。</p></li><li><p>如果您要处理远程存储的数据并/或需要反序列化，建议您使用 parallel_interleave 转换来重叠从不同文件读取（和反序列化）数据的操作。</p></li><li><p>向量化传递给 <code>map</code> 转换的低开销用户定义函数，以分摊与调度和执行相应函数相关的开销。</p></li><li><p>如果内存可以容纳您的数据，请使用 <code>cache</code> 转换在第一个周期中将数据缓存在内存中，以便后续周期可以避免与读取、解析和转换该数据相关的开销。</p></li><li><p>如果预处理操作会增加数据大小，建议您首先应用 <code>interleave</code>、<code>prefetch</code> 和 <code>shuffle</code>（如果可以）以减少内存使用量。</p></li><li><p>建议您在应用 <code>repeat</code> 转换之前先应用 <code>shuffle</code> 转换，最好使用 <code>shuffle_and_repeat</code> 混合转换。</p></li></ul><h1 id="One-text-example"><a href="#One-text-example" class="headerlink" title="One text example"></a>One text example</h1><p>首先我们需要加载一个名为file.txt的数据集：</p><pre class=" language-python"><code class="language-python">I use TensorFlowYou use PyTorchBoth are great</code></pre><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>TextLineDataset<span class="token punctuation">(</span><span class="token string">"file.txt"</span><span class="token punctuation">)</span></code></pre><p>我们可以创造一个迭代器对象 <code>iterator object</code> 在数据集上,</p><pre class=" language-python"><code class="language-python">iterator <span class="token operator">=</span> dataset<span class="token punctuation">.</span>make_one_shot_iterator<span class="token punctuation">(</span><span class="token punctuation">)</span>next_element <span class="token operator">=</span> iterator<span class="token punctuation">.</span>get_next<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>one_shot_iterator方法创建一个迭代器，该迭代器能够在数据集上迭代一次。换句话说，一旦到达数据集的末尾，它将停止生成元素并引发异常。</p><p>现在，next_element是一个图的节点(<code>tf.Variable</code>)，它将在每次执行时包含数据集上iterator的下一个元素。现在，让我们运行它:</p><pre class=" language-py"><code class="language-py">with tf.Session() as sess:  for i in range(3):    print(sess.run(next_element))>'I use Tensorflow'>'You use PyTorch'>'Both are great'</code></pre><p>我们可以轻松地将<code>map</code>应用于数据集。例如，按空格分割单词就像添加一行一样简单：</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span><span class="token keyword">lambda</span> string<span class="token punctuation">:</span> tf<span class="token punctuation">.</span>string_split<span class="token punctuation">(</span><span class="token punctuation">[</span>string<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>values<span class="token punctuation">)</span></code></pre><p>shuffle the dataset 也是直接了当的：</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>buffer_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span></code></pre><p>它将加载$3\times3$的元素，并在每次迭代中shuffle它们。</p><p>您还可以创建批:</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span></code></pre><p>并预取数据(换句话说，它总是有一个批准备加载)。</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>prefetch<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><p>完整的代码为：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tfdataset <span class="token operator">=</span>  tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>TextLineDataset<span class="token punctuation">(</span><span class="token string">"./file.txt"</span><span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span><span class="token keyword">lambda</span> string<span class="token punctuation">:</span> tf<span class="token punctuation">.</span>string_split<span class="token punctuation">(</span><span class="token punctuation">[</span>string<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>values<span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>buffer_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>prefetch<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>iterator <span class="token operator">=</span> dataset<span class="token punctuation">.</span>make_one_shot_iterator<span class="token punctuation">(</span><span class="token punctuation">)</span>next_element <span class="token operator">=</span> iterator<span class="token punctuation">.</span>get_next<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>next_element<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p><code>tf.shuffle</code> 使用指南</p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>    buffer_size<span class="token punctuation">,</span>    seed<span class="token operator">=</span>None<span class="token punctuation">,</span>    reshuffle_each_iteration<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre><p>随机打乱此数据集的元素。</p><p>该数据集使用buffer_size元素填充缓冲区，然后从该缓冲区随机抽取元素，用新元素替换所选元素。对于完美的洗牌，需要<strong>大于或等于数据集的完整大小的缓冲区大小</strong>。</p><p>例如，如果数据集包含10,000个元素，但是buffer_size被设置为1,000，那么shuffle将首先从缓冲区中的前1,000个元素中随机选择一个元素。一旦选择了一个元素，它在缓冲区中的空间将被下一个(即1,001-st)元素替换，以维护1,000个元素缓冲区。</p><p>参数:<br>buffer_size:一个tf.int64标量 tf.Tensor，表示新数据集将从中采样的数据集中元素的数量。<br>seed:(可选)一个tf.int64标量 tf.Tensor，表示用来创建分布的随机种子。<br>reshuffle_each_iteration:(可选)。一个布尔值，如果为真，则表示每次遍历数据集时，数据集都应该被伪随机地重新shuffle。(default to True)。<br>Returns:<br>Dataset: A Dataset.</p><p>它的行为与上面的类似，但是由于<code>init_op</code>，我们可以选择从头“restart”。当我们想要执行多个epochs时，这将变得非常方便!</p><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>TextLineDataset<span class="token punctuation">(</span><span class="token string">"file.txt"</span><span class="token punctuation">)</span>iterator <span class="token operator">=</span> dataset<span class="token punctuation">.</span>make_initial_iterator<span class="token punctuation">(</span><span class="token punctuation">)</span>next_element <span class="token operator">=</span> iterator<span class="token punctuation">.</span>get_next<span class="token punctuation">(</span><span class="token punctuation">)</span>init_op <span class="token operator">=</span> iterator<span class="token punctuation">.</span>initializer<span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>     <span class="token comment" spellcheck="true"># Initialize the iterators</span>     sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>init_op<span class="token punctuation">)</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>next_element<span class="token punctuation">)</span><span class="token punctuation">)</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>next_element<span class="token punctuation">)</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># Move the iterator back to the begining</span>     sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>init<span class="token punctuation">)</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>next_element<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token string">'I use Tensorflow'</span>  <span class="token string">'You use PyTorch'</span>  <span class="token string">'I use Tensorflow'</span> <span class="token comment" spellcheck="true"># Iterator moved back at the beginning</span></code></pre><p>由于我们在不同的epoch中只使用一个session，因此需要能够重新启动迭代器。其他一些方法(如tf.Estimator)通过在每个epoch中创建一个新会话来减少使用可初始化迭代器的需求。但是这是有代价的:每次调用estimator.train()或estimator.evaluate()时，都必须重新加载权重和图并重新初始化。</p><h1 id="Building-an-image-data-pipeline"><a href="#Building-an-image-data-pipeline" class="headerlink" title="Building an image data pipeline"></a>Building an image data pipeline</h1><p>下面是图像数据集的样子。这里我们已经有了jpeg图像的文件名列表和相应的标签列表。我们采用以下步骤进行训练:</p><ul><li>从文件名和标签的切片创建数据集</li><li>使用与数据集长度相等的缓冲区大小来shuffle数据。</li><li>解析从文件名到像素值的图像。使用多线程来提高预处理的速度(可选用于训练)图像数据增强。使用多线程来提高预处理的速度</li><li>批处理图像</li><li>预取一批，以确保随时可以提供一批</li></ul><pre class=" language-python"><code class="language-python">dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>from_tensor_slices<span class="token punctuation">(</span><span class="token punctuation">(</span>filenames<span class="token punctuation">,</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>len<span class="token punctuation">(</span>filenames<span class="token punctuation">)</span><span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span>parse_function<span class="token punctuation">,</span>num_parallel_calls<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span>train_preprocess<span class="token punctuation">,</span>num_parallel_calls<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>prefetch<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><p><code>parse_function</code>的作用：</p><ul><li>read the content of the file</li><li>decode using jpeg format</li><li>convert to float values in [0, 1]</li><li>resize to size (64, 64)</li></ul><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">parse_function</span><span class="token punctuation">(</span>filename<span class="token punctuation">,</span> label<span class="token punctuation">)</span><span class="token punctuation">:</span>    image_string <span class="token operator">=</span> tf<span class="token punctuation">.</span>read_file<span class="token punctuation">(</span>filename<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Don't use tf.image.decode_image, or the output shape will be undefined</span>    image <span class="token operator">=</span> tf<span class="token punctuation">.</span>image<span class="token punctuation">.</span>decode_jpeg<span class="token punctuation">(</span>image_string<span class="token punctuation">,</span> channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># This will convert to float values in [0, 1]</span>    image <span class="token operator">=</span> tf<span class="token punctuation">.</span>image<span class="token punctuation">.</span>convert_image_dtype<span class="token punctuation">(</span>image<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>    image <span class="token operator">=</span> tf<span class="token punctuation">.</span>image<span class="token punctuation">.</span>resize_images<span class="token punctuation">(</span>image<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> resized_image<span class="token punctuation">,</span> label</code></pre><p>最后，<code>train_preprocess</code>可以在训练过程中是可选的，进行数据扩充:</p><ul><li>水平翻转图像的概率为1/2</li><li>应用随机亮度和饱和度</li></ul><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_preprocess</span><span class="token punctuation">(</span>image<span class="token punctuation">,</span>label<span class="token punctuation">)</span><span class="token punctuation">:</span>    image <span class="token operator">=</span> tf<span class="token punctuation">.</span>image<span class="token punctuation">.</span>random_flip_left_right<span class="token punctuation">(</span>image<span class="token punctuation">)</span>    image <span class="token operator">=</span> tf<span class="token punctuation">.</span>image<span class="token punctuation">.</span>random_brightness<span class="token punctuation">(</span>image<span class="token punctuation">,</span>max_delta<span class="token operator">=</span><span class="token number">32.0</span><span class="token operator">/</span><span class="token number">255.0</span><span class="token punctuation">)</span>    image <span class="token operator">=</span> tf<span class="token punctuation">.</span>image<span class="token punctuation">.</span>random_saturation<span class="token punctuation">(</span>image<span class="token punctuation">,</span>lower<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>upper <span class="token operator">=</span> <span class="token number">1.5</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Make sure image is still in [0,1]</span>    image <span class="token operator">=</span> tf<span class="token punctuation">.</span>clip_by_value<span class="token punctuation">(</span>image<span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> image<span class="token punctuation">,</span>label</code></pre><h1 id="Building-a-text-data-pipeline"><a href="#Building-a-text-data-pipeline" class="headerlink" title="Building a text data pipeline"></a>Building a text data pipeline</h1><p>假设我们的任务是命名实体识别。换句话说，输入是一个句子，输出是每个单词的标签，比如：</p><pre class=" language-python"><code class="language-python">John   lives <span class="token keyword">in</span> New   YorkB<span class="token operator">-</span>PER  O     O  B<span class="token operator">-</span>LOC I<span class="token operator">-</span>LOC</code></pre><p>因此，我们的数据集需要同时加载句子和标签。我们将把它们存储在两个不同的文件中，一个句子.txt文件(每行一个)和一个标签.txt文件(包含标签)。例如:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># sentences.txt</span>John lives <span class="token keyword">in</span> New YorkWhere <span class="token keyword">is</span> John ?</code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># labels.txt</span>B<span class="token operator">-</span>PER O O B<span class="token operator">-</span>LOC I<span class="token operator">-</span>LOCO O B<span class="token operator">-</span>PER O</code></pre><p>构造 <code>tf.data</code> object。遍历这些文件的数据对象很简单,</p><pre class=" language-python"><code class="language-python">sentences <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>TextLineDataset<span class="token punctuation">(</span><span class="token string">"sentence.txt"</span><span class="token punctuation">)</span>labels <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>TextLineDataset<span class="token punctuation">(</span><span class="token string">"labels.txt"</span><span class="token punctuation">)</span></code></pre><h2 id="Zip-datasets-together"><a href="#Zip-datasets-together" class="headerlink" title="Zip datasets together"></a>Zip datasets together</h2><p>在这个阶段，我们可能需要同时遍历这两个文件。这种操作通常称为“zip”。幸运的是，<code>tf.data</code>中带有这样一个函数：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># zip the sentence and the labels togehters</span>dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>zip<span class="token punctuation">(</span><span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Create a one shot iterator over the zipped dataset</span>iterator <span class="token operator">=</span> dataset<span class="token punctuation">.</span>make_one_shot_iterator<span class="token punctuation">(</span><span class="token punctuation">)</span>next_element <span class="token operator">=</span> iterator<span class="token punctuation">.</span>get_next<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#  Actually run in a Session</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>   <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>       <span class="token keyword">print</span><span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">></span> <span class="token punctuation">(</span><span class="token string">'John lives in New York'</span><span class="token punctuation">,</span> <span class="token string">'B-PER O O B-LOC I-LOC'</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token string">'Where is John ?'</span><span class="token punctuation">,</span> <span class="token string">'O O B-PER O'</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow指南(1)</title>
      <link href="/2019/09/17/multi-gpu-tensorflow/"/>
      <url>/2019/09/17/multi-gpu-tensorflow/</url>
      
        <content type="html"><![CDATA[<h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p>我们使用 <code>tf.Variable</code> 类操作变量。<code>tf.Variable</code> 表示可通过对其运行操作来改变其值的张量。与 <code>tf.Tensor</code> 对象不同，<code>tf.Variable</code> 存在于单个 <code>session.run</code> 调用的<code>上下文之外</code>，变量只存在与一个session之处。</p><p>在 TensorFlow 内部，<code>tf.Variable</code> 会存储持久性张量。具体<code>op</code>(<code>操作</code>，<code>图的节点</code>)允许您读取和修改此张量的值。这些修改在多个 <code>tf.Session</code> 之间是可见的，因此对于一个 tf.Variable，多个工作器可以看到相同的值。</p><h3 id="创建变量"><a href="#创建变量" class="headerlink" title="创建变量"></a>创建变量</h3><p>tf.get_variable()</p><p>要使用 tf.get_variable 创建变量，只需提供名称和形状即可</p><pre class=" language-py"><code class="language-py">my_variable = tf.get_variable("my_variable", [1, 2, 3])</code></pre><p>这将创建一个名为“my_variable”的变量，该变量是形状为 [1, 2, 3] 的三维张量。默认情况下，此变量将具有 dtypetf.float32，其初始值将通过 tf.glorot_uniform_initializer 随机设置。</p><p>您可以选择为 tf.get_variable 指定 dtype 和初始化器。例如：</p><pre class=" language-py"><code class="language-py">my_int_variable = tf.get_variable("my_int_variable", [1, 2, 3], dtype=tf.int32,  initializer=tf.zeros_initializer)</code></pre><p>TensorFlow 提供了许多方便的初始化器。或者，您也可以将 tf.Variable 初始化为 tf.Tensor 的值。例如：</p><pre class=" language-py"><code class="language-py">other_variable = tf.get_variable("other_variable", dtype=tf.int32,  initializer=tf.constant([23, 42]))</code></pre><p>请注意，当初始化器是 tf.Tensor 时，不需要再指定变量的形状，因为将使用初始化器张量的形状。</p><h3 id="变量集合"><a href="#变量集合" class="headerlink" title="变量集合"></a>变量集合</h3><p>由于 TensorFlow 程序的未连接部分可能需要创建变量，因此能有一种方式访问所有变量有时十分受用。为此，TensorFlow 提供了集合，它们是张量或其他对象（如 <code>tf.Variable</code> 实例）的命名列表。</p><p>默认情况下，每个 <code>tf.Variable</code> 都放置在以下两个集合中：</p><p><code>tf.GraphKeys.GLOBAL_VARIABLES</code> - 可以在多台设备间共享的变量，<br><code>tf.GraphKeys.TRAINABLE_VARIABLES</code> - TensorFlow 将计算其梯度的变量。<br>如果您不希望变量可训练，可以将其添加到<code>tf.GraphKeys.LOCAL_VARIABLES</code> 集合中。例如，以下代码段展示了如何将名为 <code>my_local</code> 的变量添加到此集合中：</p><pre class=" language-python"><code class="language-python">my_local <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"my_local"</span><span class="token punctuation">,</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>collections <span class="token operator">=</span> <span class="token punctuation">[</span>tf<span class="token punctuation">.</span>GraphKeys<span class="token punctuation">.</span>LOCAL_VARIABLES<span class="token punctuation">]</span></code></pre><p>或者，一种更常用的方式，您可以指定 trainable=False(作为tf.get_variable的参数)：</p><pre class=" language-python"><code class="language-python">my_non_trainable <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"my_non_trainable"</span><span class="token punctuation">,</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><p>您也可以使用自己的集合。集合名称可为任何字符串，且您无需显式创建集合。创建变量（或任何其他对象）后，要将其添加到集合中，请调用 <code>tf.add_to_collection</code>。例如，以下代码将名为 <code>my_local</code> 的现有变量添加到名为 <code>my_collection_name</code> 的集合中：</p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>add_to_collection<span class="token punctuation">(</span><span class="token string">"my_collection_name"</span><span class="token punctuation">,</span>my_local<span class="token punctuation">)</span></code></pre><p>要检索您放置在某个集合中的所有变量（或其他对象）的列表，您可以使用：</p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>get_collection<span class="token punctuation">(</span><span class="token string">"my_collection_name"</span><span class="token punctuation">)</span></code></pre><h3 id="变量放置在哪个设备上？"><a href="#变量放置在哪个设备上？" class="headerlink" title="变量放置在哪个设备上？"></a>变量放置在哪个设备上？</h3><p>与任何其他 TensorFlow 指令一样，您可以将变量放置在特定设备上。例如，以下代码段创建了名为 v 的变量并将其放置在第二个 GPU 设备上：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"/device:GPU:1"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><p>在分布式设置中，将变量放置在正确设备上尤为重要。如果不小心将变量放在工作器而不是参数服务器上，可能会严重减慢训练速度，最坏的情况下，可能会让每个工作器不断复制各个变量。为此，我们提供了 tf.train.replica_device_setter，它可以自动将变量放置在参数服务器中。例如：</p><pre class=" language-py"><code class="language-py">cluster_spec = {    "ps": ["ps0:2222", "ps1:2222"],    "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}with tf.device(tf.train.replica_device_setter(cluster=cluster_spec)):  v = tf.get_variable("v", shape=[20, 20])  # this variable is placed                                            # in the parameter server                                            # by the replica_device_setter</code></pre><h3 id="初始化变量"><a href="#初始化变量" class="headerlink" title="初始化变量"></a>初始化变量</h3><p>变量必须先初始化后才可使用。如果您在低级别 TensorFlow API 中进行编程（即您在显式创建自己的图和会话），则必须明确初始化变量。<code>tf.contrib.slim</code>、<code>tf.estimator.Estimator</code> 和 <code>Keras</code> 等大多数高级框架在训练模型前会自动为您初始化变量。</p><p>显式初始化在其他方面很有用。它允许您在从检查点重新加载模型时不用重新运行潜在资源消耗大的初始化器，并允许在分布式设置中共享随机初始化的变量时具有确定性。</p><p>要在训练开始前一次性初始化所有可训练变量，请调用 <code>tf.global_variables_initializer()</code>。此函数会返回一个操作，负责初始化 <code>tf.GraphKeys.GLOBAL_VARIABLES</code> 集合中的所有变量. 例如：</p><pre class=" language-python"><code class="language-python">session<span class="token punctuation">.</span>run<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Now all variables are initialized</code></pre><p>如果你需要自行初始化变量，则可以运行变量的初始化操作，例如：</p><pre class=" language-python"><code class="language-python">session<span class="token punctuation">.</span>run<span class="token punctuation">(</span>my_variable<span class="token punctuation">.</span>initializer<span class="token punctuation">)</span></code></pre><p>您还可以查询哪些变量尚未初始化。例如，以下代码会打印所有的尚未初始化的变量名称：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>session<span class="token punctuation">.</span>run<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>report_uninitialized_variables<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>请注意，默认情况下，tf.global_variables_initializer()不会指定变量的初始化顺序。因此，如果变量的初始值取决于另一个变量的值，那么很可能会出现错误。 <strong>任何时候，如果您在并非所有变量都已初始化的上下文中使用某个变量值（例如在初始化某个变量时使用另一变量的值），最好使用 variable.initialized_value()，而非 variable：</strong></p><pre class=" language-python"><code class="language-python">v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>zeros_initializer<span class="token punctuation">)</span>w <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"w"</span><span class="token punctuation">,</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>initializer<span class="token operator">=</span>v<span class="token punctuation">.</span>initialized_value<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><h3 id="使用变量：要在Tensoflow-图中使用-tf-Variable的值，只需要将其视为普通的tf-Tensor即可："><a href="#使用变量：要在Tensoflow-图中使用-tf-Variable的值，只需要将其视为普通的tf-Tensor即可：" class="headerlink" title="使用变量：要在Tensoflow 图中使用 tf.Variable的值，只需要将其视为普通的tf.Tensor即可："></a>使用变量：要在Tensoflow 图中使用 tf.Variable的值，只需要将其视为普通的tf.Tensor即可：</h3><pre class=" language-python"><code class="language-python">v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>zeros_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>w <span class="token operator">=</span> v<span class="token operator">+</span><span class="token number">1</span> <span class="token comment" spellcheck="true"># w is a tf.Tensor which is computed based on the value of v.</span>           <span class="token comment" spellcheck="true"># Any time a variable is used in an expression it gets automatically</span>           <span class="token comment" spellcheck="true"># converted to a tf.Tensor representing its value.</span></code></pre><p>要为变量赋值，请使用<code>assign</code>,<code>assign_add</code>方法以及<code>tf.Variable</code>类中的友元。</p><pre class=" language-python"><code class="language-python">v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>zeros_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>assignment <span class="token operator">=</span> v<span class="token punctuation">.</span>assign_add<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>assignment<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># or assignment.op.run() , or assignment.eval()</span></code></pre><p>由于变量是可变的，因此及时了解任意时间点所使用的变量值版本有时十分有用。要在事件发生后强制重新读取变量的值，可以使用 tf.Variable.read_value。例如：</p><pre class=" language-python"><code class="language-python">v <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"v"</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>zeros_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>assignment <span class="token operator">=</span> v<span class="token punctuation">.</span>assign<span class="token operator">-</span>add<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>control_dependencies<span class="token punctuation">(</span><span class="token punctuation">[</span>assignment<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   w <span class="token operator">=</span> v<span class="token punctuation">.</span>read_value<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># w is guaranteed to reflect v's value after the</span>                       <span class="token comment" spellcheck="true"># assign_add operation.</span></code></pre><h1 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h1><p>Tensorflow 支持两种 <code>共享变量</code>的方式：</p><ul><li>显示传递 tf.Variable 对象</li><li>将tf.Variable对象隐式封装在 tf.variable_scope 对象内。</li></ul><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">conv_relu</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span> kernel_shape<span class="token punctuation">,</span> bias_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># Create variable named "weights".</span>    weights <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"weights"</span><span class="token punctuation">,</span> kernel_shape<span class="token punctuation">,</span>        initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>random_normal_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Create variable named "biases".</span>    biases <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"biases"</span><span class="token punctuation">,</span> bias_shape<span class="token punctuation">,</span>        initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>constant_initializer<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    conv <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>input<span class="token punctuation">,</span> weights<span class="token punctuation">,</span>        strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv <span class="token operator">+</span> biases<span class="token punctuation">)</span></code></pre><p>此函数使用短名称 weights 和 biases，这有利于清晰区分二者。然而，在真实模型中，我们需要很多此类卷积层，而且<strong>重复调用此函数将不起作用</strong>：</p><pre class=" language-python"><code class="language-python">input1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span>input2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span>x <span class="token operator">=</span> conv_relu<span class="token punctuation">(</span>input1<span class="token punctuation">,</span> kernel_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bias_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span>x <span class="token operator">=</span> conv_relu<span class="token punctuation">(</span>x<span class="token punctuation">,</span> kernel_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bias_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># This fails.</span></code></pre><p>由于期望的操作不清楚（创建新变量还是重新使用现有变量？），因此 TensorFlow 将会失败。不过，在不同作用域内调用 conv_relu 可表明我们想要创建新变量：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">my_image_filter</span><span class="token punctuation">(</span>input_images<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"conv1"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Variables created here will be named "conv1/weights", "conv1/biases".</span>        relu1 <span class="token operator">=</span> conv_relu<span class="token punctuation">(</span>input_images<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"conv2"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Variables created here will be named "conv2/weights", "conv2/biases".</span>        <span class="token keyword">return</span> conv_relu<span class="token punctuation">(</span>relu1<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>如果您想要共享变量，有两种方法可供选择。首先，您可以使用 reuse=True 创建具有相同名称的作用域：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"model"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  output1 <span class="token operator">=</span> my_image_filter<span class="token punctuation">(</span>input1<span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"model"</span><span class="token punctuation">,</span> reuse <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  output2 <span class="token operator">=</span> my_image_filter<span class="token punctuation">(</span>input2<span class="token punctuation">)</span></code></pre><p>您也可以调用<code>scope.reuse_variables</code> 以触发重用：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"model"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> scope<span class="token punctuation">:</span>   output1 <span class="token operator">=</span> my_image_filter<span class="token punctuation">(</span>input1<span class="token punctuation">)</span>   scope<span class="token punctuation">.</span>reuse_variables<span class="token punctuation">(</span><span class="token punctuation">)</span>   output2 <span class="token operator">=</span> my_image_filter<span class="token punctuation">(</span>input2<span class="token punctuation">)</span></code></pre><p>由于依赖于作用域的确切字符串可能比较危险，因此也可以根据另一作用域初始化某个变量作用域：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"model"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> scope<span class="token punctuation">:</span>   output1 <span class="token operator">=</span> my_image_filter<span class="token punctuation">(</span>input1<span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span>scope<span class="token punctuation">,</span>reuse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   output2 <span class="token operator">=</span> my_image_filter<span class="token punctuation">(</span>input2<span class="token punctuation">)</span></code></pre><h1 id="使用多个GPU卡训练模型"><a href="#使用多个GPU卡训练模型" class="headerlink" title="使用多个GPU卡训练模型"></a>使用多个GPU卡训练模型</h1><p>现代工作站可能会包含多个用于科学计算的 GPU。TensorFlow 可利用此环境在多个卡上同时运行训练操作。</p><p>如果要以并行的分布式方式训练模型，则需要协调训练过程。在接下来的内容中，术语“模型副本”指在数据子集上训练的模型副本。</p><p>简单地采用<strong>模型参数异步更新方法会导致训练性能无法达到最佳，因为单个模型副本在训练时使用的可能是过时的模型参数。反之，如果采用完全同步的更新后参数，其速度堪比最慢的模型副本</strong>。</p><p>在具有多个 GPU 卡的工作站中，每个 GPU 的速度大致相当，且具有足够的内存来运行整个 <strong>CIFAR-10</strong> 模型。因此，我们选择按照以下方式设计训练系统：</p><ul><li>在每个GPU上放一个模型副本</li><li>等待所有的GPU完成一批数据的处理工作，然后同步更新模型参数<br>模型示意图如下所示：<p align="center"><img width="48%" src="img/Parallelism.png"><br></p></li></ul><p>请注意，每个 GPU 都会针对一批唯一的数据计算推理和梯度。这种设置可以有效地将一大批数据划分到各个 GPU 上。这种设置要求所有 GPU 都共享模型参数。众所周知，将数据传输到 GPU 或从中向外传输数据的速度非常慢。因此，我们决定在 <strong>CPU 上存储和更新所有模型参数</strong>（如绿色方框所示）。当所有 GPU 均处理完一批新数据时，系统会将一组全新的模型参数传输给相应 GPU。</p><p>GPU会同步运行。GPU的所有梯度将累积并求平均值(如绿色方框所示)。模型参数会更新为所有模型副本的梯度平均值。</p><h1 id="将变量和操作放在多个设备上"><a href="#将变量和操作放在多个设备上" class="headerlink" title="将变量和操作放在多个设备上"></a>将变量和操作放在多个设备上</h1><p>将操作和变量放到多个设备上需要一些特殊的抽象操作。</p><p>第一个抽象操作是计算单个模型副本的推理和梯度的函数。在代码中，我们将此抽象操作称为“<code>tower</code>”。我们必须为每个 <code>tower</code> 设置两个属性：</p><ul><li>tower 中所有操作的唯一名称。 <code>tf.name_scope</code>  通过添加作用域前缀提供唯一的名称。例如，第一个 <code>tower</code> 中的所有操作都会附带 <code>tower_0</code> 前缀，例如 <code>tower_0/conv1/Conv2D</code></li><li>运行 tower 中操作(op)的首选硬件设备。 <code>tf.device</code> 会指定该属性。例如，第一个 <code>tower</code> 中的所有操作都位于 <code>device(&#39;/device:GPU:0&#39;)</code> 作用域内，表示它们应在第一个 GPU 上运行<br>为了在多 GPU 版本中共享变量，<strong>所有变量都固定到 CPU 上</strong>且通过 <code>tf.get_variable</code> 访问。了解如何共享变量。</li></ul><h3 id="在多个GPU上启动并训练模型"><a href="#在多个GPU上启动并训练模型" class="headerlink" title="在多个GPU上启动并训练模型"></a>在多个GPU上启动并训练模型</h3><p>如果计算机上安装了多个 GPU 卡，您可以使用 cifar10_multi_gpu_train.py 脚本借助它们加快模型的训练过程。此版训练脚本可在多个 GPU 卡上并行训练模型。</p><pre class=" language-py"><code class="language-py">python cifar10_multi_gpu_train.py --num_gpus=2</code></pre><p>请注意，使用的 GPU 卡数量默认为 1。此外，如果计算机上仅有一个 GPU，则所有计算都会在该 GPU 上运行，即使您设置的是多个 GPU。</p><p>以下是一个示例代码:</p><p>如果您想要在多个 GPU 上运行 TensorFlow，则可以采用<strong>多塔式方式构建模型</strong>，其中每个塔都会分配给不同 GPU。例如：</p><h1 id="Creates-a-graph"><a href="#Creates-a-graph" class="headerlink" title="Creates a graph."></a>Creates a graph.</h1><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token comment" spellcheck="true"># Creates a graph</span>c <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> d <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'/device:GPU:0'</span><span class="token punctuation">,</span><span class="token string">'/device:GPU:1'</span><span class="token punctuation">,</span><span class="token string">'/device:GPU:2'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>device<span class="token punctuation">(</span>d<span class="token punctuation">)</span><span class="token punctuation">:</span>        a <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">,</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">,</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">6.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        b <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">,</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">,</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">6.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        c<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"/cpu:0"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    sum <span class="token operator">=</span> tf<span class="token punctuation">.</span>add_n<span class="token punctuation">(</span>c<span class="token punctuation">)</span>init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>config  <span class="token operator">=</span> tf<span class="token punctuation">.</span>ConfigProto<span class="token punctuation">(</span>allow_soft_placement<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                         log_device_placement<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span>config<span class="token operator">=</span>config<span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>init<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># run the op.</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>sum<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>您可以看到一下输出内容:</p><pre class=" language-python"><code class="language-python">init<span class="token punctuation">:</span> <span class="token punctuation">(</span>NoOp<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">0</span><span class="token number">2019</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">17</span> <span class="token number">21</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">:</span><span class="token number">30.112143</span><span class="token punctuation">:</span> I tensorflow<span class="token operator">/</span>core<span class="token operator">/</span>common_runtime<span class="token operator">/</span>placer<span class="token punctuation">.</span>cc<span class="token punctuation">:</span><span class="token number">886</span><span class="token punctuation">]</span> init<span class="token punctuation">:</span> <span class="token punctuation">(</span>NoOp<span class="token punctuation">)</span><span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">0</span>MatMul_2<span class="token punctuation">:</span> <span class="token punctuation">(</span>MatMul<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">2</span><span class="token number">2019</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">17</span> <span class="token number">21</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">:</span><span class="token number">30.112162</span><span class="token punctuation">:</span> I tensorflow<span class="token operator">/</span>core<span class="token operator">/</span>common_runtime<span class="token operator">/</span>placer<span class="token punctuation">.</span>cc<span class="token punctuation">:</span><span class="token number">886</span><span class="token punctuation">]</span> MatMul_2<span class="token punctuation">:</span> <span class="token punctuation">(</span>MatMul<span class="token punctuation">)</span><span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">2</span>MatMul_1<span class="token punctuation">:</span> <span class="token punctuation">(</span>MatMul<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">1</span><span class="token number">2019</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">17</span> <span class="token number">21</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">:</span><span class="token number">30.112170</span><span class="token punctuation">:</span> I tensorflow<span class="token operator">/</span>core<span class="token operator">/</span>common_runtime<span class="token operator">/</span>placer<span class="token punctuation">.</span>cc<span class="token punctuation">:</span><span class="token number">886</span><span class="token punctuation">]</span> MatMul_1<span class="token punctuation">:</span> <span class="token punctuation">(</span>MatMul<span class="token punctuation">)</span><span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">1</span>MatMul<span class="token punctuation">:</span> <span class="token punctuation">(</span>MatMul<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">0</span><span class="token number">2019</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">17</span> <span class="token number">21</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">:</span><span class="token number">30.112198</span><span class="token punctuation">:</span> I tensorflow<span class="token operator">/</span>core<span class="token operator">/</span>common_runtime<span class="token operator">/</span>placer<span class="token punctuation">.</span>cc<span class="token punctuation">:</span><span class="token number">886</span><span class="token punctuation">]</span> MatMul<span class="token punctuation">:</span> <span class="token punctuation">(</span>MatMul<span class="token punctuation">)</span><span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">0</span>AddN<span class="token punctuation">:</span> <span class="token punctuation">(</span>AddN<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>CPU<span class="token punctuation">:</span><span class="token number">0</span><span class="token number">2019</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">17</span> <span class="token number">21</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">:</span><span class="token number">30.112223</span><span class="token punctuation">:</span> I tensorflow<span class="token operator">/</span>core<span class="token operator">/</span>common_runtime<span class="token operator">/</span>placer<span class="token punctuation">.</span>cc<span class="token punctuation">:</span><span class="token number">886</span><span class="token punctuation">]</span> AddN<span class="token punctuation">:</span> <span class="token punctuation">(</span>AddN<span class="token punctuation">)</span><span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>CPU<span class="token punctuation">:</span><span class="token number">0</span>Const_5<span class="token punctuation">:</span> <span class="token punctuation">(</span>Const<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">2</span><span class="token number">2019</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">17</span> <span class="token number">21</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">:</span><span class="token number">30.112232</span><span class="token punctuation">:</span> I tensorflow<span class="token operator">/</span>core<span class="token operator">/</span>common_runtime<span class="token operator">/</span>placer<span class="token punctuation">.</span>cc<span class="token punctuation">:</span><span class="token number">886</span><span class="token punctuation">]</span> Const_5<span class="token punctuation">:</span> <span class="token punctuation">(</span>Const<span class="token punctuation">)</span><span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">2</span>Const_4<span class="token punctuation">:</span> <span class="token punctuation">(</span>Const<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">2</span><span class="token number">2019</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">17</span> <span class="token number">21</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">:</span><span class="token number">30.112240</span><span class="token punctuation">:</span> I tensorflow<span class="token operator">/</span>core<span class="token operator">/</span>common_runtime<span class="token operator">/</span>placer<span class="token punctuation">.</span>cc<span class="token punctuation">:</span><span class="token number">886</span><span class="token punctuation">]</span> Const_4<span class="token punctuation">:</span> <span class="token punctuation">(</span>Const<span class="token punctuation">)</span><span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">2</span>Const_3<span class="token punctuation">:</span> <span class="token punctuation">(</span>Const<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">1</span><span class="token number">2019</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">17</span> <span class="token number">21</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">:</span><span class="token number">30.112247</span><span class="token punctuation">:</span> I tensorflow<span class="token operator">/</span>core<span class="token operator">/</span>common_runtime<span class="token operator">/</span>placer<span class="token punctuation">.</span>cc<span class="token punctuation">:</span><span class="token number">886</span><span class="token punctuation">]</span> Const_3<span class="token punctuation">:</span> <span class="token punctuation">(</span>Const<span class="token punctuation">)</span><span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">1</span>Const_2<span class="token punctuation">:</span> <span class="token punctuation">(</span>Const<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">1</span><span class="token number">2019</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">17</span> <span class="token number">21</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">:</span><span class="token number">30.112254</span><span class="token punctuation">:</span> I tensorflow<span class="token operator">/</span>core<span class="token operator">/</span>common_runtime<span class="token operator">/</span>placer<span class="token punctuation">.</span>cc<span class="token punctuation">:</span><span class="token number">886</span><span class="token punctuation">]</span> Const_2<span class="token punctuation">:</span> <span class="token punctuation">(</span>Const<span class="token punctuation">)</span><span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">1</span>Const_1<span class="token punctuation">:</span> <span class="token punctuation">(</span>Const<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">0</span><span class="token number">2019</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">17</span> <span class="token number">21</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">:</span><span class="token number">30.112266</span><span class="token punctuation">:</span> I tensorflow<span class="token operator">/</span>core<span class="token operator">/</span>common_runtime<span class="token operator">/</span>placer<span class="token punctuation">.</span>cc<span class="token punctuation">:</span><span class="token number">886</span><span class="token punctuation">]</span> Const_1<span class="token punctuation">:</span> <span class="token punctuation">(</span>Const<span class="token punctuation">)</span><span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">0</span>Const<span class="token punctuation">:</span> <span class="token punctuation">(</span>Const<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">0</span><span class="token number">2019</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">17</span> <span class="token number">21</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">:</span><span class="token number">30.112275</span><span class="token punctuation">:</span> I tensorflow<span class="token operator">/</span>core<span class="token operator">/</span>common_runtime<span class="token operator">/</span>placer<span class="token punctuation">.</span>cc<span class="token punctuation">:</span><span class="token number">886</span><span class="token punctuation">]</span> Const<span class="token punctuation">:</span> <span class="token punctuation">(</span>Const<span class="token punctuation">)</span><span class="token operator">/</span>job<span class="token punctuation">:</span>localhost<span class="token operator">/</span>replica<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>task<span class="token punctuation">:</span><span class="token number">0</span><span class="token operator">/</span>device<span class="token punctuation">:</span>GPU<span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">66</span><span class="token punctuation">.</span>  <span class="token number">84</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token number">147</span><span class="token punctuation">.</span> <span class="token number">192</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span></code></pre><p>更具体的一个例子请参看 <a href>cifar</a>教程.</p>]]></content>
      
      
      <categories>
          
          <category> Tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tensorflow </tag>
            
            <tag> Tricks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>multimodal neural translation series 2</title>
      <link href="/2019/09/15/multimodal-neural-translation-tutorial2/"/>
      <url>/2019/09/15/multimodal-neural-translation-tutorial2/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>作者</strong>: [Thang Luong, Eugene Brevdo, Rui Zhao] (<a href="https://research.googleblog.com/2017/07/building-your-own-neural-machine.html" target="_blank" rel="noopener">Google Research Blogpost</a>, <a href="https://github.com/tensorflow/nmt" target="_blank" rel="noopener">Github</a>)</p></blockquote><blockquote><p>译者: <a href="https://github.com/zongdaoming" target="_blank" rel="noopener">宗道明</a><br>If make use of this codebase for your research, please cite <a href="#bibtex">this</a>.</p></blockquote><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><hr><p>为了更深入地了解神经机器翻译和seq2seq模型，我们推荐以下材料:<a href="https://sites.google.com/site/acl16nmt/" target="_blank" rel="noopener">Luong, Cho, Manning, (2016)</a>; <a href="https://github.com/lmthang/thesis" target="_blank" rel="noopener">Luong, (2016)</a>; and <a href="https://arxiv.org/abs/1703.01619" target="_blank" rel="noopener">Neubig, (2017)</a>.有很多构建seq2seq模型的工具，所以我们选择每种语言中的一个: </p><ul><li>tf-seq2seq (<a href="https://github.com/google/seq2seq)*[TensorFlow]" target="_blank" rel="noopener">https://github.com/google/seq2seq)*[TensorFlow]</a>* </li><li>OpenNMT <a href="http://opennmt.net/" target="_blank" rel="noopener">http://opennmt.net/</a> *[Torch]*</li><li>OpenNMT-py <a href="https://github.com/OpenNMT/OpenNMT-py" target="_blank" rel="noopener">https://github.com/OpenNMT/OpenNMT-py</a> *[PyTorch]*</li></ul><p>Sequence-to-sequence (seq2seq) 模型在机器翻译、语音识别、文本摘要等领域取得了不小的成功。我们旨在构建一个competitive seq2seq模型。我们将从一下三个方面着手实施我们的模型</p><ol><li><p>使用最新的<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops" target="_blank" rel="noopener">解码器/注意力包装器</a>和 Tensorflow 1.2 data iterator</p></li><li><p>构建完整的seq2seq模型</p></li><li><p>提供技巧以建立最好的NMT模型和复现<a href="https://research.google.com/pubs/pub45610.html" target="_blank" rel="noopener">谷歌NMT (GNMT)系统</a>。</p></li></ol><p>我们认为，提供人们可以复现的基准非常重要。因此，我们提供了完整的实验结果，并以下公开可用数据集对我们的模型进行了预训练:</p><ol><li><p><em>samll-scale</em>: 由<a href="https://sites.google.com/site/iwsltevaluation2015/" target="_blank" rel="noopener">IWSLT Evaluation Campaign</a>提供的英语-越语平行TED演讲语料库(133K句子对)</p></li><li><p><em>large-scale</em>: 由<a href="(http://www.statmt.org/wmt16/translation-task.html)">WMT Evaluation Campaign</a>提供的德语-英语平行语料库(450万对句子)。</p></li></ol><p>我们首先建立了一些关于NMT的seq2seq模型的基本知识，解释如何建立和训练一个普通的NMT模型。第二部分将详细介绍建立具有注意机制的竞争性的NMT模型。然后，我们将讨论一些技巧和技巧，以构建尽可能好的NMT模型(在速度和翻译质量方面)，比如<code>TensorFlow最佳实践(批处理、分段)</code>、<code>双向RNNs</code>、<code>束搜索</code>，以及使用<code>GNMT注意力</code>扩展到多个gpu。</p><h1 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h1><h2 id="神经机器翻译的背景"><a href="#神经机器翻译的背景" class="headerlink" title="神经机器翻译的背景"></a>神经机器翻译的背景</h2><p>在过去，传统的基于短语(phrase-based)的翻译系统通过将源句分成多个块，然后逐句翻译来完成任务。这导致了翻译输出的不流畅，与我们人类的翻译方式不太一样。我们通读整篇文章，理解它的意思，然后再翻译出来。神经机器翻译(NMT)模仿了这一点!</p><p align="center"><img width="80%" src="img/encdec.jpg"><br>Figure 1. <b>Encoder-decoder architecture</b> – example of a general approach forNMT. An encoder converts a source sentence into a "meaning" vector which ispassed through a <i>decoder</i> to produce a translation.</p><p>具体地说，NMT系统首先使用编码器读取源语句来构建一个“thought”向量，即表示句子含义的数字序列;然后，解码器处理句子向量以发出翻译，如图1所示。这通常称为编解码器体系结构。通过这种方式，NMT以传统的基于短语的方法解决了本地翻译问题:它可以捕获语言中的长期依赖关系(long-range dependicies)，例如性别协议;语法结构;如谷歌神经机器翻译系统所演示的那样，生成更流畅的翻译。</p><p><code>NMT模型</code>根据其确切的体系结构而有所不同。序列数据的自然选择是<code>循环神经网络(RNN)</code>，大多数NMT模型都使用这种网络。通常RNN用于编码器和解码器。然而，RNN模型在以下方面有所不同:(a)方向性——单向或双向;(b)深度-单层或多层;(c)类型——通常是一个普通的RNN，一个长短时记忆(LSTM)，或者一个门控递归单元(GRU)。</p><p>在本教程中，我们以一个单向的、使用LSTM作为递归单元的多层RNN为例。在图2中我们展示了这样一个模型。在这个例子中，我们建立了一个模型，将源句“I am a student ”翻译成目标句“”Je suis étudiant”。在较高的层次上，NMT模型由两个循环神经网络组成:编码器RNN只消耗输入源词，不做任何预测;另一方面，解码器在处理<code>目标句子</code>的同时预测下一个单词。</p><p align="center"><img width="50%" src="img/seq2seq.jpg"><br>Figure 2. <b>Neural machine translation</b> – example of a deep recurrentarchitecture proposed by for translating a source sentence "I am a student" intoa target sentence "Je suis étudiant". Here, "&lts&gt" marks the start of thedecoding process while "&lt/s&gt" tells the decoder to stop.</p>在训练之前，确保你安装了tensorflow并下载了源码：```pygit clone https://github.com/tensorflow/nmt/```<h2 id="训练——如何建立我们的第一个NMT系统训练"><a href="#训练——如何建立我们的第一个NMT系统训练" class="headerlink" title="训练——如何建立我们的第一个NMT系统训练"></a>训练——如何建立我们的第一个NMT系统训练</h2><pre class=" language-py"><code class="language-py">```让我们首先深入到用具体的代码片段构建NMT模型的核心，通过这些代码片段，我们将更详细地解释图2。这部分引用[**model.py**]().对于训练，我们将向系统提供以下张量，这些张量格式如下:-  **encoder_inputs** [max_encoder_time, batch_size]: source input words.-  **decoder_inputs** [max_decoder_time, batch_size]: target input words.-  **decoder_outputs** [max_decoder_time, batch_size]: target output words, these are decoder_inputs shifted to the left by one time step with an end-of-sentence tag appended on the right.为了提高效率，我们一次训练多个句子(batch_size)。测试时略有不同，所以我们将在稍后讨论它。## Embedding考虑到单词的分类特性，模型必须首先查找源和目标嵌入(look up source and target embddings)，以检索相应的单词表示。要使这个嵌入层起作用，首先为每种语言选择一个词汇表。通常，选择词汇量为V的单词，只有最频繁的V单词才被视为惟一的。所有其他单词都转换为“unknown” token，并得到相同的嵌入。嵌入权重(每种语言一组)通常是在训练中学习的。```pyembedding_encoder = variable_scope.get_variable("embedding_encoder",[src_vocab_size,embedding_size],...)# Look up embedding:# encoder_inputs: [max_time,batch_size]# encoder_emb_inp:[max_time,batch_size,embedding_size]encoder_emb_inp = embedding_ops.embedding_lookup(embedding_encoder,encoder_inputs)</code></pre><p>类似地，我们可以构建embedding_decoder和decoder_emb_inp。注意，可以选择使用预先训练的单词表示(如<code>word2vec</code>或<code>Glove向量</code>)初始化嵌入权重。通常，给定大量的训练数据，我们可以从头开始学习这些嵌入。</p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>一旦检索到单词embeddings，就将其作为输入输入到主网络中，主网络由两个多层RNNs组成——源语言的编码器和目标语言的解码器。这两个RNNs在原则上可以共享相同的权重;然而，在实践中，我们经常使用两个不同的RNN参数(当拟合大型训练数据集时，这样的模型做得更好)。编码器RNN以零向量作为起始状态(starting state)，构造如下:</p><pre class=" language-py"><code class="language-py"># Build RNN cellencoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)# Run Dynamic RNN#   encoder_outputs: [max_time, batch_size, num_units]#   encoder_state: [batch_size, num_units]encoder_outputs, encoder_state = tf.nn.dynamic_rnn(    encoder_cell, encoder_emb_inp,    sequence_length=source_sequence_length, time_major=True)# Build RNN cellencoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)# Run Dynamic RNN#   encoder_outputs: [max_time, batch_size, num_units]#   encoder_state: [batch_size, num_units]encoder_outputs, encoder_state = tf.nn.dynamic_rnn(    encoder_cell, encoder_emb_inp,    sequence_length=source_sequence_length, time_major=True)</code></pre><p>注意，句子有不同的长度，为了避免浪费计算，我们通过<code>source_sequence_length</code>告诉<code>dynamic_rnn</code>确切的源语句长度。因为我们的输入是<code>time major</code>，所以设置<code>time_major=True</code>。在这里，我们只构建了单层LSTM encoder_cell。我们将在后面的部分中描述如何构建多层LSTMs、添加dropout和使用注意力。</p><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>解码器也需要访问源信息，实现这一点的一个简单方法是使用编码器的最后一个隐藏状态encoder_state初始化它。在图2中，我们将源单词“student”的隐藏状态传递给解码器端。</p><pre class=" language-py"><code class="language-py"># Build RNN Celldecoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)# Helperhelper = tf.contrib.seq2seq.TrainingHelper(  decoder_emb_inp, decoder_lengths, time_major= True)# Decoderdecoder = tf.contrib.seq2seq.BasicDecoder(  decoder_cell,helper,encoder_state,  output_layer = projection_layer)# Dynamic decodingoutputs, _  =  tf.contrib.seq2seq.dynamic_decode(decoder,...)logits = outputs.rnn_output</code></pre><p>这里，这段代码的核心部分是<code>BasicDecoder对象decoder</code>，它接收<code>decoder_cell</code>(类似于encoder_cell)、一个<code>helper</code>和前面的<code>encoder_state</code>作为输入。通过分离decoders和helpers，我们可以重用不同的代码库，例如，TrainingHelper可以用<code>GreedyEmbeddingHelper</code>替换来进行贪婪解码。请参阅<strong>helper.py</strong>。</p><p>最后，我们还没有提到projection_layer，它是一个将最上面的隐藏状态转换为维度$V$的logit向量的权重矩阵(dense matrix)。</p><pre class=" language-py"><code class="language-py">projection_layer = layers_core.Dense(tgt_vocab_size,use_bias =False)</code></pre><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p>根据上面的逻辑，我们现在可以计算我们的训练损失:</p><pre class=" language-py"><code class="language-py">crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(    labels=decoder_outputs, logits=logits)train_loss = tf.reduce_sum(crossent * target_weights) /batch_size</code></pre><p>这里，target_weights是一个与decoder_output大小相同的0 - 1(mask)矩阵。它用值0掩盖目标序列长度之外的填充位置。</p><p>Note:值得指出的是，我们将损失除以batch_size，因此我们的超参数对于batch_size是“invarant”。有些人将损失除以(batch_size * num_time_steps)，这样可以减少短句子中的错误。更微妙的是，我们的超参数(应用于前一种方法)不能用于后一种方法。例如，如果两种方法都使用学习1.0的SGD，那么后一种方法有效地使用1 / num_time_steps的学习速率要小得多。</p><h2 id="Gradient-computation-amp-optimization"><a href="#Gradient-computation-amp-optimization" class="headerlink" title="Gradient computation &amp; optimization"></a>Gradient computation &amp; optimization</h2><p>我们现在已经定义了NMT模型的正向传递。计算反向传播传递只需几行代码:</p><pre class=" language-py"><code class="language-py">params = tf.trainable_variables()gradients = tf.gradients(train_loss,params)clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)</code></pre><p>梯度裁剪是训练神经网络的重要步骤之一。这里，我们按global norm, max_gradient_norm通常设置为5或1之类的值。最后一步是选择优化器。Adam优化器是一种常见的选择。我们还选择了一个学习率。learning_rate can的值通常在0.0001到0.001之间;并且可以随着训练步数的而减少。</p><pre class=" language-py"><code class="language-py">optimizer = tf.train.Adamoptimizer(learning_rate)update_step = optimizer.apply_gradients(zip(clipped_gradients,params))</code></pre><h1 id="训练一个NMT模型"><a href="#训练一个NMT模型" class="headerlink" title="训练一个NMT模型"></a>训练一个NMT模型</h1><p>我们将使用一个小型的并行TED演讲语料库(133K个训练示例)来进行这个练习。我们在这里使用的所有数据都可以在<a href="1">1</a>中找到。我们将使用tst2012作为开发集，tst2013作为测试集。</p><p>运行以下命令下载训练NMT模型的数据:</p><pre class=" language-py"><code class="language-py">nmt/scripts/download_iwslt15.sh /tmp/nmt_data</code></pre><p>开始训练：</p><pre class=" language-shell"><code class="language-shell">mkdir /tmp/nmt_modelpython -m nmt.nmt \    --src=vi --tgt=en \    --vocab_prefix=/tmp/nmt_data/vocab  \    --train_prefix=/tmp/nmt_data/train \    --dev_prefix=/tmp/nmt_data/tst2012  \    --test_prefix=/tmp/nmt_data/tst2013 \    --out_dir=/tmp/nmt_model \    --num_train_steps=12000 \    --steps_per_stats=100 \    --num_layers=2 \    --num_units=128 \    --dropout=0.2 \    --metrics=bleu</code></pre><p>上面的命令训练了一个2层的LSTM seq2seq模型。隐藏层单元为128，训练的epoch为12轮。</p><pre class=" language-py"><code class="language-py"># First evaluation, global step 0  eval dev: perplexity 17193.66  eval test: perplexity 17193.27# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017  sample train data:    src_reverse: </s> </s> Điều đó , dĩ nhiên , là câu chuyện trích ra từ học thuyết của Karl Marx .    ref: That , of course , was the <unk> distilled from the theories of Karl Marx . </s> </s> </s>  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00</code></pre><p>我们可以启动Tensorboard查看训练时模型的总结:</p><pre class=" language-py"><code class="language-py">tensorboard --port 22222 --logdir /tmp/nmt_model/</code></pre><h1 id="推理：如何得到产生翻译"><a href="#推理：如何得到产生翻译" class="headerlink" title="推理：如何得到产生翻译"></a>推理：如何得到产生翻译</h1><p>当您在训练NMT模型(以及一旦您训练了模型)时，您可以获得给定的未见源语句的翻译。这个过程叫做推理。训练和推理(测试)有明显的区别:在推理时，我们只能访问源句，即 <code>encoder_inputs</code>。执行解码的方法有很多。译码方法包括<code>贪婪解码</code>、<code>采样解码</code>和<code>波束解码</code>译码。在这里，我们将讨论<code>贪婪解码</code>策略。</p><p>这个想法很简单，我们在图3中进行了说明:</p><ul><li><p>我们仍然使用与训练期间相同的方法对源语句进行编码，以获得 <code>encoder_state</code>，并且这个<code>encoder_state</code>用于初始化<code>解码器</code>。</p></li><li><p><code>解码器</code>一收到起始符号“&lt; s &gt;”(在我们的代码中称为tgt_sos_id)，即开始解码(翻译)过程;&lt;/ s&gt;</p></li><li><p>&lt; s &gt;对于解码器端的每个时间步，我们都将RNN的输出作为一组logits。我们选择最有可能的单词，即与最大logit值关联的id作为发出的单词(这是“贪婪”行为)。例如，在图3中，单词“moi”在第一个解码步骤中具有最高的翻译概率。然后我们将这个单词作为输入输入到下一个时间步中。&lt;/ s &gt;</p></li><li><p>&lt; s &gt;这个过程一直持续到句末标记“&lt;/ s &gt;”作为输出符号生成(在我们的代码中称tgt_eos_id)。</p></li></ul><p align="center"><img width="40%" src="img/greedy.jpg"><br>Figure 3. <b>Greedy decoding</b> – example of how a trained NMT model produces atranslation for a source sentence "Je suis étudiant" using greedy search.</p><p>第三步是推理和训练的不同之处。推理并不总是输入正确的目标单词(teacher forcing)，而是使用模型预测的单词。下面是实现<code>贪心解码</code>的代码。它与训练解码器非常相似。</p><pre class=" language-py"><code class="language-py"># Helperhelper = tf.contrib.seq2seq.GreedyEmbeddingHelper(    embedding_decoder,    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)# Decoderdecoder = tf.contrib.seq2seq.BasicDecoder(  decoder_cell,helper,encoder_state,  output_layer =  projection_layers)# Dynamic decodingoutputs, _ = tf.contrib.seq2seq.dynamic_decoder(  decoder, maximum_iterations = maximum_iterations  translations = outputs.sample_id)</code></pre><p>tf.fill(dims,values,name=None): 创建一个维度为dims，值为value的tensor对象．该操作会创建一个维度为dims的tensor对象，并将其值设置为value，该tensor对象中的值类型和value一致</p><p>当value为0时，该方法等同于tf.zeros()<br>当value为1时，该方法等同于tf.ones()<br>参数:</p><ul><li>dims: 类型为int32的tensor对象，用于表示输出的维度(1-D, n-D)，通常为一个int32数- 组，如：<a href="https://nlp.stanford.edu/projects/nmt/" target="_blank" rel="noopener">1</a>, [2,3]等</li><li>value: 常量值(字符串，数字等)，该参数用于设置到最终返回的tensor对象值中</li><li>name: 当前操作别名(可选)<br>返回: </li><li>tensor对象，类型和value一致<pre class=" language-py"><code class="language-py">import tensorflow as tfsess = tf.InteractiveSession()dim = [2,3]tf.fill(dim, 5)# [[5 5 5]#  [5 5 5]]tf.fill(dim, 5.0)# [[ 5.  5.  5.]#  [ 5.  5.  5.]]tf.fill(dim, "5.0")# [['5.0' '5.0' '5.0']#  ['5.0' '5.0' '5.0']]</code></pre>在这里，我们使用greedembeddinghelper而不是traininghelper。由于我们事先不知道 <code>目标序列</code>的长度，所以我们使用最大迭代次数来限制翻译长度。一种启发式方法是解码长度最多是<code>源语句</code>的两倍。<pre class=" language-py"><code class="language-py">maximum_iteration =  tf.round(tf.reduce_max(source_length)*2)</code></pre></li></ul><p>在训练了一个模型之后，我们现在可以创建一个推理文件并翻译一些句子:</p><pre class=" language-shell"><code class="language-shell">cat > /tmp/my_infer_file.vi# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)python -m nmt.nmt \    --out_dir=/tmp/nmt_model \    --inference_input_file=/tmp/my_infer_file.vi \    --inference_output_file=/tmp/nmt_model/output_infercat /tmp/nmt_model/output_infer # To view the inference as output</code></pre><p>注意，只要有训练检查点,上述命令也可以在模型仍在训练时运行。有关详细信息，请参见[<strong>inference.py</strong>]</p><h1 id="Intermediate"><a href="#Intermediate" class="headerlink" title="Intermediate"></a>Intermediate</h1><p>在经历了最基本的seq2seq模型之后，让我们更进一步!为了构建最先进的神经机器翻译系统，我们需要更多的“secret source”:注意力机制，它最初由Bahdanau等人在2015年提出，后来由Luong等人在2015年等人完善。注意机制的核心思想是通过在翻译过程中“注意”相关的源内容，在目标和源之间建立直接的捷径连接。注意力机制的一个很好的副产品是源句和目标句之间的一个易于可视化的对齐矩阵(如图4所示)。</p><p align="center"><img width="40%" src="img/attention_vis.jpg"><br>Figure 4. <b>Attention visualization</b> – example of the alignments between sourceand target sentences. Image is taken from (Bahdanau et al., 2015).</p><p>在普通的seq2seq模型中，在开始解码过程时，我们将<strong>最后一个源状态</strong>从编码器传递给解码器。这对于中短句很有效;然而，对于长句，<strong>单一的固定大小的隐藏状态</strong>成为信息瓶颈。<strong>注意力机制没有丢弃源RNN中计算的所有隐藏状态</strong>，而是提供了一种方法，允许解码器查看它们(将它们视为<strong>源信息的动态内存</strong>)。通过这样做，注意机制可以提高长句的翻译。如今，注意力机制已经成为事实上的标准 defacto standard，并已成功地应用于许多其他任务(包括图像标题生成(caption generation)、语音识别(speech recognition)和文本摘要(text summarization))。</p><h1 id="注意力机制的背景"><a href="#注意力机制的背景" class="headerlink" title="注意力机制的背景"></a>注意力机制的背景</h1><p>我们现在描述<a href>Luong et al.</a>中提出的注意机制的一个实例，该实例已在多个最先进的系统中使用，包括<a href="http://opennmt.net/about/" target="_blank" rel="noopener">OpenNMT</a>等开源工具包和本教程中的 <code>TF seq2seq API</code>。我们还将提供与注意机制的其他变体的连接。</p><p align="center"><img width="48%" src="img/attention_mechanism.jpg"><br>Figure 5. <b>Attention mechanism</b> – example of an attention-based NMT systemas described in (Luong et al., 2015) . We highlight in detail the first step ofthe attention computation. For clarity, we don't show the embedding andprojection layers in Figure (2).</p><p>如图5所示，注意计算发生在每个解码器的时间步长。它包括下列各阶段:</p><ul><li>将当前<code>目标隐状态</code>与所有<code>源状态</code>进行比较，以获得<code>注意力权重</code>(可以如图4所示)。</li></ul><p>$$\alpha_{t s}=\frac{\exp \left(\operatorname{score}\left(\boldsymbol{h}<em>{t}, \overline{\boldsymbol{h}}</em>{s}\right)\right)}{\sum_{s^{\prime}=1}^{S} \exp \left(\operatorname{score}\left(\boldsymbol{h}<em>{t}, \overline{\boldsymbol{h}}</em>{s^{\prime}}\right)\right)} \quad \quad \text { [Attention weights ] }$$</p><ul><li><p>根据<code>注意力权值</code>计算<code>上下文向量</code>作为<code>源状态的加权平均</code>。$$\boldsymbol{c}<em>{t}=\sum</em>{s} \alpha_{t s} \overline{\boldsymbol{h}}_{s} \quad \quad \text{ [Context vector] }$$</p></li><li><p>结合上下文向量和当前目标隐藏状态，生成最终的注意力向量(注意力隐状态)<br>$$\boldsymbol{\tilde{h}}<em>{t}=\tanh(\boldsymbol{W}</em>{c}[\boldsymbol{c}<em>{t};\boldsymbol{h}</em>{t}]) \quad \quad \text{ [Attention vector ]}$$</p></li><li><p>将注意力向量作为输入(input feeding)输入到下一个时间步。</p></li></ul><p>这里，<code>score function</code>用于将<code>目标隐状态</code>$\boldsymbol{h}<em>t$与每个<code>源状态</code> $\overline{h}</em>{s}$进行比较，并将结果规范化为产生的注意权重(分布在源位置上)。<code>评分函数</code>有多种选择; 常用的<code>评分函数</code>包括式(4)中给出的乘法和加法形式。一旦计算完毕，注意力向量$\boldsymbol{\tilde{h}}<em>{t}$将用于推导softmax logit 和 loss。这类似于普通seq2seq模型顶层的目标隐状态。注意力机制还可以有其他的选择，注意力机制的各种实现可以在<a href>attention_wrapper.py</a>中找到。<br>$$<br>\operatorname{score}\left(\boldsymbol{h}</em>{t}, \overline{\boldsymbol{h}}<em>{s}\right)=\left{\begin{array}{ll}{\boldsymbol{h}</em>{t}^{\top} \boldsymbol{W} \overline{\boldsymbol{h}}<em>{s}} &amp; {\text { [Luong’s multiplicative style] }} \ {\boldsymbol{v}</em>{a}^{\top} \tanh \left(\boldsymbol{W}<em>{1} \boldsymbol{h}</em>{t}+\boldsymbol{W}<em>{2} \overline{\boldsymbol{h}}</em>{s}\right)} &amp; {[\text { Bahdanau’s additive style }]}\end{array}\right.<br>$$</p><h3 id="在注意机制中什么是重要的"><a href="#在注意机制中什么是重要的" class="headerlink" title="在注意机制中什么是重要的?"></a>在注意机制中什么是重要的?</h3><p>正如上面的等式所暗示的，注意力有许多不同的变体。这些变体取决于<code>评分函数</code>和<code>注意力函数</code>的形式，以及是否如(Bahdanau et al.， 2015)论文中最初建议的那样，在<code>评分函数</code>中使用之前的目标隐状态$h_{t-1}$而不是当前的目标隐状态$h_t$。根据经验，我们发现只有特定的选择才重要。第一，注意力的基本形式，需要目标和源之间的直接连接。其次，重要的是将注意力向量提供给下一个时间步骤，以便将过去的注意力决策告知网络(Luong et al.， 2015)。最后，评分函数的选择常常会导致不同的性能。更多信息请参见基准测试结果部分。</p><h2 id="Attention-Wrapper-API"><a href="#Attention-Wrapper-API" class="headerlink" title="Attention Wrapper API"></a>Attention Wrapper API</h2><p>在实现AttentionWrapper时，我们借用了(Weston et al., 2015)](<a href="https://arxiv.org/abs/1410.3916)中关于内存网络的工作中的一些术语。本教程中介绍的注意机制是只读内存[read-only" target="_blank" rel="noopener">https://arxiv.org/abs/1410.3916)中关于内存网络的工作中的一些术语。本教程中介绍的注意机制是只读内存[read-only</a> memory]()，而不是可读和可写内存。具体地说，源隐状态集(或它们的转换版本，例如Luong评分风格中的$W\overline{h}_s$或Bahdanau评分风格的$W_2\overline{h}_s$)被称为“内存”。在每个时间步中，我们使用当前目标隐状态作为“查询”<a href>query</a>来决定读取内存的哪些部分。通常，需要将“查询”与对应于各个内存槽的键<a href>key</a>进行比较。在上面介绍注意机制时，我们碰巧使用了一组源隐状态(或者它们的转换版本，例如Bahdanau评分风格中的$W_1h_t$)作为“键”。我们可以从这个记忆网络术语中得到启发，衍生出其他形式的注意力!</p><p>有了注意力包装器，用注意力扩展我们的普通seq2seq代码将变得非常简单。这部分引用了代码<a href>attention_model.py</a>. 首先，我们需要定义一个注意机制，例如，from (Luong et al.， 2015):</p><pre class=" language-py"><code class="language-py"># attention_states: [batch_size, max_time, num_units]attention_states = tf.transpose(encoder_outputs, [1, 0, 2])# Create an attention mechanismattention_mechanism = tf.contrib.seq2seq.LuongAttention(    num_units, attention_states,    memory_sequence_length=source_sequence_length)</code></pre><p>在前面的编码器部分中，encoder_output是顶层的所有<code>源隐状态</code>的集合(encoder_outputs is the set of all source hidden states at the top layer)，其形状为<a href>max_time、batch_size、num_units</a>,因为我们使用dynamic_rnn，为了提高效率，将time_major设置为True。对于注意机制，我们需要确保传入的“内存”是批处理主内存，因此需要转置attention_states。我们将source_sequence_length传递给注意机制，以确保注意力权重得到适当的规范化(仅针对非填充位置)。</p><p>定义了注意里机制后，我们使用AttentionWrapper来包装解码单元:</p><pre class=" language-py"><code class="language-py">decoder_cell = tf.contrib.seq2seq.AttentionWrapper(  decoder_cell,attention_mechanism,  attention_layer_size = num_units)</code></pre><p>剩下的代码几乎与解码器中的代码相同！</p><h1 id="动手-建立一个基于注意力的NMT模型"><a href="#动手-建立一个基于注意力的NMT模型" class="headerlink" title="动手-建立一个基于注意力的NMT模型"></a>动手-建立一个基于注意力的NMT模型</h1><p>为了提高注意力，我们需要使用luong、scaled_luong、bahdanau或normed_bahdanau中的一个作为训练期间注意力机制。。此外，我们需要为注意力模型创建一个新目录，因此我们不重用以前训练过的基本NMT模型。</p><p>运行以下命令开始训练:</p><pre class=" language-shell"><code class="language-shell">mkdir /tmp/nmt_attention_modelpython -m nmt.nmt \     -- attention = scaled_luong \     -- src = vi --tgt = en \    -- vocab_prefix = /tmp/nmt_data/vocab \    -- train_prefix = /tmp/nmt_data/train \    -- dev_prefix = /tmp/nmt_data/tst2012 \    -- test_prefix = /tmp/nmt_data/tst2013 \    -- out_dir = /tmp/nmt_attention_model \    -- num_train_steps = 12000 \    -- steps_per_stats = 100 \    -- num_layers = 2 \    -- num_units = 128 \    -- dropout = 0.2 \    -- metrics = bleu</code></pre><p>经过训练，我们可以使用与新的out_dir相同的推理命令进行推理:</p><pre class=" language-shell"><code class="language-shell">python -m nmt.nmt \    --out_dir=/tmp/nmt_attention_model \    --inference_input_file=/tmp/my_infer_file.vi \    --inference_output_file=/tmp/nmt_attention_model/output_infer</code></pre><h1 id="Tips-amp-Tricks"><a href="#Tips-amp-Tricks" class="headerlink" title="Tips &amp; Tricks"></a>Tips &amp; Tricks</h1><h2 id="Building-Training-Eval-and-Inference-Graphs"><a href="#Building-Training-Eval-and-Inference-Graphs" class="headerlink" title="Building Training, Eval, and Inference Graphs"></a>Building Training, Eval, and Inference Graphs</h2><p>在Tensorflow中构建机器学习模型时，通常最好构建三张单独的图：</p><ul><li><p>训练图(Training Graph):</p><ul><li>Batches, buckets, and possibly subsamples input data from a set of<br>files/external inputs.</li><li>Includes the forward and backprop ops.</li><li>Constructs the optimizer, and adds the training op.</li></ul></li><li><p>评估图(Eval Graph):</p><ul><li>Batches and buckets input data from a set of files/external inputs.</li><li>Includes the training forward ops, and additional evaluation ops that<br>aren’t used for training.</li></ul></li><li><p>推理图(Inference Graph):</p><ul><li>May not batch input data.</li><li>Does not subsample or bucket input data.</li><li>Reads input data from placeholders (data can be fed directly to the graph<br>via <em>feed_dict</em> or from a C++ TensorFlow serving binary).</li><li>Includes a subset of the model forward ops, and possibly additional<br>special inputs/outputs for storing state between session.run calls.</li></ul></li></ul><p>构建单独的图有几个好处:</p><ul><li><p>推理图通常与其他两个图非常不同，因此单独构建它是有意义的。 评估图(Eval Graph)变得更简单了，因为它不再有所有额外的支持操作。</p></li><li><p>可以为每个图分别实现数据输入。 变量重用要简单得多。例如，在评估图中，不需要使用<code>reuse=True</code>重新打开变量作用域，因为训练模型已经创建了这些变量。因此，可以重用相同的代码，而不需要到处散布<code>resue=arguments</code>。</p></li><li><p>在分布式训练中，让单独的工作人员执行训练、计算和推理是很常见的。无论如何，它们都需要构建自己的图。因此，以这种方式构建系统为分布式训练做好了准备。</p></li><li><p>复杂性的主要来源是如何在一个机器设置中跨三个图共享变量。这可以通过为每个图使用单独的会话来解决。训练会话(training Session)定期保存检查点，评估会话(eval session)和推断会话(infer session)从检查点恢复参数。下面的示例显示了这两种方法之间的主要区别。</p></li></ul><h3 id="例子1：一个图（a-single-graph）中的三个模型-three-models-和共享一个会话-a-single-session"><a href="#例子1：一个图（a-single-graph）中的三个模型-three-models-和共享一个会话-a-single-session" class="headerlink" title="例子1：一个图（a single graph）中的三个模型(three models)和共享一个会话 (a single session)"></a>例子1：一个图（a single graph）中的三个模型(three models)和共享一个会话 (a single session)</h3><pre class=" language-py"><code class="language-py">with tf.variable_scope("root"):     train_inputs =  tf.placeholder()    train_op, loss = BuildTrainModel(train_inputs)    initializer = tf.global_variables_initiazlizer()with tf.variable_scope("root",reuse=True):    eval_inputs =  tf.placeholder()    eval_loss = BuilderEvalModel(eval_inputs)with tf.variable_scope("root",reuse=True):    infer_inputs = tf.placeholder()    inference_output = BuildInference()sess = tf.Session()sess.run(initializer)for i in itertools.count():   train_input_data = ...   sess.run([loss,train_op],feed_dict={train_inputs:train_input_data})   if i % EVAL_STEPS == 0:     while data_to_eval:       eval_input_data = ...       sess.run([eval_loss],feed_dict={eval_inputs:eval_input_data})   if i % INFER_STEPS == 0:       sess.run(inference_output, feed_dict ={infer_inputs: infer_input_data})sess = tf.Session()sess.run(initializer)for i in itertools.count():  train_input_data = ...  sess.run([loss, train_op], feed_dict={train_inputs: train_input_data})  if i % EVAL_STEPS == 0:    while data_to_eval:      eval_input_data = ...      sess.run([eval_loss], feed_dict={eval_inputs: eval_input_data})  if i % INFER_STEPS == 0:    sess.run(inference_output, feed_dict={infer_inputs: infer_input_data})</code></pre><p>注意后一种方法是如何“准备”转换为分布式版本的。</p><p>新方法的另一个区别是，我们使用有状态迭代器对象，而不是在每个<code>session.run</code>调用中使用<code>feed</code>指令来提供数据（从而执行我们自己的批处理、<code>bucketing</code>和数据操作）。这些迭代器使输入管道在单机和分布式设置中都更加容易。我们将在下一节介绍新的输入数据管道（如tensorflow 1.2中介绍的）。</p><h3 id="例子2：三个图中的三个模型，三个会话共享相同的变量"><a href="#例子2：三个图中的三个模型，三个会话共享相同的变量" class="headerlink" title="例子2：三个图中的三个模型，三个会话共享相同的变量"></a>例子2：三个图中的三个模型，三个会话共享相同的变量</h3><pre class=" language-py"><code class="language-py">train_graph = tf.Graph()eval_graph = tf.Graph()infer_graph = tf.Graph()with train_graph.as_default():  train_iterator = ...  train_model = BuildTrainModel(train_iterator)  initializer = tf.global_variables_initializer()with eval_graph.as_default():  eval_iterator = ...  eval_model = BuildEvalModel(eval_iterator)with infer_graph.as_default():  infer_iterator, infer_inputs = ...  infer_model = BuildInferenceModel(infer_iterator)checkpoints_path = "/tmp/model/checkpoints"train_sess = tf.Session(graph = train_graph)eval_sess = tf.Session(graph =  eval_graph)infer_sess = tf.Session(graph = infer_graph)train_sess.run(initializer)train_sess.run(train_iterator.initializer)for i in itertools.count():  train_model.train(train_sess)  if i % EVAL_STEPS == 0:    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)    eval_model.saver.restore(eval_sess, checkpoint_path)    eval_sess.run(eval_iterator.initializer)    while data_to_eval:      eval_model.eval(eval_sess)  if i % INFER_STEPS == 0:    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)    infer_model.saver.restore(infer_sess,checkpoint_path)    infer_sess.run(infer_iterator.initializer, feed_dict={infer_inputs:infer_input_data})    while data_to_infer:      infer_model.infer(infer_sess)</code></pre><h1 id="Other-details-for-better-NMT-models"><a href="#Other-details-for-better-NMT-models" class="headerlink" title="Other details for better NMT models"></a>Other details for better NMT models</h1><h2 id="Bidirectional-RNNs"><a href="#Bidirectional-RNNs" class="headerlink" title="Bidirectional RNNs"></a>Bidirectional RNNs</h2><p>编码器端的双向性通常会提供更好的性能（随着使用更多层，速度会有所下降）。这里，我们给出了一个简单的例子，说明如何构建一个具有单个双向层的编码器：</p><pre class=" language-py"><code class="language-py"># Construct forward and backward cellsforward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)bi_outputs, encoder_state =  tf.nn.bidirectional_dynamic_rnn(  forward_cell, backward_cell, encoder_emb_inp,  sequence_length = source_sequence_length, time_major = True))encoder_outputs =  tf.concat(bi_outputs,-1)</code></pre><p>变量encoder_outputs和encoder_state使用方法与之前章节的编码器使用方法相同。请注意，对于多个双向层，我们需要稍微操作编码器状态，有关详细信息，请参见<a href>model.py</a>。</p><h2 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h2><p><code>贪婪解码</code>可以提供相当合理的翻译质量，而<code>波束搜索解码</code>可以进一步提高性能。beam搜索的思想是，在我们翻译的时候，通过保留一小部分优秀的候选词，更好地探索所有可能翻译的搜索空间。束的大小称为束宽；最小光束宽（如尺寸10）通常就足够了。下面是一个如何进行波束搜索的示例：</p><h3 id="Data-Input-Pipeline"><a href="#Data-Input-Pipeline" class="headerlink" title="Data Input Pipeline"></a>Data Input Pipeline</h3><p>在TensorFlow 1.2之前，用户有两个选项可以将数据提供给TensorFlow training 和eval pipeline:</p><ul><li>在每个训练会话中直接通过feed_dict提供数据。</li><li>使用tf中的排队机制。训练(例如tf.train.batch)和tf.contrib.train。</li><li>使用来自更高级别框架(如tf.contrib)的帮助程序。学习或tf.contrib。slim(有效地使用了#2)。</li></ul><p>第一种方法对于不熟悉TensorFlow或需要进行外来输入修改(即)，这只能在Python中完成。第二和第三种方法更标准，但灵活性稍差; 它们还需要启动多个python线程(队列运行器)。此外，<code>如果使用不当，队列可能导致死锁或不透明的错误消息</code>。然而，队列比使用<code>feed_dict</code>效率高得多，并且是单机和分布式训练的标准。</p><p>从Tensorflow 1.2开始，有一个新的系统可用于将数据读入tensorflow模型：dataset迭代器，如tf.data模块中所示。数据迭代器是灵活的，易于推理和操作，并通过利用Tensorflow C++ 运行时提供效率和多线程。</p><p>可以从批处理数据张量、文件名或包含多个文件名的张量创建数据集。一些例子：</p><pre class=" language-py"><code class="language-py"># Training dataset consists of multiple files.train_dataset = tf.data.TextLineDataset(train_files)# Evaluation dataset uses a single file, but we may# point to a different file for each evaluation round.eval_file = tf.placeholder(tf.string, shape=())eval_dataset = tf.data.TextLineDataset(eval_file)# For inference, feed input data to the dataset directly via feed_dict.infer_batch = tf.placeholder(tf.string, shape=(num_infer_examples,))infer_dataset = tf.data.Dataset.from_tensor_slices(infer_batch)</code></pre><p>所有数据集都可以通过输入处理进行类似的处理。这包括读取和清理数据、bucketing（在训练和评估的情况下）、过滤和批处理。</p><p>例如，要将每个句子转换为字符串向量，我们使用数据集映射转换：</p><pre class=" language-py"><code class="language-py">dataset = dataset.map(lambda string: tf.string_split([string]).values)</code></pre><p>然后，我们可以将每个<code>句子向量</code>转换为包含<code>向量</code>及其<code>动态长度</code>的<code>元组</code>：</p><pre class=" language-py"><code class="language-py">dataset = dataset.map(lambda words: (words, tf.size(words))</code></pre><p>最后，我们可以对每个句子执行词汇查找。给定一个查找表对象表，此映射将第一个元组元素从<code>字符串向量</code>转换为<code>整数向量</code>。</p><pre class=" language-py"><code class="language-py">dataset = dataset.map(lambda words, size:(table.looktup(words),size))</code></pre><p>连接两个数据集也很容易。如果两个文件包含彼此的逐行翻译，并且每个文件都读入自己的数据集，则可以通过以下方式创建包含压缩行元组的新数据集：</p><pre class=" language-py"><code class="language-py">source_target_data = tf.data.Dateset.zip((source_dataset,target_dataset))</code></pre><p>句子的变长批处理非常简单。以下转换将批处理<code>source_target_data</code>数据集中的<code>batch_size</code>元素，并分别将<code>源向量</code>和<code>目标向量</code>填充到每个批中最长的<code>源向量</code>和<code>目标向量</code>的长度。</p><pre class=" language-py"><code class="language-py">batched_dataset = source_target_dataset.padded_batch(        batch_size,        padded_shapes=((tf.TensorShape([None]),  # source vectors of unknown size                        tf.TensorShape([])),     # size(source)                       (tf.TensorShape([None]),  # target vectors of unknown size                        tf.TensorShape([]))),    # size(target)        padding_values=((src_eos_id,  # source vectors padded on the right with src_eos_id                         0),          # size(source) -- unused                        (tgt_eos_id,  # target vectors padded on the right with tgt_eos_id                         0)))         # size(target) -- unused</code></pre><p>从该数据集中发出的值将是嵌套元组，其张量的最左维度为size <code>batch_size</code>。结构如下：</p><ul><li>迭代器<code>[0][0]</code>具有批处理和填充的源语句矩阵。</li><li>迭代器<code>[0][1]</code>具有批处理的源大小向量。</li><li>迭代器<code>[1][0]</code>具有批处理和填充的目标句子矩阵。</li><li>迭代器<code>[1][1]</code>具有批处理的目标大小向量。</li></ul><p>最后，将大小相似的源语句批量放在一起也是可能的。有关详细信息和完整的实现，请参阅文件<a href>utils/iterator_utils.py</a>。</p><p>从数据集中读取数据需要三行代码:创建迭代器、获取其值并初始化它。</p><pre class=" language-py"><code class="language-py">batched_iterator = batched_dataset.make_initializable_iterator ()((source, source_length)， (target, target_length)) = batched_iterator.get_next()</code></pre><pre class=" language-py"><code class="language-py"># At initialization timesession.run (batched_iterator.initializer feed_dict = {…})</code></pre><p>初始化迭代器之后，访问源或目标张量的每个session.run调用都将从底层数据集中请求下一个mini批处理。</p><h1 id="附录-Tensorflow使用技巧"><a href="#附录-Tensorflow使用技巧" class="headerlink" title="附录 Tensorflow使用技巧"></a>附录 Tensorflow使用技巧</h1><h3 id="How-to-build-the-data-Pipeline"><a href="#How-to-build-the-data-Pipeline" class="headerlink" title="How to build the data Pipeline"></a>How to build the data Pipeline</h3><ul><li>学习如何使用tf.data和最佳实践</li><li>建立一个有效的管道来加载图像并对其进行预处理</li><li>为文本构建一个有效的管道，包括如何构建词汇表(build a vocabulary)</li></ul><p>Tensorflow 入门手册中一般介绍的是采用 <code>feed_dict</code>方法，在<code>tf.Session.run()</code> 会话运行或 <code>tf.Tensor.eval()</code> 函数调用时，将数据加载进模型. 然而，还有另一种更加有效和更简单的方式，即，采用 <code>tf.data</code> API，只需几行代码即可实现高效的数据管道(pipelines).</p><p>在 <code>feed_dict</code> 管道中，GPU 存在等待时间，需要等 CPU 提供下一个 batch 的数据. 如图：</p><p><img src="img/feeddict.jpg" alt="feeddict"></p><p>Dataset API允许构建一个<code>异步的、高度优化的数据管道</code>，以防止GPU的数据匮乏。它从磁盘(图像或文本)加载数据，应用优化转换，创建批并将其发送到GPU。以前的数据管道让GPU等待CPU加载数据，导致性能问题。在tf.data管道中，可以异步地拉取下一个batches的数据，以最小化闲置时间，而且还可以并行化数据加载和预处理操作，进一步加速数据管道。</p><p><img src="img/tfdata.jpg" alt="tf.data"></p><h3 id="Tensorflwo中变量管理reuse参数使用"><a href="#Tensorflwo中变量管理reuse参数使用" class="headerlink" title="Tensorflwo中变量管理reuse参数使用"></a>Tensorflwo中变量管理reuse参数使用</h3><p>Tensorflow中两个用于变量管理的函数</p><ul><li>tf.get_variable() ：用于创建和获取变量的值</li><li>tf.variable_scope()：  用于生成上下文管理器，创建命名空间，命名空间可以嵌套<br>其中， tf.get_variable()既可以创建变量，也可以获取变量。控制创建还是获取的开关来自函数tf.variable.scope()中的参数 <code>reuse=True</code> or <code>reuse = Flase</code>。</li></ul><ol><li>设置 reuse = False时，函数 get_variable()表示创建变量<pre class=" language-py"><code class="language-py">with tf.variable_scope("foo", reuse = False):v =  tf.get_variable("v",[1],initializer = tf.constant_initialzier(1.0))</code></pre>在tf.variable_scope()函数中，设置reuse=False(默认reuse=None)时，在其命名空间”foo”中执行函数get_variable（）时，表示创建变量”v”，若在该命名空间中已经有了变量”v”，则在创建时会报错，如下面的例子:</li></ol><pre class=" language-py"><code class="language-py">import tensorflow as tfwith  tf.variable_scope("foo"):    v = tf.get_variable(name="v",shape=[1],initializer=tf.constant_initializer(1.0))    v1 = tf.get_variable(name="v",shape=[1],initializer=tf.constant_initializer(1.0))ValueError: Variable foo/v already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:</code></pre><ol start="2"><li>设置reuse = True 时，函数 get_variable() 表示获取变量<pre class=" language-py"><code class="language-py">import tensorflow as tfwith  tf.variable_scope("foo"): v = tf.get_variable(name="v",shape=[1],initializer=tf.constant_initializer(1.0))with tf.variable_scope("foo",reuse=True): v_test = tf.get_variable("v",shape=[1])print(v_test==v)True</code></pre></li><li>在tf.variable_scope()函数中，设置reuse=True时，在其命名空间”foo”中执行函数get_variable()时，表示获取变量”v”。若在该命名空间中还没有该变量，则在获取时会报错，如下面的例子<pre class=" language-py"><code class="language-py">import tensorflow as tfwith tf.variable_scope("foo",reuse=True): v_test = tf.get_variable("v",shape=[1])print(v_test)ValueError: Variable foo/v does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?</code></pre></li></ol><h1 id="BibTex"><a href="#BibTex" class="headerlink" title="BibTex"></a>BibTex</h1><pre><code>@article{luong17,  author  = {Minh{-}Thang Luong and Eugene Brevdo and Rui Zhao},  title   = {Neural Machine Translation (seq2seq) Tutorial},  journal = {https://github.com/tensorflow/nmt},  year    = {2017},}</code></pre>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> multimodal translation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker Usage</title>
      <link href="/2019/09/12/docker-usage/"/>
      <url>/2019/09/12/docker-usage/</url>
      
        <content type="html"><![CDATA[<h1 id="Docker从入门到放弃"><a href="#Docker从入门到放弃" class="headerlink" title="Docker从入门到放弃"></a>Docker从入门到放弃</h1><hr><blockquote><p>作者: <a href="https://github.com/zongdaoming" target="_blank" rel="noopener">宗道明</a></p></blockquote><p>请参阅<a href="https://tensorflow.google.cn/install/docker" target="_blank" rel="noopener">Tensorflow官方文档</a>和<a href="https://docs.docker.com/engine/reference/run/#clean-up---rm" target="_blank" rel="noopener">Docker官方文档获得详细信息</a>。</p><h2 id="启动-TensorFlow-Docker-容器"><a href="#启动-TensorFlow-Docker-容器" class="headerlink" title="启动 TensorFlow Docker 容器"></a>启动 TensorFlow Docker 容器</h2><p>要启动配置 TensorFlow 的容器，请使用以下命令格式：</p><pre class=" language-py"><code class="language-py">docker run [-it] [--rm] [-p hostPort:containerPort] tensorflow/tensorflow[:tag] [command]</code></pre><p>其中，</p><ul><li><code>docker run</code>是使用一个镜像生成一个运行的容器；</li><li><code>-it</code>指交互模式，启动后终端在运行着的容器里面，与之对应的有-d后端运行模式，启动后终端交互在实体机，要想进入容器需要使用命令<code>docker exec -it container-name bash</code> <code>docker exec -it container-name</code>意为交互模式进入正在运行的一个容器，bash意为进入容器后使用的命令，这里用的是bash，这样进入容器后就能执行shell；</li><li><code>--rm</code>意为退出shell的时候自动删除容器，常在测试的时候使用，这样不用每次修改去删除已有的容器</li><li><code>-p hostport:containerPort</code>指的是端口映射，前面的是实体机的端口，后面是容器里面暴露出的端口，两边端口可以不一样，这样同一个镜像可以启动多个对应不同端口的服务，如<code>-p 8000:4000</code></li><li><code>tensorflow/tensorflow[:tag]</code>表示的是镜像的名字，冒号后面是tag,表示版本，如果不显示给出tag这每次都会从hub上拉取latest的镜像。如基础镜像jupyter常推荐使用的一个版本(jupyter镜像预装了很多常用的模块)：<code>jupyter/datascience-notebook:281505737f8a</code>。 <strong>如果网络环境不好的话比较费时间，推荐显式给出tag，这样每次构建都会使用已有的镜像。</strong></li></ul><p>参看<a href="https://hub.docker.com/r/tensorflow/tensorflow/tags/" target="_blank" rel="noopener"><code>here</code></a>查看每个tensorflow镜像具体的版本。<br>下载tensorflow镜像的命令是：<code>docker pull tensflow/tensorflow:tag</code>,如：</p><ul><li>docker pull tensorflow/tensorflow                     # latest stable release</li><li>docker pull tensorflow/tensorflow:devel-gpu           # nightly dev release w/ GPU support</li><li>docker pull tensorflow/tensorflow:latest-gpu-jupyter  # latest release w/ GPU support and Jupyter</li></ul><h2 id="启用带权限控制的容器"><a href="#启用带权限控制的容器" class="headerlink" title="启用带权限控制的容器"></a>启用带权限控制的容器</h2><p>docker run -d -p 8000:4000 jupyter/datascience-notebook start-notebook.sh –NotebookApp.password=’pwd’</p><h3 id="运行带有目录共享的容器"><a href="#运行带有目录共享的容器" class="headerlink" title="运行带有目录共享的容器"></a>运行带有目录共享的容器</h3><p>docker run -it –rm  -p 8000:4000 -v /home/zongdaoming/cv/GraphSAGE/:/home/zongdaoming/work  jupyter/datascience-notebook start-notebook.sh –NotebookApp.password=’pwd’</p><p>1.密码生成的两种方式 </p><pre class=" language-py"><code class="language-py">import IPythonIPython.lib.passwd()</code></pre><pre class=" language-py"><code class="language-py">ipythonfrom noteboook.auth import passwdpasswd()quit()</code></pre><ol start="2"><li>生成jupyter notebook 的配置文件<br><code>jupyter notebook --generate-config</code> 这时候会生成配置文件 在<code>~/.jupyter/jupyter_notebook_config.py</code>,我们需要修改配置文件<pre class=" language-py"><code class="language-py">vim ~/.jupyter/jupyter_notebook_config.py</code></pre>加入如下内容，其中sha1那一串秘钥是上面生成的那一串<pre class=" language-py"><code class="language-py">c.NotebookApp.ip='*'c.NotebookApp.password = u'sha1:41e4da01dde4:e820dc9c0398eda2dc9323c9e4a51ea1228166a2'c.NotebookApp.open_browser = Falsec.NotebookApp.port =4000</code></pre>4000表明要使用container的4000端口访问jupyter，然后保存退出</li><li>打开jupyter notebook<pre class=" language-py"><code class="language-py">jupyter notebook  --allow-root</code></pre></li></ol><h2 id="带有目录共享的容器"><a href="#带有目录共享的容器" class="headerlink" title="带有目录共享的容器"></a>带有目录共享的容器</h2><p>docker run -it –rm -p 8888:8888 -v /home/zongdaoming/cv/GraphSAGE/notebook:/home/zongdaoming/   jupyter/datascience-notebook start-notebook.sh –NotebookApp.password=’pwd’<br><code>VOLUME (shared filesystems)-v, --volume=[host-src:]container-dest[:&lt;options&gt;]: Bind</code> 如”/home/zongdaoming/cv/GraphSAGE/notebook:/home/zongdaoming/“</p><p>container-dest必须始终是绝对路径，例如/src/docs。host-src可以是绝对路径，也可以是名称值。如果为host-dir提供绝对路径，Docker绑定将挂载到指定的路径。如果提供一个名称，Docker将根据该名称创建一个已命名的卷。 例如，可以为host-src值指定/foo或foo。如果提供/foo值，Docker将创建绑定挂载。如果您提供foo规范，Docker将创建一个命名卷。</p><h2 id="构建Dockerfile"><a href="#构建Dockerfile" class="headerlink" title="构建Dockerfile"></a>构建Dockerfile</h2><p>Dockerfile由多条指令组成，每条指令在编译镜像时执行相应的程序完成某些功能，由指令+参数组成，以逗号分隔，#作为注释起始符，虽说指令不区分大小写，但是一般指令使用大些，参数使用小写。一个例子:</p><pre class=" language-py"><code class="language-py">FROM jupyter/datascience-notebook:281505737f8aMAINTAINER zong "15901768536@163.com"# pytorchRUN conda install pytorch torchvision -c soumith# spotlight(https://github.com/maciejkula/spotlight)RUN conda install -c maciejkula -c soumith spotlight=0.1.2</code></pre><p>其中，</p><ul><li>指令：FROM 功能描述：设置基础镜像 语法：FROM &lt; image&gt;[:&lt; tag&gt; | @&lt; digest&gt;] </li><li>指令：MAINTAINER 功能描述：设置镜像作者 语法：MAINTAINER &lt; name&gt; </li><li>指令：RUN 语法：RUN &lt; command&gt; RUN [“executable”,”param1”,”param2”] 提示：RUN指令会生成容器，在容器中执行脚本，容器使用当前镜像，脚本指令完成后，Docker Daemon会将该容器提交为一个<strong>中间镜像</strong>，供后面的指令使用 </li><li>指令：COPY  功能描述：复制文件到镜像中 语法：COPY &lt; src&gt;… &lt; dest&gt;|[“&lt; src&gt;”,… “&lt; dest&gt;”] 提示：指令逻辑和ADD十分相似，同样Docker Daemon会从编译目录寻找文件或目录，dest为镜像中的绝对路径或者相对于WORKDIR的路径</li></ul><p>从构建的镜像运行容器</p><pre class=" language-python"><code class="language-python">docker build <span class="token operator">-</span>t graphsage<span class="token punctuation">:</span>gpu <span class="token operator">-</span>f Dockerfile<span class="token punctuation">.</span>gpu <span class="token punctuation">.</span>nvidia<span class="token operator">-</span>docker run <span class="token operator">-</span>it graphsage<span class="token punctuation">:</span>gpu bash</code></pre><p>##其他命令</p><ul><li>docker search python # 搜寻镜像</li><li>docker stop container-name/container-id # 停止运行着的容器</li><li>docker rm container-name # 删除已有的容器，要先停止</li><li>docker rmi image-name # 删除已有的镜像</li></ul>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3d_reconstruction</title>
      <link href="/2019/09/12/3d-reconstruction/"/>
      <url>/2019/09/12/3d-reconstruction/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>multimodal neural translation tutorial</title>
      <link href="/2019/09/09/multimodal-neural-translation/"/>
      <url>/2019/09/09/multimodal-neural-translation/</url>
      
        <content type="html"><![CDATA[<!-- # 多模态神经翻译 --导论（01）---## 多模态学习的研究方向 模态可以简单理解为文字信息，视觉信息（文字和视频）和听觉信息（语音）。其主要研究方向可分为四点：* **多模态表示学习**：主要研究如何将多个模态数据所蕴含的语义信息数值化为实值向量。* **模态间映射** 主要研究如何将某一特定模态数据中的信息映射至另一模态。* **对齐** 主要研究如何识别不同模态之间的部件、元素的对应关系。* **融合** 主要研究如何整合不同模态间的模型与特征。* **协同学习** 主要研究如何将信息富集的模态上学习的知识迁移到信息匮乏的模态，使各个模态的学习互相辅助。典型的方法包括多模态的零样本学习、领域自适应等。### --><h1 id="聊天机器人的实现"><a href="#聊天机器人的实现" class="headerlink" title="聊天机器人的实现"></a>聊天机器人的实现</h1><blockquote><p><strong>作者</strong>: <a href="https://github.com/MatthewInkawhich" target="_blank" rel="noopener">Matthew Inkawhich</a></p><p>译者: <a href="https://github.com/zongdaoming" target="_blank" rel="noopener">宗道明</a></p></blockquote><!-- 在本教程中，我们探索了一个好玩和有趣的循环序列到序列的模型用例。我们将用 [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) 处的电影剧本来训练一个简单的聊天机器人。在人工智能研究领域中对话模型是一个非常热门的话题。聊天机器人可以在各种设置中找到，包括客户服务应用和在线帮助。这些机器人通常由基于检索的模型提供支持，这些输出是某些形式问题预先定义的响应。在像公司IT服务台这样高度受限制的领域中，这些模型可能足够了，但是，对于更一般的用例它们不够健壮。教一台机器与多领域的人进行有意义的对话是一个远未解决的研究问题。最近，深度学习热潮已经允许强大的生成模型，如谷歌的神经对话模型[Neural Conversational Model](https://arxiv.org/abs/1506.05869)，这标志着向多领域生成对话模型迈出了一大步。 在本教程中，我们将在PyTorch中实现这种模型。 --><p>上个学期仔细看了一篇<strong>神经对话模型</strong>的Pytorch实现，踩了一些小坑，现记录如下。通过本教程，你可以快速实现一个聊天机器人并掌握Pytorch的一些入门技巧。可在谷歌提供的 <a href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/chatbot_tutorial.ipynb" target="_blank" rel="noopener">Google Colab</a> 训练此模型。</p><p><img src="img/encdec.jpg" alt="bot"></p><pre class=" language-py"><code class="language-py">> hello?Bot: hello .> where am I?Bot: you re in a hospital .> who are you?Bot: i m a lawyer .> how are you doing?Bot: i m fine .> are you my friend?Bot: no .> you're under arrestBot: i m trying to help you !> i'm just kiddingBot: i m sorry .> where are you from?Bot: san francisco .> it's time for me to leaveBot: i know .> goodbyeBot: goodbye .</code></pre><p><strong>Highlights</strong></p><ul><li>数据集的加载和预处理 (这是一段标准的NLP处理程序)</li><li>用 <a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="noopener">Luong attention mechanism(s)</a> 实现一个sequence-to-sequence模型</li><li>使用小批量数据联合训练解码器和编码器模型</li><li>实现贪婪搜索解码模块</li><li>学习Pytorch的基础技巧</li></ul><p><strong>鸣谢</strong></p><p>本教程借鉴以下源码：</p><ol><li>Yuan-Kuei Wu’s pytorch-chatbot implementation: <a href="https://github.com/ywk991112/pytorch-chatbot" target="_blank" rel="noopener">https://github.com/ywk991112/pytorch-chatbot</a></li><li>Sean Robertson’s practical-pytorch seq2seq-translation example: <a href="https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation" target="_blank" rel="noopener">https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation</a></li><li>FloydHub’s Cornell Movie Corpus preprocessing code: <a href="https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus" target="_blank" rel="noopener">https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus</a></li></ol><h2 id="Preparations"><a href="#Preparations" class="headerlink" title="Preparations"></a>Preparations</h2><p>首先，下载数据文件 <a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html" target="_blank" rel="noopener">Cornell Movie-Dialogs Corpus</a> 并将其放入当前目录下的<code>data/</code>文件夹下。之后，让我们引入一些必须的包。</p><pre class=" language-py"><code class="language-py">from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionfrom __future__ import unicode_literalsimport torchfrom torch.jit import script, traceimport torch.nn as nnfrom torch import optimimport torch.nn.functional as Fimport csvimport randomimport reimport osimport unicodedataimport codecsfrom io import openimport itertoolsimport mathUSE_CUDA = torch.cuda.is_available()device = torch.device("cuda" if USE_CUDA else "cpu")# specify gpu # os.environ["CUDA_VISIBLE_DEVICES"]="0,..."</code></pre><h2 id="下载和预处理数据"><a href="#下载和预处理数据" class="headerlink" title="下载和预处理数据"></a>下载和预处理数据</h2><p><a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html" target="_blank" rel="noopener">Cornell Movie-Dialogs Corpus</a>[size=9.5M] 是一个丰富的电影角色对话数据集：</p><ul><li>10,292 对电影角色的220,579 次对话</li><li>617部电影中的9,035电影角色</li><li>总共304,713中语调</li></ul><p>这个数据集庞大而多样，在语言形式、时间段、情感上等都有很大的变化。我们希望这种多样性使我们的模型能够适应多种形式的输入和查询。</p><p>首先，把数据集下载并解压到Google云盘,在colab中挂在google云盘：</p><pre class=" language-py"><code class="language-py">from google.colab import drivedrive.mount('/content/drive')</code></pre><p>然后,我们通过数据文件的某些行来查看原始数据的格式</p><pre class=" language-py"><code class="language-py"># corpus_name = "cornell movie-dialogs corpus"# corpus = os.path.join("data", corpus_name)corpus = "/content/drive/My Drive/Colab Notebooks"def preview(file,n=10):  with open(file,'rb') as f:    lines = f.readlines()  for line in lines[:n]:    print(line)preview(os.path.join(corpus, "movie_lines.txt"))</code></pre><p>输出:</p><pre class=" language-py"><code class="language-py">b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n'b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n'b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n'b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n'b"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n"b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n'b"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\n"b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n'b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\'m kidding.  You know how sometimes you just become this "persona"?  And you don\'t know how to quit?\n'b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\n'</code></pre><pre class=" language-py"><code class="language-py"># corpus_name = "cornell movie-dialogs corpus"# corpus = os.path.join("data", corpus_name)corpus = "/content/drive/My Drive/Colab Notebooks"def preview(file,n=10):  with open(file,'rb') as f:    lines = f.readlines()  for line in lines[:n]:    print(line)preview(os.path.join(corpus, "movie_conversations.txt"))</code></pre><p>输出：</p><pre class=" language-py"><code class="language-py">b"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n"b"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n"b"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n"b"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\n"b"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\n"b"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\n"b"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\n"b"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\n"b"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\n"b"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\n"</code></pre><h3 id="创建格式化数据文件"><a href="#创建格式化数据文件" class="headerlink" title="创建格式化数据文件"></a>创建格式化数据文件</h3><p>为了方便起见，我们将创建一个格式良好的数据文件，其中每一行包含一个由 <code>tab</code> 制表符分隔的查询语句和响应语句对。直接运行google提供的源码可能会出错，这里已经修补了一个小bug。</p><p>以下三个函数便于解析原始 movie_lines.txt 数据文件。</p><ul><li><code>loadLines</code> 将文件的每一行拆分为字段(lineID, characterID, movieID, character, text)组合的字典 </li><li><code>loadConversations</code> 根据 movie_conversations.txt 将 <code>loadLines</code> 中的每一行数据进行归类</li><li><code>extractSentencePairs</code> 从对话中提取一对句子</li></ul><pre class=" language-py"><code class="language-py"># fields = ["lineID", "characterID", "movieID", "character", "text"]def loadLines(fileName, fields):    lines = {}    with open(fileName, 'r', encoding='iso-8859-1') as f:        for line in f:            values = line.split(" +++$+++ ")            # Extract fields            lineObj = {}            for i, field in enumerate(fields):                lineObj[field] = values[i]            lines[lineObj['lineID']] = lineObj    return lines# 将文件的每一行拆分为字段字典# line = {#     'L183198': {#         'lineID': 'L183198', #         'characterID': 'u5022', #         'movieID': 'm333', #         'character': 'FRANKIE', #         'text': "Well we'd sure like to help you.\n"#     }, {...}# }# lines为loadLines()提取文本之后的字典, fields == ["character1ID", "character2ID", "movieID", "utteranceIDs"]def loadConversations(fileName, lines, fields):    conversations = []    with open(fileName, 'r', encoding='iso-8859-1') as f:        for line in f:            values = line.split(" +++$+++ ")            # Extract fields            convObj = {}            for i, field in enumerate(fields):                convObj[field] = values[i]            # Convert string to list (convObj["utteranceIDs"] == "['L598485', 'L598486', ...]")            # eval() 函数用来执行一个字符串表达式，并返回表达式的值 x=7,eval('3*x')=21            lineIds = eval(convObj["utteranceIDs"])            # Reassemble lines            # 多加了lines这一个key 把 utteranceIDs对应的 lines加载进来            convObj["lines"] = []            for lineId in lineIds:              if lines.get(lineId) is not None:                convObj["lines"].append(lines[lineId])nv            conversations.append(convObj)    return conversations# 将 `loadLines` 中的行字段分组为基于 *movie_conversations.txt* 的对话# [{#     'character1ID': 'u0',#     'character2ID': 'u2',#     'movieID': 'm0',#     'utteranceIDs': "['L194', 'L195', 'L196', 'L197']\n",#     'lines': [{#         'lineID': 'L194',#         'characterID': 'u0',#         'movieID': 'm0',#         'character': 'BIANCA',#         'text': 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n'#     }, {#         'lineID': 'L195',#         'characterID': 'u2',#         'movieID': 'm0',#         'character': 'CAMERON',#         'text': "Well, I thought we'd start with pronunciation, if that's okay with you.\n"#     }, {#         'lineID': 'L196',#         'characterID': 'u0',#         'movieID': 'm0',#         'character': 'BIANCA',#         'text': 'Not the hacking and gagging and spitting part.  Please.\n'#     }, {#         'lineID': 'L197',#         'characterID': 'u2',#         'movieID': 'm0',#         'character': 'CAMERON',#         'text': "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n"#     }]# }, {...}]# 从对话中提取一对句子def extractSentencePairs(conversations):    qa_pairs = []    for conversation in conversations:        # Iterate over all the lines of the conversation        for i in range(len(conversation["lines"]) - 1):  # We ignore the last line (no answer for it)            inputLine = conversation["lines"][i]["text"].strip()            targetLine = conversation["lines"][i+1]["text"].strip()            # Filter wrong samples (if one of the lists is empty)            if inputLine and targetLine:                qa_pairs.append([inputLine, targetLine])    return qa_pairs</code></pre><p>现在我们将调用这些函数来创建文件，我们命名为 <em>formatted_movie_lines.txt</em>.</p><pre class=" language-py"><code class="language-py"># Define path to new filedatafile = os.path.join(corpus, "formatted_movie_lines.txt")delimiter = '\t'# Unescape the delimiterdelimiter = str(codecs.decode(delimiter, "unicode_escape"))# Initialize lines dict, conversations list, and field idslines = {}conversations = []MOVIE_LINES_FIELDS = ["lineID", "characterID", "movieID", "character", "text"]MOVIE_CONVERSATIONS_FIELDS = ["character1ID", "character2ID", "movieID", "utteranceIDs"]# Load lines and process conversationsprint("\nProcessing corpus...")lines = loadLines(os.path.join(corpus, "movie_lines.txt"), MOVIE_LINES_FIELDS)print("\nLoading conversations...")conversations = loadConversations(os.path.join(corpus, "movie_conversations.txt"),                                  lines, MOVIE_CONVERSATIONS_FIELDS)# Write new csv fileprint("\nWriting newly formatted file...")with open(datafile, 'w', encoding='utf-8') as outputfile:    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\n')    for pair in extractSentencePairs(conversations):        writer.writerow(pair)# Print a sample of linesprint("\nSample lines from file:")preview(datafile)</code></pre><p>输出:</p><pre class=" language-py"><code class="language-py">Processing corpus...Loading conversations...Writing newly formatted file...Sample lines from file:b"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\tWell, I thought we'd start with pronunciation, if that's okay with you.\r\n"b"Well, I thought we'd start with pronunciation, if that's okay with you.\tNot the hacking and gagging and spitting part.  Please.\r\n"b"Not the hacking and gagging and spitting part.  Please.\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\r\n"b"You're asking me out.  That's so cute. What's your name again?\tForget it.\r\n"b"No, no, it's my fault -- we didn't have a proper introduction ---\tCameron.\r\n"b"Cameron.\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\r\n"b"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\tSeems like she could get a date easy enough...\r\n"b'Why?\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\r\n'b"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\tThat's a shame.\r\n"b'Gosh, if only we could find Kat a boyfriend...\tLet me see what I can do.\r\n'</code></pre><h3 id="加载和清洗数据"><a href="#加载和清洗数据" class="headerlink" title="加载和清洗数据"></a>加载和清洗数据</h3><p>我们下一个任务是创建词汇表并将查询/响应句子对（对话）加载到内存。</p><p>注意我们正在处理<strong>词序</strong>，这些词序没有映射到离散数值空间。因此，我们必须通过数据集中的单词来创建一个索引。</p><p>为此我们创建了一个<code>Voc</code>类,它会存储从单词到索引的映射、索引到单词的反向映射、每个单词的计数和总单词量。这个类提供向词汇表中添加单词的方法(<code>addWord</code>)、添加所有单词到句子中的方法 (<code>addSentence</code>) 和清洗不常见的单词方法(<code>trim</code>)。更多的数据清洗在后面进行。</p><pre class=" language-py"><code class="language-py"># Default word tokensPAD_token = 0  # Used for padding short sentencesSOS_token = 1  # Start-of-sentence tokenEOS_token = 2  # End-of-sentence tokenclass Voc:    def __init__(self, name):        self.name = name        self.trimmed = False        self.word2index = {}        self.word2count = {}        self.index2word = {PAD_token: "PAD", SOS_token: "SOS", EOS_token: "EOS"}        self.num_words = 3  # Count SOS, EOS, PAD    def addSentence(self, sentence):        for word in sentence.split(' '):            self.addWord(word)    def addWord(self, word):        if word not in self.word2index:            self.word2index[word] = self.num_words            self.word2count[word] = 1            self.index2word[self.num_words] = word            self.num_words += 1        else:            self.word2count[word] += 1    # 删除低于特定计数阈值的单词    def trim(self, min_count):        if self.trimmed:            return        self.trimmed = True        keep_words = []        for k, v in self.word2count.items():            if v >= min_count:                keep_words.append(k)        print('keep_words {} / {} = {:.4f}'.format(            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)        ))        # Reinitialize dictionaries        self.word2index = {}        self.word2count = {}        self.index2word = {PAD_token: "PAD", SOS_token: "SOS", EOS_token: "EOS"}        self.num_words = 3 # Count default tokens        for word in keep_words:            self.addWord(word)</code></pre><p>现在我们可以组装词汇表和查询/响应语句对。在使用数据之前，我们必须做一些预处理。</p><p>首先，我们必须使用<code>unicodeToAscii</code>将unicode字符串转换为ASCII。然后，我们应该将所有字母转换为小写字母并清洗掉除基本标点之外的所有非字母字符 (<code>normalizeString</code>)。最后，为了帮助训练收敛，我们将过滤掉长度大于<code>MAX_LENGTH</code> 的句子 (<code>filterPairs</code>)。</p><pre class=" language-py"><code class="language-py">MAX_LENGTH = 10  # Maximum sentence length to consider# Turn a Unicode string to plain ASCII, thanks to# https://stackoverflow.com/a/518232/2809427def unicodeToAscii(s):    return ''.join(        c for c in unicodedata.normalize('NFD', s)        if unicodedata.category(c) != 'Mn'    )# 初始化Voc对象 和 格式化pairs对话存放到list中def readVocs(datafile, corpus_name):    print("Reading lines...")    # Read the file and split into lines    lines = open(datafile, encoding='utf-8').read().strip().split('\n')    # Split every line into pairs and normalize    pairs = [[normalizeString(s) for s in l.split('\t')] for l in lines]    voc = Voc(corpus_name)    return voc, pairs# 如果对 'p' 中的两个句子都低于 MAX_LENGTH 阈值，则返回Truedef filterPair(p):    # Input sequences need to preserve the last word for EOS token    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH# 过滤满足条件的 pairs 对话def filterPairs(pairs):    return [pair for pair in pairs if filterPair(pair)]# 使用上面定义的函数，返回一个填充的voc对象和对列表def loadPrepareData(corpus, datafile, save_dir):    print("Start preparing training data ...")    voc, pairs = readVocs(datafile, corpus)    print("Read {!s} sentence pairs".format(len(pairs)))    pairs = filterPairs(pairs)    print("Trimmed to {!s} sentence pairs".format(len(pairs)))    print("Counting words...")    for pair in pairs:        voc.addSentence(pair[0])        voc.addSentence(pair[1])    print("Counted words:", voc.num_words)    return voc, pairs# Load/Assemble voc and pairssave_dir = os.path.join("data", "save")voc, pairs = loadPrepareData(corpus, corpus, datafile, save_dir)# Print some pairs to validateprint("\npairs:")for pair in pairs[:10]:    print(pair)</code></pre><p>输出：</p><pre class=" language-py"><code class="language-py">Start preparing training data ...Reading lines...Read 221282 sentence pairsTrimmed to 64271 sentence pairsCounting words...Counted words: 18008pairs:['there .', 'where ?']['you have my word . as a gentleman', 'you re sweet .']['hi .', 'looks like things worked out tonight huh ?']['you know chastity ?', 'i believe we share an art instructor']['have fun tonight ?', 'tons']['well no . . .', 'then that s all you had to say .']['then that s all you had to say .', 'but']['but', 'you always been this selfish ?']['do you listen to this crap ?', 'what crap ?']['what good stuff ?', 'the real you .']</code></pre><p>另一种有利于让训练更快收敛的策略是去除词汇表中很少使用的单词。减少特征空间也会降低模型学习目标函数的难度。我们通过以下两个步骤完成这个操作:</p><ol><li>使用 <code>voc.trim</code> 函数去除 <code>MIN_COUNT</code> 阈值以下单词 。</li><li>如果句子中包含词频过小的单词，那么整个句子也被过滤掉。</li></ol><pre class=" language-py"><code class="language-py">MIN_COUNT = 3    # Minimum word count threshold for trimmingdef trimRareWords(voc, pairs, MIN_COUNT):    # Trim words used under the MIN_COUNT from the voc    voc.trim(MIN_COUNT)    # Filter out pairs with trimmed words    keep_pairs = []    for pair in pairs:        input_sentence = pair[0]        output_sentence = pair[1]        keep_input = True        keep_output = True        # Check input sentence        for word in input_sentence.split(' '):            if word not in voc.word2index:                keep_input = False                break        # Check output sentence        for word in output_sentence.split(' '):            if word not in voc.word2index:                keep_output = False                break        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence        if keep_input and keep_output:            keep_pairs.append(pair)    print("Trimmed from {} pairs to {}, {:.4f} of total".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))    return keep_pairs# Trim voc and pairspairs = trimRareWords(voc, pairs, MIN_COUNT)</code></pre><p>输出:</p><pre class=" language-py"><code class="language-py">keep_words 7823 / 18005 = 0.4345Trimmed from 64271 pairs to 53165, 0.8272 of total</code></pre><h2 id="为模型准备数据"><a href="#为模型准备数据" class="headerlink" title="为模型准备数据"></a>为模型准备数据</h2><p>尽管我们已经投入了大量精力来准备和清洗我们的数据变成一个很好的词汇对象和一系列的句子对，但我们的模型最终希望以numerical torch 张量作为输入。 可以在 <a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" target="_blank" rel="noopener">seq2seq translation tutorial</a> 中找到为模型准备处理数据的一种方法。 在该教程中，我们使用batch size 大小为1，这意味着我们所要做的就是将句子对中的单词转换为词汇表中的相应索引，并将其提供给模型。</p><p>但是，如果你想要加速训练或者想要利用GPU并行计算能力，则需要使用小批量 <code>mini-batches</code> 来训练。</p><p>使用小批量 <code>mini-batches</code> 也意味着我们必须注意批量处理中句子长度的变化。 为了容纳同一批次中不同大小的句子，我们将使我们的批量输入张量大小 <em>(max_length，batch_size)</em>，其中短于 <em>max_length</em> 的句子在 <em>EOS_token</em> 之后进行零填充（zero padded）。</p><p><strong>如果我们简单地通过将单词转换为索引 <code>indicesFromSentence</code> 和零填充 <code>zero-pad</code> 将我们的英文句子转换为张量，我们的张量将具有大小 <code>(batch_size，max_length)</code>，并且索引第一维将在所有时间步骤中返回完整序列。 但是，我们需要沿着时间对我们批量数据进行索引并且包括批量数据中所有序列。 因此，我们将输入批处理大小转换为 <code>(max_length，batch_size)</code>，以便跨第一维的索引返回批处理中所有句子的时间步长。 我们在 <code>zeroPadding</code> 函数中隐式处理这个转置。</strong></p><p><img src="./img/maxlength_batchsize.jpg" alt="batches"></p><p><code>inputvar</code> 函数处理将句子转换为张量的过程，最终创建正确大小的零填充张量。它还返回批处理中每个序列的长度张量 <code>(tensor of lengths)</code>，长度张量稍后将传递给我们的解码器。</p><p><code>outputvar</code> 函数执行与 <code>inputvar</code> 类似的函数，但他不返回长度张量，而是返回二进制 <code>mask tensor</code> 和最大目标句子长度。二进制 <code>mask tensor</code> 的大小与输出目标张量的大小相同，但作为 <em>PAD_token</em> 的每个元素都是0而其他元素都是1。</p><p><code>batch2traindata</code> 只需要取一批句子对，并使用上述函数返回输入张量和目标张量。</p><pre class=" language-py"><code class="language-py">def indexesFromSentence(voc, sentence):    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]# zip 对数据进行合并了，相当于行列转置了def zeroPadding(l, fillvalue=PAD_token):    return list(itertools.zip_longest(*l, fillvalue=fillvalue))# 记录 PAD_token的位置为0， 其他的为1def binaryMatrix(l, value=PAD_token):    m = []    for i, seq in enumerate(l):        m.append([])        for token in seq:            if token == PAD_token:                m[i].append(0)            else:                m[i].append(1)    return m# 返回填充前（加入结束index EOS_token做标记）的长度 和 填充后的输入序列张量def inputVar(l, voc):    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])    padList = zeroPadding(indexes_batch)    padVar = torch.LongTensor(padList)    return padVar, lengths# 返回填充前（加入结束index EOS_token做标记）最长的一个长度 和 填充后的输入序列张量, 和 填充后的标记 maskdef outputVar(l, voc):    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]    max_target_len = max([len(indexes) for indexes in indexes_batch])    padList = zeroPadding(indexes_batch)    mask = binaryMatrix(padList)    mask = torch.ByteTensor(mask)    padVar = torch.LongTensor(padList)    return padVar, mask, max_target_len# Returns all items for a given batch of pairsdef batch2TrainData(voc, pair_batch):    pair_batch.sort(key=lambda x: len(x[0].split(" ")), reverse=True)    input_batch, output_batch = [], []    for pair in pair_batch:        input_batch.append(pair[0])        output_batch.append(pair[1])    inp, lengths = inputVar(input_batch, voc)    output, mask, max_target_len = outputVar(output_batch, voc)    return inp, lengths, output, mask, max_target_len# Example for validationsmall_batch_size = 5batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])input_variable, lengths, target_variable, mask, max_target_len = batchesprint("input_variable:", input_variable)print("lengths:", lengths)print("target_variable:", target_variable)print("mask:", mask)print("max_target_len:", max_target_len)</code></pre><p>输出：</p><pre class=" language-py"><code class="language-py">input_variable: tensor([[  33,   50, 1134,  274,   34],        [  50,   92,   25,    4,    4],        [  47,    7,  148,    2,    2],        [   7,  278,  356,    0,    0],        [ 118,    6,   40,    0,    0],        [  40,    2,    2,    0,    0],        [  47,    0,    0,    0,    0],        [   6,    0,    0,    0,    0],        [   2,    0,    0,    0,    0]])lengths: tensor([9, 6, 6, 3, 3])target_variable: tensor([[  25,   45,   27,  147,  145],        [  74,  201,  132,  582,    6],        [  25, 2009,  385,    7,    2],        [  89,  115,  188,   44,    0],        [7048,  180,   53,  188,    0],        [ 350, 3524, 7558,   76,    0],        [   4, 1295,    2,    6,    0],        [   2,    4,    0,    2,    0],        [   0,    2,    0,    0,    0]])mask: tensor([[1, 1, 1, 1, 1],        [1, 1, 1, 1, 1],        [1, 1, 1, 1, 1],        [1, 1, 1, 1, 0],        [1, 1, 1, 1, 0],        [1, 1, 1, 1, 0],        [1, 1, 1, 1, 0],        [1, 1, 0, 1, 0],        [0, 1, 0, 0, 0]], dtype=torch.uint8)max_target_len: 9</code></pre><h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><h3 id="Seq2Seq模型"><a href="#Seq2Seq模型" class="headerlink" title="Seq2Seq模型"></a>Seq2Seq模型</h3><p>我们聊天机器人的大脑是序列到序列（seq2seq）模型。 seq2seq模型的目标是将可变长度序列作为输入，并使用固定大小的模型将可变长度序列作为输出返回。</p><p><a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="noopener">Sutskever et al.</a> 发现通过一起使用两个独立的RNN，我们可以完成这项任务。 第一个RNN充当<strong>编码器</strong>，其将可变长度输入序列编码为固定长度上下文向量。 理论上，该上下文向量（RNN的最终隐藏层）将包含关于输入到机器人的查询语句的语义信息。 第二个RNN是一个<strong>解码器</strong>，它接收输入文字和上下文矢量，并返回序列中下一句文字的概率和在下一次迭代中使用的隐藏状态。</p><p><img src="img/encoder_decoder.jpg" alt="model"></p><p>图片来源: <a href="https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/" target="_blank" rel="noopener">https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/</a></p><h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>编码器RNN每次迭代中输入一个语句输出一个token（例如，一个单词），同时在这时间内输出“输出”向量和“隐藏状态”向量。 然后将隐藏状态向量传递到下一步，并记录输出向量。 编码器将其在序列中的每一点处看到的上下文转换为高维空间中的一系列点，解码器将使用这些点为给定任务生成有意义的输出。</p><p>我们的编码器的核心是由  <a href="https://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Cho et al.</a> 等人发明的多层门循环单元。 在2014年，我们将使用GRU的双向变体，这意味着基本上有两个独立的RNN：一个以正常的顺序输入输入序列，另一个以相反的顺序输入输入序列。 每个网络的输出在每个时间步骤求和。 使用双向GRU将为我们提供编码过去和未来上下文的优势。</p><p>双向RNN：</p><p><a href="https://pytorch.org/tutorials/_images/RNN-bidirectional.png" target="_blank" rel="noopener"><img src="img/gru_bidirection.jpg" alt="rnn_bidir"></a></p><p>图片来源: <a href="https://colah.github.io/posts/2015-09-NN-Types-FP/" target="_blank" rel="noopener">https://colah.github.io/posts/2015-09-NN-Types-FP/</a></p><p>注意:<code>embedding</code>层用于在任意大小的特征空间中对我们的单词索引进行编码。 对于我们的模型，此图层会将每个单词映射到大小为<em>hidden_size</em>的特征空间。 训练后，这些值会被编码成和他们相似的有意义词语。</p><p>最后，如果将填充的一批序列传递给RNN模块，我们必须分别使用<code>torch.nn.utils.rnn.pack_padded_sequence</code>和<code>torch.nn.utils.rnn.pad_packed_sequence</code>在RNN传递时分别进行填充和反填充。这两个函数的具体功能是为了防止LSTM或者GRU前向传播时吧加入在训练数据的padding考虑进去，理论上可以不用，但是为了提高效率最好还是用。</p><h3 id="回顾一下两个函数"><a href="#回顾一下两个函数" class="headerlink" title="回顾一下两个函数"></a>回顾一下两个函数</h3><hr><p><code>pack_padded_sequence</code> 有三个参数：(input,lengths,batch_first)。<code>input</code>是上一步加padding的数据，length是各个sequence的实际长度，batch_first是数据各个dimension按照<code>[batch_size,sequence_legnth,data_dim]</code>顺序排列。 One example:<br>一个<code>lengths=[7,5,2]</code>的 mini-batch <code>batch_x</code>为：</p><pre class=" language-py"><code class="language-py">batch_xOut[2]: tensor([[1, 1, 1, 1, 1, 1, 1],        [3, 3, 3, 3, 3, 0, 0],        [6, 6, 0, 0, 0, 0, 0]])</code></pre><p>经过<code>torch.nn.utils.pack_padded_sequence</code>压缩后得到<code>batch_x_pack</code>：</p><pre class=" language-py"><code class="language-py">rnn_utils.pack_padded_sequence(batch_x, [7,5,2], batch_first=True)Out[3]: PackedSequence(data=tensor([1., 3., 6., 1., 3., 6., 1., 3., 1., 3., 1., 3., 1., 1.]),batch_sizes=tensor([3, 3, 2, 2, 2, 1, 1]))</code></pre><p>分析可以发现，它的输出有两部分组成，分别是data和batch_sizes,第一部分为原来的数据按照time step重新排列，而padding的部分，直接空过了。batch_sizes则是每次实际读入的数据量，也就是说吗，LSTM把一个mini-batch sequence 又重新划分了很多小的batch,每个小batch为所有sequence在当前time step对应的值，如果某sequence在当前time step已经没有值了，那么，就不再读入填充的0，而是降低batch_size。batch_size相当于是对训练数据的重新划分。体会一下传入<code>DataLoader</code> 中 <code>collate_fn</code> 的精妙之处：</p><pre class=" language-py"><code class="language-py">def clollate_fn(data):    data.sort(key=lambda x: len(x),reverse=True)    data_length = [len(sq) for sq in data]    data = rnn_utils.pad_sequence(data,batch_first=True,padding_value=0)    return data.unsqueeze(-1), data_length</code></pre><p>首先，排序的目的是为了方便获取每个time_step的batch,防止中间夹杂着<code>padding</code>.在 <code>collate_fn</code> 函数中，我们还用到了<code>run.utils.pad_sequence</code>函数，它的作用就是填充0，如给定一个原始的<strong>排序</strong>后输入序列 data_x</p><pre><code>tensor([[1, 1, 1, 1, 1, 1, 1],        [3, 3, 3, 3, 3],        [6, 6]        ...,        ])</code></pre><p>输入到<code>rnn_utils.pad_sequence(data,batch_first=True,padding_value=0)</code>之后的输出为：</p><pre><code>tensor([[1, 1, 1, 1, 1, 1, 1],        [3, 3, 3, 3, 3, 0, 0],        [6, 6, 0, 0, 0, 0, 0]        ...,        ])</code></pre><p>一般来说，经过LSTM后的<code>out</code>和<code>batch_x_pack</code>的一样，都是分为两部分：<code>data</code>和<code>batch_sizes</code>。其中<code>out</code>的data shape由原来的 <code>input_size (data_dim)</code>变成了<code>hidden size</code>, <code>batch_size</code>的shape保持一致。One example:</p><pre class=" language-py"><code class="language-py">out.data.shapeOut[5]: torch.Size([14, 10])batch_x_pack.data.shapeOut[6]: torch.Size([14, 1])out.batch_sizesOut[7]: tensor([3, 3, 2, 2, 2, 1, 1])batch_x_pack.batch_sizesOut[8]: tensor([3, 3, 2, 2, 2, 1, 1])</code></pre><p>将 <code>out</code>输入到 <code>pad_packed_sequence</code>执行的是<code>pack_padded_sequence</code>的逆操作：</p><pre class=" language-py"><code class="language-py">out, (h1, c1) = net(batch_x_pack, (h0, c0))out_pad, out_len = rnn_utils.pad_packed_sequence(out, batch_first=True)out_pad.shapeOut[2]: torch.Size([3, 7, 10])out.data.shapeOut[3]: torch.Size([14, 10])out_lenOut[4]: tensor([7, 5, 2])</code></pre><p>我们发现，经过这样的操作之后，得到的<code>out_pad</code>的shape为<code>[3,7,10]</code>,对比 <code>batch_x</code>的形状<code>[3,7,1]</code>，这说明一个minibatch中有三个句子，每个句子有7个time step, 每个 time_step 数据从输入的1维映射成了LSTM的10维。 out_len的shape为[7,5,2]。</p><hr><p><strong>计算图:</strong></p><blockquote><ol><li>将单词索引转换为词嵌入 embeddings。</li><li>为RNN模块打包填充批次序列。</li><li>通过GRU进行前向传播。</li><li>反填充。</li><li>对双向GRU输出求和。</li><li>返回输出和最终隐藏状态。</li></ol></blockquote><p><strong>输入:</strong></p><ul><li><code>input_seq</code>：一批输入句子; shape =（<em>max_length，batch_size</em>）</li><li><code>input_lengths</code>：一批次中每个句子对应的句子长度列表;shape=(<em>batch_size</em>)</li><li><code>hidden:</code>隐藏状态; shape =(<em>n_layers x num_directions，batch_size，hidden_size</em>)</li></ul><p><strong>输出:</strong></p><ul><li><code>outputs：</code> GRU最后一个隐藏层的输出特征（双向输出之和）; shape =（<em>max_length，batch_size，hidden_size</em>）</li><li><code>hidden：</code> GRU更新隐藏状态; shape =（<em>n_layers x num_directions，batch_size，hidden_size</em>）</li></ul><pre class=" language-py"><code class="language-py">class EncoderRNN(nn.Module):    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):        super(EncoderRNN, self).__init__()        self.n_layers = n_layers        self.hidden_size = hidden_size        self.embedding = embedding        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'        #   because our input size is a word embedding with number of features == hidden_size        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)    def forward(self, input_seq, input_lengths, hidden=None):        # Convert word indexes to embeddings        embedded = self.embedding(input_seq)        # Pack padded batch of sequences for RNN module        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)        # Forward pass through GRU        outputs, hidden = self.gru(packed, hidden)        # Unpack padding        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)        # Sum bidirectional GRU outputs        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]        # Return output and final hidden state        return outputs, hidden</code></pre><h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>解码器RNN以token-by-token的方式生成响应语句。 它使用编码器的上下文向量和内部隐藏状态来生成序列中的下一个单词。 它持续生成单词，直到输出是<em>EOS_token</em>，这个表示句子的结尾。 一个vanilla seq2seq解码器的常见问题是，如果我们只依赖于上下文向量来编码整个输入序列的含义，那么我们很可能会丢失信息。尤其是在处理长输入序列时，这极大地限制了我们的解码器的能力。</p><p>为了解决这个问题,<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Bahdanau et al.</a> 等人创建了一种“attention mechanism”，允许解码器关注输入序列的某些部分，而不是在每一步都使用完全固定的上下文。模型首先预测当前目标词的单个对齐位置Aliged Position $p_{t}$。 然后使用以源位置$p_{t}$为中心的窗口来计算上下文向量$c_{t}$，即窗口中源隐状态的加权平均值。 权重$\alpha_{t}$是从当前目标状态(current target state)$h_{t}$和窗口中的那些源状态$\bar{h}_{s}$中推断出来的。<br><img src="img/local_attention.png" alt="attn2"></p><p><a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="noopener">Luong et al.</a> 通过创造“Global attention”，改善了<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Bahdanau et al.</a> 的基础工作。 关键的区别在于，对于“Global attention”，我们考虑所有编码器的隐藏状态，而不是Bahdanau等人的“Local attention”，它只考虑当前步中编码器的隐藏状态。 另一个区别在于，通过“Global attention”，我们仅使用当前步的解码器的隐藏状态来计算注意力权重（或者能量）。 Bahdanau等人的注意力计算需要知道前一步中解码器的状态。 此外，Luong等人提供各种方法来计算编码器输出和解码器输出之间的注意权重（能量），称之为“score functions”：</p><p><a href="https://pytorch.org/tutorials/_images/scores.png" target="_blank" rel="noopener"><img src="img/score_function.png" alt="scores"></a></p><p>其中 $h_t = 当前目标解码器状态$，$\bar{h}_s= 所有编码器状态$。</p><p>总体而言，Global attention机制可以通过下图进行总结。 请注意，我们将“Attention Layer”用一个名为 <code>Attn</code> 的 <code>nn.Module</code> 来单独实现。 该模块的输出是经过softmax标准化后权重张量的大小（<em>batch_size，1，max_length</em>）。</p><p><a href="https://pytorch.org/tutorials/_images/global_attn.png" target="_blank" rel="noopener"><img src="img/global_attention.png" alt="global_attn"></a></p><pre class=" language-py"><code class="language-py"># Luong attention layerclass Attn(torch.nn.Module):    def __init__(self, method, hidden_size):        super(Attn, self).__init__()        self.method = method        if self.method not in ['dot', 'general', 'concat']:            raise ValueError(self.method, "is not an appropriate attention method.")        self.hidden_size = hidden_size        if self.method == 'general':            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)        elif self.method == 'concat':            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))    def dot_score(self, hidden, encoder_output):        return torch.sum(hidden * encoder_output, dim=2)    def general_score(self, hidden, encoder_output):        energy = self.attn(encoder_output)        return torch.sum(hidden * energy, dim=2)    def concat_score(self, hidden, encoder_output):        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()        return torch.sum(self.v * energy, dim=2)    def forward(self, hidden, encoder_outputs):        # Calculate the attention weights (energies) based on the given method        if self.method == 'general':            attn_energies = self.general_score(hidden, encoder_outputs)        elif self.method == 'concat':            attn_energies = self.concat_score(hidden, encoder_outputs)        elif self.method == 'dot':            attn_energies = self.dot_score(hidden, encoder_outputs)        # Transpose max_length and batch_size dimensions        attn_energies = attn_energies.t()        # Return the softmax normalized probability scores (with added dimension)        return F.softmax(attn_energies, dim=1).unsqueeze(1)</code></pre><p>注意到 有一个 <code>self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))</code>,这个函数可以理解为一个类型转换函数，将一个不可训练类型的Tensor转换成可以训练的类型parameter，所以在参数优化的时候还是可以进行优化的)。在<code>concat注意力机制</code>中，权值$v_{a}^{T}$是不断学习的所以要是parameter类型，不直接使用一个torch.nn.Linear()可能是因为学习的效果不好。</p><p>现在我们已经定义了注意力子模块，我们可以实现真实的解码器模型。 对于解码器，我们将每次手动进行一批次的输入。 这意味着我们的词嵌入张量和GRU输出都将具有相同大小（<em>1，batch_size，hidden_size</em>）。</p><p><strong>计算图:</strong></p><blockquote><ol><li>获取当前输入的词嵌入</li><li>通过单向GRU进行前向传播</li><li>通过2输出的当前GRU计算注意力权重</li><li>将注意力权重乘以编码器输出以获得新的“weighted sum”上下文向量</li><li>使用[Luong eq.5]连接加权上下文向量和GRU输出</li><li>使用[Luong eq.6]预测下一个单词（没有softmax）</li><li>返回输出和最终隐藏状态</li></ol></blockquote><p>[Luong eq.5]: Computation path goes from $h_{t}$ → $\alpha_{t}$ → $c_{t}$ → $\tilde{h}<em>{t}$. Specifically, given the target hidden state $h</em>{t}$ and the source-side context vector $c_{t}$, we employ a simple concatenation layer to combine the information from both vectors to produce an attentional hidden state as follows:<br>$$\boldsymbol{\tilde{h}}<em>{t}=tanh(\boldsymbol{W}</em>{c}[\boldsymbol{c}<em>{t};\boldsymbol{h}</em>{t}])$$</p><p>[Luong eq.6]: The attention vector $\bm{\tilde{h}}<em>{t}$ si then fed through the softmax layer to produce the predictvie distribution formulated as:<br>$$ p(y</em>{t}|y_{&lt; t},x)=softmax(\bm{W}<em>{s}\bm{\tilde{h}}</em>{t})$$</p><p><strong>输入:</strong></p><ul><li><code>input_step</code>：每一步输入序列批次（一个单词）; shape =（<em>1，batch_size</em>）</li><li><code>last_hidden</code>：GRU的最终隐藏层; shape =（<em>n_layers x num_directions，batch_size，hidden_size</em>）</li><li><code>encoder_outputs</code>：编码器模型的输出; shape =（<em>max_length，batch_size，hidden_size</em>）</li></ul><p><strong>输出:</strong></p><ul><li><code>output</code>: 一个softmax标准化后的张量， 代表了每个单词在解码序列中是下一个输出单词的概率; shape =（<em>batch_size，voc.num_words</em>）</li><li><code>hidden</code>: GRU的最终隐藏状态; shape =（<em>n_layers x num_directions，batch_size，hidden_size</em>）</li></ul><pre class=" language-py"><code class="language-py">class LuongAttnDecoderRNN(nn.Module):    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):        super(LuongAttnDecoderRNN, self).__init__()        # Keep for reference        self.attn_model = attn_model        self.hidden_size = hidden_size        self.output_size = output_size        self.n_layers = n_layers        self.dropout = dropout        # Define layers        self.embedding = embedding        self.embedding_dropout = nn.Dropout(dropout)        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))        self.concat = nn.Linear(hidden_size * 2, hidden_size)        self.out = nn.Linear(hidden_size, output_size)        self.attn = Attn(attn_model, hidden_size)    def forward(self, input_step, last_hidden, encoder_outputs):        # Note: we run this one step (word) at a time        # Get embedding of current input word        embedded = self.embedding(input_step)        embedded = self.embedding_dropout(embedded)        # Forward through unidirectional GRU        rnn_output, hidden = self.gru(embedded, last_hidden)        # Calculate attention weights from the current GRU output        attn_weights = self.attn(rnn_output, encoder_outputs)        # Multiply attention weights to encoder outputs to get new "weighted sum" context vector        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))        # Concatenate weighted context vector and GRU output using Luong eq. 5        rnn_output = rnn_output.squeeze(0)        context = context.squeeze(1)        concat_input = torch.cat((rnn_output, context), 1)        concat_output = torch.tanh(self.concat(concat_input))        # Predict next word using Luong eq. 6        output = self.out(concat_output)        output = F.softmax(output, dim=1)        # Return output and final hidden state        return output, hidden</code></pre><h2 id="定义训练步骤"><a href="#定义训练步骤" class="headerlink" title="定义训练步骤"></a>定义训练步骤</h2><h3 id="Masked-损失"><a href="#Masked-损失" class="headerlink" title="Masked 损失"></a>Masked 损失</h3><p>由于我们处理的是批量填充序列，因此在计算损失时我们不能简单地考虑张量的所有元素。 我们定义<code>maskNLLLoss</code>可以根据解码器的输出张量、描述目标张量填充的binary mask张量来计算损失。 该损失函数计算与mask tensor中的1对应的元素的平均负对数似然。</p><p><code>torch.gather(input, dim, index, out=None)-&gt;Tensor</code> 沿着某个轴，按照指定维度采集数据，对于3维数据，相当于进行如下操作：</p><pre class=" language-py"><code class="language-py">out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2</code></pre><p>One example:<br>outputs1的维度为<code>[batch_size,vocab_size]</code>,即每个词在词汇表中的（softmax之后）概率。</p><p>outputs1</p><pre class=" language-py"><code class="language-py">tensor([[ 0.2000,  0.1000,  0.7000],        [ 0.3000,  0.6000,  0.1000],        [ 0.4000,  0.5000,  0.1000]])</code></pre><p><code>torch.gather(outputs1,1,torch.LongTensor([[1],[1],[1]])</code>表示在第一维选取第二个元素。</p><pre class=" language-py"><code class="language-py">temp = torch.gather(outputs1,1,torch.LongTensor([[1],[1],[1]]))print(temp)tensor([[ 0.1000],        [ 0.6000],        [ 0.5000]])</code></pre><p><code>torch.masked_slect(input,mask,out=None)</code>表示根据<code>mask(ByteTensor)</code>选取对应位置的值，返回一维张量。One example,</p><pre class=" language-py"><code class="language-py">print(mask)tensor([[ 0],        [ 1],        [ 1]], dtype=torch.uint8)</code></pre><pre class=" language-py"><code class="language-py">temp2=torch.masked_select(temp,mask)print(temp2)tensor([ 0.6000,  0.5000])</code></pre><pre class=" language-py"><code class="language-py">def maskNLLLoss(inp, target, mask):    nTotal = mask.sum()    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))    loss = crossEntropy.masked_select(mask).mean()    loss = loss.to(device)    return loss, nTotal.item()</code></pre><h3 id="单次训练迭代"><a href="#单次训练迭代" class="headerlink" title="单次训练迭代"></a>单次训练迭代</h3><p> <code>train</code> 函数包含单次训练迭代的算法（单批输入）。</p><p>我们将使用一些巧妙的技巧来帮助融合：</p><ul><li>第一个技巧是使用 <strong>teacher forcing</strong>。 这意味着在一些概率是由<code>teacher_forcing_ratio</code>设置，我们使用当前目标单词作为解码器的下一个输入，而不是使用解码器的当前推测。 该技巧充当解码器的 training wheels，有助于更有效的训练。 然而，<strong>teacher forcing</strong> 可能导致推导中的模型不稳定，因为解码器可能没有足够的机会在训练期间真正地制作自己的输出序列。 因此，我们必须注意我们如何设置<code>teacher_forcing_ratio</code>，同时不要被快速的收敛所迷惑。</li><li>我们实现的第二个技巧是<strong>梯度裁剪(gradient clipping)</strong>。 这是一种用于对抗“爆炸梯度（exploding gradient）”问题的常用技术。 本质上，通过将梯度剪切或阈值化到最大值，我们可以防止在损失函数中梯度以指数方式增长并发生溢出（NaN）或者越过梯度陡峭的悬崖。</li></ul><p><a href="https://pytorch.org/tutorials/_images/grad_clip.png" target="_blank" rel="noopener"><img src="img/gradient_clipping.jpg" alt="grad_clip"></a></p><p>图片来源: Goodfellow et al. <em>Deep Learning</em>. 2016. <a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">https://www.deeplearningbook.org/</a></p><p><strong>Sequence of Operations:</strong></p><p><strong>操作顺序:</strong></p><blockquote><ol><li>通过编码器前向计算整个批次输入。</li><li>将解码器输入初始化为SOS_token，将隐藏状态初始化为编码器的最终隐藏状态。</li><li>通过解码器一次一步地前向计算输入一批序列。</li><li>如果teacher forcing算法：将下一个解码器输入设置为当前目标; 否则：将下一个解码器输入设置为当前解码器输出。</li><li>计算并累积损失。</li><li>执行反向传播。</li><li>裁剪梯度。</li><li>更新编码器和解码器模型参数。</li></ol></blockquote><p>注意:</p><p>PyTorch的RNN模块（<code>RNN</code>，<code>LSTM</code>，<code>GRU</code>）可以像任何其他非重复层一样使用，只需将整个输入序列（或一批序列）传递给它们。 我们在<code>编码器</code>中使用<code>GRU</code>层就是这样的。 实际情况是，在计算中有一个迭代过程循环计算隐藏状态的每一步。 或者，你每次只运行一个模块。 在这种情况下，我们在训练过程中手动循环遍历序列就像我们必须为<code>解码器</code>模型做的那样。 只要你正确的维护这些模型的模块，就可以非常简单的实现顺序模型。</p><pre class=" language-py"><code class="language-py">def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):    # Zero gradients    encoder_optimizer.zero_grad()    decoder_optimizer.zero_grad()    # Set device options    input_variable = input_variable.to(device)    lengths = lengths.to(device)    target_variable = target_variable.to(device)    mask = mask.to(device)    # Initialize variables    loss = 0    print_losses = []    n_totals = 0    # Forward pass through encoder    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)    # Create initial decoder input (start with SOS tokens for each sentence)    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])    decoder_input = decoder_input.to(device)    # Set initial decoder hidden state to the encoder's final hidden state    decoder_hidden = encoder_hidden[:decoder.n_layers]    # Determine if we are using teacher forcing this iteration    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False    # Forward batch of sequences through decoder one time step at a time    if use_teacher_forcing:        for t in range(max_target_len):            decoder_output, decoder_hidden = decoder(                decoder_input, decoder_hidden, encoder_outputs            )            # Teacher forcing: next input is current target            decoder_input = target_variable[t].view(1, -1)            # Calculate and accumulate loss            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])            loss += mask_loss            print_losses.append(mask_loss.item() * nTotal)            n_totals += nTotal    else:        for t in range(max_target_len):            decoder_output, decoder_hidden = decoder(                decoder_input, decoder_hidden, encoder_outputs            )            # No teacher forcing: next input is decoder's own current output            _, topi = decoder_output.topk(1)            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])            decoder_input = decoder_input.to(device)            # Calculate and accumulate loss            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])            loss += mask_loss            print_losses.append(mask_loss.item() * nTotal)            n_totals += nTotal    # Perform backpropatation    loss.backward()    # Clip gradients: gradients are modified in place    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)    # Adjust model weights    encoder_optimizer.step()    decoder_optimizer.step()    return sum(print_losses) / n_totals</code></pre><h3 id="训练迭代"><a href="#训练迭代" class="headerlink" title="训练迭代"></a>训练迭代</h3><p>现在终于将完整的训练步骤与数据结合在一起了。 给定传递的模型，优化器，数据等，<code>trainIters</code>函数负责运行<code>n_iterations</code>的训练。这个功能不言自明，因为我们通过<code>train</code>函数的完成了繁重工作。</p><p>需要注意的一点是，当我们保存模型时，我们会保存一个包含编码器和解码器<code>state_dicts</code>（参数）、优化器的state_dicts、损失、迭代等的压缩包。以这种方式保存模型将为我们checkpoint,提供最大的灵活性。 加载checkpoint后，我们将能够使用模型参数进行推理，或者我们可以在我们中断的地方继续训练。</p><pre class=" language-py"><code class="language-py">def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):    # Load batches for each iteration    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])                      for _ in range(n_iteration)]    # Initializations    print('Initializing ...')    start_iteration = 1    print_loss = 0    if loadFilename:        start_iteration = checkpoint['iteration'] + 1    # Training loop    print("Training...")    for iteration in range(start_iteration, n_iteration + 1):        training_batch = training_batches[iteration - 1]        # Extract fields from batch        input_variable, lengths, target_variable, mask, max_target_len = training_batch        # Run a training iteration with batch        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)        print_loss += loss        # Print progress        if iteration % print_every == 0:            print_loss_avg = print_loss / print_every            print("Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}".format(iteration, iteration / n_iteration * 100, print_loss_avg))            print_loss = 0        # Save checkpoint        if (iteration % save_every == 0):            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))            if not os.path.exists(directory):                os.makedirs(directory)            torch.save({                'iteration': iteration,                'en': encoder.state_dict(),                'de': decoder.state_dict(),                'en_opt': encoder_optimizer.state_dict(),                'de_opt': decoder_optimizer.state_dict(),                'loss': loss,                'voc_dict': voc.__dict__,                'embedding': embedding.state_dict()            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))</code></pre><h2 id="评估定义"><a href="#评估定义" class="headerlink" title="评估定义"></a>评估定义</h2><p>在训练模型后，我们希望能够自己与机器人交谈。 首先，我们必须定义我们希望模型如何解码编码输入。</p><h3 id="贪婪解码"><a href="#贪婪解码" class="headerlink" title="贪婪解码"></a>贪婪解码</h3><p>贪婪解码是我们在不使用 teacher forcing时在训练期间使用的解码方法。 换句话说，对于每一步，我们只需从具有最高softmax值的<code>decoder_output</code>中选择单词。 该解码方法在单步长级别上是最佳的。</p><p>为了便于贪婪解码操作，我们定义了一个<code>GreedySearchDecoder</code>类。 当运行时，类的实例化对象输入序列（<code>input_seq</code>）的大小是（<em>input_seq length，1</em>），标量输入（<code>input_length</code>）长度的张量和<code>max_length</code>来约束响应句子长度。 使用以下计算图来评估输入句子：</p><p><strong>计算图:</strong></p><blockquote><ol><li>通过编码器模型前向计算。</li><li>准备编码器的最终隐藏层，作为解码器的第一个隐藏输入。</li><li>将解码器的第一个输入初始化为SOS_token。</li><li>将初始化张量追加到解码后的单词中。</li><li>一次迭代解码一个单词token：  <ol><li>通过解码器进行前向计算。</li><li>获得最可能的单词token及其softmax分数。</li><li>记录token和分数。</li><li>准备当前token作为下一个解码器的输入。</li></ol></li><li>返回收集到的单词 tokens 和 分数。</li></ol></blockquote><pre class=" language-py"><code class="language-py">class GreedySearchDecoder(nn.Module):    def __init__(self, encoder, decoder):        super(GreedySearchDecoder, self).__init__()        self.encoder = encoder        self.decoder = decoder    def forward(self, input_seq, input_length, max_length):        # Forward input through encoder model        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)        # Prepare encoder's final hidden layer to be first hidden input to the decoder        decoder_hidden = encoder_hidden[:decoder.n_layers]        # Initialize decoder input with SOS_token        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token        # Initialize tensors to append decoded words to        all_tokens = torch.zeros([0], device=device, dtype=torch.long)        all_scores = torch.zeros([0], device=device)        # Iteratively decode one word token at a time        for _ in range(max_length):            # Forward pass through decoder            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)            # Obtain most likely word token and its softmax score            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)            # Record token and score            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)            all_scores = torch.cat((all_scores, decoder_scores), dim=0)            # Prepare current token to be next decoder input (add a dimension)            decoder_input = torch.unsqueeze(decoder_input, 0)        # Return collections of word tokens and scores        return all_tokens, all_scores</code></pre><h3 id="评估我们的文本"><a href="#评估我们的文本" class="headerlink" title="评估我们的文本"></a>评估我们的文本</h3><p>现在我们已经定义了解码方法，我们可以编写用于评估字符串输入句子的函数。 <code>evaluate</code>函数管理输入句子的低层级处理过程。我们首先使用batch_size == 1将句子格式化为输入批量的单词索引。我们通过将句子的单词转换为相应的索引，并通过转换维度来为我们的模型准备张量。我们还创建了一个 <code>lengths</code> 张量，其中包含输入句子的长度。在这种情况下，<code>lengths</code> 是标量因为我们一次只评估一个句子（batch_size == 1）。接下来，我们使用我们的<code>GreedySearchDecoder</code>实例化后的对象（<code>searcher</code>）获得解码响应句子的张量。最后，我们将响应的索引转换为单词并返回已解码单词的列表。</p><p><code>evaluateInput</code>充当聊天机器人的用户接口。调用时，将生成一个输入文本字段，我们可以在其中输入查询语句。在输入我们的输入句子并按Enter后，我们的文本以与训练数据相同的方式标准化，并最终被输入到评估函数以获得解码的输出句子。我们循环这个过程，这样我们可以继续与我们的机器人聊天直到我们输入“q”或“quit”。</p><p>最后，如果输入的句子包含一个不在词汇表中的单词，我们会通过打印错误消息并提示用户输入另一个句子来优雅地处理。</p><pre class=" language-py"><code class="language-py">def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):    ### Format input sentence as a batch    # words -> indexes    indexes_batch = [indexesFromSentence(voc, sentence)]    # Create lengths tensor    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])    # Transpose dimensions of batch to match models' expectations    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)    # Use appropriate device    input_batch = input_batch.to(device)    lengths = lengths.to(device)    # Decode sentence with searcher    tokens, scores = searcher(input_batch, lengths, max_length)    # indexes -> words    decoded_words = [voc.index2word[token.item()] for token in tokens]    return decoded_wordsdef evaluateInput(encoder, decoder, searcher, voc):    input_sentence = ''    while(1):        try:            # Get input sentence            input_sentence = input('> ')            # Check if it is quit case            if input_sentence == 'q' or input_sentence == 'quit': break            # Normalize sentence            input_sentence = normalizeString(input_sentence)            # Evaluate sentence            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)            # Format and print response sentence            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]            print('Bot:', ' '.join(output_words))        except KeyError:            print("Error: Encountered unknown word.")</code></pre><h2 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h2><p>最后，是时候运行我们的模型了！</p><p>无论我们是否想要训练或测试聊天机器人模型，我们都必须初始化各个编码器和解码器模型。 在接下来的部分中，我们设置所需要的配置，选择从头开始或设置检查点以从中加载，并构建和初始化模型。 您可以随意使用不同的配置来优化性能。</p><pre class=" language-py"><code class="language-py"># Configure modelsmodel_name = 'cb_model'attn_model = 'dot'#attn_model = 'general'#attn_model = 'concat'hidden_size = 500encoder_n_layers = 2decoder_n_layers = 2dropout = 0.1batch_size = 64# Set checkpoint to load from; set to None if starting from scratchloadFilename = Nonecheckpoint_iter = 4000#loadFilename = os.path.join(save_dir, model_name, corpus_name,#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),#                            '{}_checkpoint.tar'.format(checkpoint_iter))# Load model if a loadFilename is providedif loadFilename:    # If loading on same machine the model was trained on    checkpoint = torch.load(loadFilename)    # If loading a model trained on GPU to CPU    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))    encoder_sd = checkpoint['en']    decoder_sd = checkpoint['de']    encoder_optimizer_sd = checkpoint['en_opt']    decoder_optimizer_sd = checkpoint['de_opt']    embedding_sd = checkpoint['embedding']    voc.__dict__ = checkpoint['voc_dict']print('Building encoder and decoder ...')# Initialize word embeddingsembedding = nn.Embedding(voc.num_words, hidden_size)if loadFilename:    embedding.load_state_dict(embedding_sd)# Initialize encoder & decoder modelsencoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)if loadFilename:    encoder.load_state_dict(encoder_sd)    decoder.load_state_dict(decoder_sd)# Use appropriate deviceencoder = encoder.to(device)decoder = decoder.to(device)print('Models built and ready to go!')</code></pre><p>输出:</p><pre class=" language-py"><code class="language-py">Building encoder and decoder ...Models built and ready to go!</code></pre><h3 id="执行训练"><a href="#执行训练" class="headerlink" title="执行训练"></a>执行训练</h3><p>如果要训练模型，请运行以下部分。</p><p>首先我们设置训练参数，然后初始化我们的优化器，最后我们调用<code>trainIters</code>函数来运行我们的训练迭代。</p><pre class=" language-py"><code class="language-py"># Configure training/optimizationclip = 50.0teacher_forcing_ratio = 1.0learning_rate = 0.0001decoder_learning_ratio = 5.0n_iteration = 4000print_every = 1save_every = 500# Ensure dropout layers are in train modeencoder.train()decoder.train()# Initialize optimizersprint('Building optimizers ...')encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)if loadFilename:    encoder_optimizer.load_state_dict(encoder_optimizer_sd)    decoder_optimizer.load_state_dict(decoder_optimizer_sd)# Run training iterationsprint("Starting Training!")trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,           print_every, save_every, clip, corpus_name, loadFilename)</code></pre><p>输出:</p><pre class=" language-py"><code class="language-py">Building optimizers ...Starting Training!Initializing ...Training...Iteration: 1; Percent complete: 0.0%; Average loss: 8.9717Iteration: 2; Percent complete: 0.1%; Average loss: 8.8521Iteration: 3; Percent complete: 0.1%; Average loss: 8.6360Iteration: 4; Percent complete: 0.1%; Average loss: 8.4234Iteration: 5; Percent complete: 0.1%; Average loss: 7.9403Iteration: 6; Percent complete: 0.1%; Average loss: 7.3892Iteration: 7; Percent complete: 0.2%; Average loss: 7.0589Iteration: 8; Percent complete: 0.2%; Average loss: 7.0130Iteration: 9; Percent complete: 0.2%; Average loss: 6.7383Iteration: 10; Percent complete: 0.2%; Average loss: 6.5343...Iteration: 3991; Percent complete: 99.8%; Average loss: 2.6607Iteration: 3992; Percent complete: 99.8%; Average loss: 2.6188Iteration: 3993; Percent complete: 99.8%; Average loss: 2.8319Iteration: 3994; Percent complete: 99.9%; Average loss: 2.5817Iteration: 3995; Percent complete: 99.9%; Average loss: 2.4979Iteration: 3996; Percent complete: 99.9%; Average loss: 2.7317Iteration: 3997; Percent complete: 99.9%; Average loss: 2.5969Iteration: 3998; Percent complete: 100.0%; Average loss: 2.2275Iteration: 3999; Percent complete: 100.0%; Average loss: 2.7124Iteration: 4000; Percent complete: 100.0%; Average loss: 2.5975</code></pre><h3 id="运行评估"><a href="#运行评估" class="headerlink" title="运行评估"></a>运行评估</h3><p>To chat with your model, run the following block.<br>运行以下部分来与你的模型聊天</p><pre class=" language-py"><code class="language-py"># Set dropout layers to eval modeencoder.eval()decoder.eval()# Initialize search modulesearcher = GreedySearchDecoder(encoder, decoder)# Begin chatting (uncomment and run the following line to begin)# evaluateInput(encoder, decoder, searcher, voc)</code></pre><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> multimodal leanring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to write your first SCI paper?</title>
      <link href="/2019/09/07/how-to-writing-paper/"/>
      <url>/2019/09/07/how-to-writing-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="How-to-write-your-first-SCI-paper"><a href="#How-to-write-your-first-SCI-paper" class="headerlink" title="How to write your first SCI paper?"></a>How to write your first SCI paper?</h1><p>这篇博客是清华大学刘洋老师在NLP会议上的报告PPT整理,原文<a href="http://nlp.csai.tsinghua.edu.cn/~ly/index_cn.html" target="_blank" rel="noopener">机器翻译学术论文的写作方法和技巧</a>. 写论文的本质是分享思想,呈现信息,虽然创新至上, 但是也需要掌握<em>科学的</em>写作技巧. 工欲善其事,必先利其器.</p><h2 id="论文发表流程"><a href="#论文发表流程" class="headerlink" title="论文发表流程"></a>论文发表流程</h2><ul><li><input checked disabled type="checkbox"> 确定方向  (影响重大,重大挑战,自己感兴趣的,即将成为热门)</li><li><input checked disabled type="checkbox"> 确定问题  (问题正确与否)</li><li><input checked disabled type="checkbox"> 确定思路  (思路新颖)</li><li><input checked disabled type="checkbox"> 确定方法  (方法正确,易于重现,对比合理)</li><li><input checked disabled type="checkbox"> 实验验证: 数据集,基线系统,评价指标</li><li><input checked disabled type="checkbox"> 撰写论文: 投稿ICML (表达清晰,文笔优美)</li></ul><h2 id="审稿"><a href="#审稿" class="headerlink" title="审稿"></a>审稿</h2><ul><li><p><strong>你以为审稿人是这样的</strong><br>审稿人一定是专家，无所不知。打印出来，仔细研读揣摩数天，对于看不懂的地方反复推敲。即使你的英文写得极其糟糕、即使你的文章组织很混乱、即使你的表述很晦涩，审稿人花费了大量的时间后终于看懂了，他认为你的工作是有意义的，决定给你个borderline或以上的分数。</p></li><li><p><strong>实际是这样的</strong><br>他不一定是专家，一直忙于其他事，在deadline到来之前一天要完成n篇。审稿时他往往先看<strong>题⽬</strong>、<strong>abstract</strong>，扫一下<strong>introduction</strong>（知道你做什么），然后直接翻到最后找核⼼心实验结果（做得好不好），然后基本确定录还是不录（<strong>也许只⽤用5分钟！</strong>）。如果决定录，剩下就是写些赞美的话，指出些次要的⼩小⽑毛病。如果决定拒，下⾯面的过程就是细看中间部分找理由拒了</p></li></ul><h2 id="转变观念"><a href="#转变观念" class="headerlink" title="转变观念"></a>转变观念</h2><p>from <em>以作者为核心整理工作</em> to  <em>以读者为核心阐述工作</em></p><ul><li>信息的呈现符合读者的认知惯性: 深入浅出，引人入胜，让读者快速找到想要的信息</li><li>尽量降低读者的理解难度：合理地综合使用各种信息元素：图&gt;曲线&gt;表&gt;正文&gt;公式</li><li>尽量提高读者阅读时的愉悦感: 思想新颖&gt;组织合理&gt;逻辑合理&gt;论证充分&gt;文笔优美&gt;排版美观</li></ul><h2 id="摘要应该怎么写"><a href="#摘要应该怎么写" class="headerlink" title="摘要应该怎么写?"></a>摘要应该怎么写?</h2><p>误区:</p><ul><li>1.力图把所有细节都说清楚 </li><li>2.用很专业的术语来描述 </li><li>3.出现数学符号</li></ul><p>用语要简单,让外行能够看懂</p><ul><li>1.问题是什么?</li><li>2.我们做了什么?</li><li>3.我们大概是怎么做的.</li><li>4.我们做的挺不错的!</li></ul><h2 id="介绍的写作技巧"><a href="#介绍的写作技巧" class="headerlink" title="介绍的写作技巧"></a>介绍的写作技巧</h2><p>介绍的写法</p><ul><li>比题目和摘要更进一步,用几段话说清你的工作</li><li>要点是充分论证你说做工作的<strong>必要性</strong>和<strong>重要性</strong></li><li>行文<strong>逻辑严密</strong>,<strong>论证充分</strong> </li></ul><p>常见的<strong>逻辑</strong>?</p><ul><li>说明问题是什么</li><li>简单罗列前人的工作</li><li>描述我们的工作</li></ul><p>更好的<strong>逻辑</strong>!</p><ul><li>说明问题是什么</li><li>目前<strong>最好的工作面临什么挑战</strong></li><li>我们的方法能缓解上述挑战</li></ul><h2 id="段落的写法"><a href="#段落的写法" class="headerlink" title="段落的写法"></a>段落的写法</h2><ul><li>每个段落有个论断性的<font color="red">中心句</font></li><li>其余部分都是<font color="blue">支撑句</font>，围绕中心句展开论证<blockquote><ul><li>前人工作</li><li>具体数据</li></ul></blockquote></li><li>支撑句之间可分类组织,支撑句要论证严密</li><li>段尾可以加上<font color="purple">衔接句</font><h3 id="图和表的重要性"><a href="#图和表的重要性" class="headerlink" title="图和表的重要性"></a>图和表的重要性</h3></li><li>图和表是论文的骨架，争取<font color="green">让读者按照顺序看就能理解论文的主要思想，不用通过看正文才能懂</font></li><li>一般第一遍看，都会看图、找例子</li><li>然后翻到后面找主要结果</li><li>再从头看正文</li><li>把论文的的元素放在最应该被放在的地方，符合读者的认知惯性，降低理解难度<h3 id="如何描述自己的方法"><a href="#如何描述自己的方法" class="headerlink" title="如何描述自己的方法"></a>如何描述自己的方法</h3></li><li>不要一上来就描述你的工作,可以先介绍<strong>背景知识</strong>(往往是baseline),在论文里面呈现的元素是Background和Preliminary</li><li>有利于降低初学者或其他领域学者的理解难度</li><li>有利于对introduction中的论文做详细的解释</li><li>有利于对比baseline和你的方法<h3 id="方法描述的逻辑顺序"><a href="#方法描述的逻辑顺序" class="headerlink" title="方法描述的逻辑顺序"></a>方法描述的逻辑顺序</h3>错误的顺序</li><li>形式化描述</li><li>解释数学符号的意义</li></ul><p>正确的顺序</p><ul><li>首先给出running example</li><li>然后利用running example,用通俗语言描述你的想法</li><li>最后是形式化描述</li></ul><h2 id="如何写-realted-work"><a href="#如何写-realted-work" class="headerlink" title="如何写 realted work"></a>如何写 realted work</h2><p><font color="tomato">Wrong</font></p><ul><li>没有引用重要论文(!dangerous)</li><li>简单的罗列和堆砌,缺乏深刻到位的review</li><li>通过批评乃至攻击前人工作证明你的工作的创新性</li></ul><p><font color="forestgreen">Right</font></p><ul><li>向审稿人显示你对本领域具有全面深刻的把握</li><li>通过与前人工作的对比凸显你的工作的创新性 (传承与创新)</li><li>为读者梳理领域的发展脉络,获得全局的认知</li></ul><h2 id="写作常见的问题"><a href="#写作常见的问题" class="headerlink" title="写作常见的问题"></a>写作常见的问题</h2><ul><li>句子过长</li><li>经常使用被动句式</li><li>结构松散,口语化</li><li>不定冠词和定冠词的使用,慎用it,there be等.</li><li>引用的写法(放在句首,放在句尾去掉不影响整个句子的含义)</li></ul><h2 id="时间管理和反馈"><a href="#时间管理和反馈" class="headerlink" title="时间管理和反馈"></a>时间管理和反馈</h2><p>coarse-to-fine </p><ul><li>截稿前一个月开始写</li><li>每隔两天改一次听取不同背景读者的反馈意见</li><li>专家:专业意见</li><li>非专家:发现信息壁垒</li><li>写到极致,完成完美精致的艺术品</li></ul>]]></content>
      
      
      <categories>
          
          <category> paper writing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> writing paper </tag>
            
            <tag> logic expression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/06/21/hello-world/"/>
      <url>/2019/06/21/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Conjugate</title>
      <link href="/2019/06/20/conjugate/"/>
      <url>/2019/06/20/conjugate/</url>
      
        <content type="html"><![CDATA[<h2 id="共轭分布"><a href="#共轭分布" class="headerlink" title="共轭分布"></a>共轭分布</h2><hr><h3 id="共轭-conjugate-后验和先验是同一种分布。"><a href="#共轭-conjugate-后验和先验是同一种分布。" class="headerlink" title="共轭(conjugate) : 后验和先验是同一种分布。"></a>共轭(conjugate) : 后验和先验是同一种分布。</h3><hr><h3 id="泊松分布-伽马分布模型"><a href="#泊松分布-伽马分布模型" class="headerlink" title="泊松分布-伽马分布模型"></a>泊松分布-伽马分布模型</h3><p>假设有一组观测样本 $x _ { 1 } , \dots , x _ { n }$ 独立同分布于泊松分布，即 $x _ { i } \sim \operatorname { Poisson } ( \lambda )$ ，则<br>$$<br>p \left( x _ { i } | \lambda \right) = \frac { e ^ { - \lambda } \lambda ^ { x _ { i } } } { x _ { i } ! }<br>$$</p><p>从而，可以很轻松地写出相应的<strong>似然（likelihood）</strong>：$$\mathcal { L } \left( x _ { 1 } , \ldots , x _ { n } | \lambda \right) = \prod _ { i = 1 } ^ { n } \frac { e ^ { - \lambda } \lambda ^ { x _ { i } } } { x _ { i } ! } = \frac { e ^ { - n \lambda } \lambda \sum _ { i = 1 } ^ { n } x _ { i } } { \prod _ { i = 1 } ^ { n } x _ { i } ! }$$</p><p>其中，$\lambda&gt;0$ 是一个未知的参数，进一步假设其服从伽马分布，<strong>给定先验 ，则$\lambda \sim \operatorname { Gamma } ( \alpha , \beta )$</strong>，则<br>$$ p ( \lambda | \alpha , \beta ) = \frac { \beta ^ { \alpha } } { \Gamma ( \alpha ) } e ^ { - \beta \lambda } \lambda ^ { \alpha - 1 }$$</p><p>其中，需要说明的是，伽马分布中的 $\alpha$ 表示形状（shape）参数， $\beta$ 表示比率（rate）参数， $\Gamma(\cdot)$ 表示伽马函数。<br>在贝叶斯分析中，有了先验和似然，则<strong>poster $\propto$ prior $\times$ likelihood</strong>,也就是所谓的后验分布正比于先验和似然的乘积。</p><p>即$$\begin{aligned}\left( \lambda | x _ { 1 } , \ldots , x _ { n } , \alpha , \beta \right) &amp;\propto p ( \lambda | \alpha , \beta ) \mathcal { L } \left( x _ { 1 } , \ldots , x _ { n } | \lambda \right) \\ &amp; \propto e ^ { - ( \beta + n ) \lambda } \lambda ^ { \alpha + \sum _ { i = 1 } ^ { n } x _ { i } - 1 } \\ &amp; \Rightarrow \left( \lambda | x _ { 1 } , \ldots , x _ { n } , \alpha , \beta \right) \sim \operatorname { Gamma } \left( \alpha + \sum _ { i = 1 } ^ { n } x _ { i } , \beta + n \right)\end{aligned}$$</p><p>因此，假设一组观测样本独立同分布于参数为$\lambda$的泊松分布，则伽马分布是参数$\lambda$的共轭先验（conjugate prior）。</p><hr><h3 id="正态分布-正态分布模型"><a href="#正态分布-正态分布模型" class="headerlink" title="正态分布-正态分布模型"></a>正态分布-正态分布模型</h3><p>假设一组观测样本 $x _ { 1 } , \dots , x _ { n }$ 独立同分布于正态分布，即 $x_{i} \sim \mathcal{N}(\mu,\sigma^{2})$ ，其中， $\boldsymbol{\mu}$是未知的，$\boldsymbol{\sigma^{2}}$ 是已知的（不需要假设先验），则似然为:<br>$$\begin{aligned}\mathcal { L } \left( x _ { 1 } , \ldots , x _ { n } | \mu \right) &amp; = \prod _ { i = 1 } ^ { n } \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp { - \frac { 1 } { 2 \sigma ^ { 2 } } \left( x _ { i } - \mu \right) ^ { 2 } } \\ &amp; \propto \exp { - \frac { 1 } { 2 \sigma ^ { 2 } } \left( x _ { i } - \mu \right) ^ { 2 } } \end{aligned}$$</p><p>进一步，假设<strong>参数</strong>$\boldsymbol{\mu}$的先验为$\boldsymbol{\mu} \sim \mathcal{N}(\mu_{0},\sigma_{0}^{2})$ ，即<br>$$ p\left( \mu | \mu _ { 0 } , \sigma _ { 0 } \right) = \frac { 1 } { \sqrt { 2 \pi } \sigma _ { 0 } } \exp { - \frac { 1 } { 2 \sigma _ { 0 } ^ { 2 } } \left( \mu - \mu _ { 0 } \right) ^ { 2 } } $$<br>从而，可以推导出后验为<br>$$\begin{aligned} p\left( \mu | x _ { 1 } , \ldots , x _ { n } , \mu _ { 0 } , \sigma _ { 0 } \right) &amp; \propto p \left( \mu | \mu _ { 0 } , \sigma _ { 0 } \right) \mathcal { L } \left( x _ { 1 } , \ldots , x _ { n } | \mu \right) \\ &amp; \propto \exp { - \frac { 1 } { 2 } \left[ \frac { 1 } { \sigma ^ { 2 } } \sum _ { i = 1 } ^ { n } \left( x _ { i } - \mu \right) ^ { 2 } + \frac { 1 } { \sigma _ { 0 } ^ { 2 } } \left( \mu - \mu _ { 0 } \right) ^ { 2 } \right] } \\ &amp; \propto \exp { - \frac { 1 } { 2 } \left( \frac { 1 } { \sigma _ { 0 } ^ { 2 } } + \frac { n } { \sigma ^ { 2 } } \right) \left( \mu - \frac { \frac { \mu _ { 0 } } { \sigma _ { 0 } ^ { 2 } } + \frac { n \overline { x } } { \sigma ^ { 2 } } } { \frac { 1 } { \sigma _ { 0 } ^ { 2 } } + \frac { n } { \sigma ^ { 2 } } } \right) ^ { 2 } } \end{aligned}$$</p><p>因此，后验也服从正态分布，一般而言，有了先验和似然，计算后验能够方便我们实现变分推断和Gibbs采样等。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> PGM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客主题之hexo-theme-matery的介绍</title>
      <link href="/2018/09/28/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/"/>
      <url>/2018/09/28/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/</url>
      
        <content type="html"><![CDATA[<p><a href="README.md">English Document</a> | <a href="https://blinkfox.github.io/" target="_blank" rel="noopener">演示示例</a> | QQ交流群:<code>926552981</code></p><blockquote><p>这是一个采用<code>Material Design</code>和响应式设计的 Hexo 博客主题。</p></blockquote><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ul><li>简单漂亮，文章内容美观易读</li><li><a href="https://material.io/" target="_blank" rel="noopener">Material Design</a> 设计</li><li>响应式设计，博客在桌面端、平板、手机等设备上均能很好的展现</li><li>首页轮播文章及每天动态切换 <code>Banner</code> 图片</li><li>瀑布流式的博客文章列表（文章无特色图片时会有 <code>24</code> 张漂亮的图片代替）</li><li>时间轴式的归档页</li><li><strong>词云</strong>的标签页和<strong>雷达图</strong>的分类页</li><li>丰富的关于我页面（包括关于我、文章统计图、我的项目、我的技能、相册等）</li><li>可自定义的数据的友情链接页面</li><li>支持文章置顶和文章打赏</li><li>支持 <code>MathJax</code></li><li><code>TOC</code> 目录</li><li>可设置复制文章内容时追加版权信息</li><li>可设置阅读文章时做密码验证</li><li><a href="https://gitalk.github.io/" target="_blank" rel="noopener">Gitalk</a>、<a href="https://imsun.github.io/gitment/" target="_blank" rel="noopener">Gitment</a>、<a href="https://valine.js.org/" target="_blank" rel="noopener">Valine</a> 和 <a href="https://disqus.com/" target="_blank" rel="noopener">Disqus</a> 评论模块（推荐使用 <code>Gitalk</code>）</li><li>集成了<a href="http://busuanzi.ibruce.info/" target="_blank" rel="noopener">不蒜子统计</a>、谷歌分析（<code>Google Analytics</code>）和文章字数统计等功能</li><li>支持在首页的音乐播放和视频播放功能</li></ul><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>当你看到这里的时候，应该已经有一个自己的 <a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">Hexo</a> 博客了。如果还没有的话，不妨使用 Hexo 和 <a href="https://www.appinn.com/markdown/" target="_blank" rel="noopener">Markdown</a> 来写博客和文章。</p><p>点击 <a href="https://codeload.github.com/blinkfox/hexo-theme-matery/zip/master" target="_blank" rel="noopener">这里</a> 下载 <code>master</code> 分支的最新稳定版的代码，解压缩后，将 <code>hexo-theme-matery</code> 的文件夹复制到你 Hexo 的 <code>themes</code> 文件夹中即可。</p><p>当然你也可以在你的 <code>themes</code> 文件夹下使用 <code>Git clone</code> 命令来下载:</p><pre class=" language-bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/blinkfox/hexo-theme-matery.git</code></pre><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="切换主题"><a href="#切换主题" class="headerlink" title="切换主题"></a>切换主题</h3><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的  <code>theme</code> 的值：<code>theme: hexo-theme-matery</code></p><h4 id="config-yml-文件的其它修改建议"><a href="#config-yml-文件的其它修改建议" class="headerlink" title="_config.yml 文件的其它修改建议:"></a><code>_config.yml</code> 文件的其它修改建议:</h4><ul><li>请修改 <code>_config.yml</code> 的 <code>url</code> 的值为你的网站主 <code>URL</code>（如：<code>http://xxx.github.io</code>）。</li><li>建议修改两个 <code>per_page</code> 的分页条数值为 <code>6</code> 的倍数，如：<code>12</code>、<code>18</code> 等，这样文章列表在各个屏幕下都能较好的显示。</li><li>如果你是中文用户，则建议修改 <code>language</code> 的值为 <code>zh-CN</code>。</li></ul><h3 id="新建分类-categories-页"><a href="#新建分类-categories-页" class="headerlink" title="新建分类 categories 页"></a>新建分类 categories 页</h3><p><code>categories</code> 页是用来展示所有分类的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>categories/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre class=" language-bash"><code class="language-bash">hexo new page <span class="token string">"categories"</span></code></pre><p>编辑你刚刚新建的页面文件 <code>/source/categories/index.md</code>，至少需要以下内容：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> categories<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-30 17:25:30</span><span class="token key atrule">type</span><span class="token punctuation">:</span> <span class="token string">"categories"</span><span class="token key atrule">layout</span><span class="token punctuation">:</span> <span class="token string">"categories"</span><span class="token punctuation">---</span></code></pre><h3 id="新建标签-tags-页"><a href="#新建标签-tags-页" class="headerlink" title="新建标签 tags 页"></a>新建标签 tags 页</h3><p><code>tags</code> 页是用来展示所有标签的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>tags/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre class=" language-bash"><code class="language-bash">hexo new page <span class="token string">"tags"</span></code></pre><p>编辑你刚刚新建的页面文件 <code>/source/tags/index.md</code>，至少需要以下内容：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> tags<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-30 18:23:38</span><span class="token key atrule">type</span><span class="token punctuation">:</span> <span class="token string">"tags"</span><span class="token key atrule">layout</span><span class="token punctuation">:</span> <span class="token string">"tags"</span><span class="token punctuation">---</span></code></pre><h3 id="新建关于我-about-页"><a href="#新建关于我-about-页" class="headerlink" title="新建关于我 about 页"></a>新建关于我 about 页</h3><p><code>about</code> 页是用来展示<strong>关于我和我的博客</strong>信息的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>about/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre class=" language-bash"><code class="language-bash">hexo new page <span class="token string">"about"</span></code></pre><p>编辑你刚刚新建的页面文件 <code>/source/about/index.md</code>，至少需要以下内容：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> about<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-30 17:25:30</span><span class="token key atrule">type</span><span class="token punctuation">:</span> <span class="token string">"about"</span><span class="token key atrule">layout</span><span class="token punctuation">:</span> <span class="token string">"about"</span><span class="token punctuation">---</span></code></pre><h3 id="新建友情连接-friends-页（可选的）"><a href="#新建友情连接-friends-页（可选的）" class="headerlink" title="新建友情连接 friends 页（可选的）"></a>新建友情连接 friends 页（可选的）</h3><p><code>friends</code> 页是用来展示<strong>友情连接</strong>信息的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>friends/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre class=" language-bash"><code class="language-bash">hexo new page <span class="token string">"friends"</span></code></pre><p>编辑你刚刚新建的页面文件 <code>/source/friends/index.md</code>，至少需要以下内容：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> friends<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-12-12 21:25:30</span><span class="token key atrule">type</span><span class="token punctuation">:</span> <span class="token string">"friends"</span><span class="token key atrule">layout</span><span class="token punctuation">:</span> <span class="token string">"friends"</span><span class="token punctuation">---</span></code></pre><p>同时，在你的博客 <code>source</code> 目录下新建 <code>_data</code> 目录，在 <code>_data</code> 目录中新建 <code>friends.json</code> 文件，文件内容如下所示：</p><pre class=" language-json"><code class="language-json"><span class="token punctuation">[</span><span class="token punctuation">{</span>    <span class="token property">"avatar"</span><span class="token operator">:</span> <span class="token string">"http://image.luokangyuan.com/1_qq_27922023.jpg"</span><span class="token punctuation">,</span>    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"码酱"</span><span class="token punctuation">,</span>    <span class="token property">"introduction"</span><span class="token operator">:</span> <span class="token string">"我不是大佬，只是在追寻大佬的脚步"</span><span class="token punctuation">,</span>    <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"http://luokangyuan.com/"</span><span class="token punctuation">,</span>    <span class="token property">"title"</span><span class="token operator">:</span> <span class="token string">"前去学习"</span><span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>    <span class="token property">"avatar"</span><span class="token operator">:</span> <span class="token string">"http://image.luokangyuan.com/4027734.jpeg"</span><span class="token punctuation">,</span>    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"闪烁之狐"</span><span class="token punctuation">,</span>    <span class="token property">"introduction"</span><span class="token operator">:</span> <span class="token string">"编程界大佬，技术牛，人还特别好，不懂的都可以请教大佬"</span><span class="token punctuation">,</span>    <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"https://blinkfox.github.io/"</span><span class="token punctuation">,</span>    <span class="token property">"title"</span><span class="token operator">:</span> <span class="token string">"前去学习"</span><span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>    <span class="token property">"avatar"</span><span class="token operator">:</span> <span class="token string">"http://image.luokangyuan.com/avatar.jpg"</span><span class="token punctuation">,</span>    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"ja_rome"</span><span class="token punctuation">,</span>    <span class="token property">"introduction"</span><span class="token operator">:</span> <span class="token string">"平凡的脚步也可以走出伟大的行程"</span><span class="token punctuation">,</span>    <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"ttps://me.csdn.net/jlh912008548"</span><span class="token punctuation">,</span>    <span class="token property">"title"</span><span class="token operator">:</span> <span class="token string">"前去学习"</span><span class="token punctuation">}</span><span class="token punctuation">]</span></code></pre><h3 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h3><p>由于 Hexo 自带的代码高亮主题显示不好看，所以主题中使用到了 <a href="https://github.com/ele828/hexo-prism-plugin" target="_blank" rel="noopener">hexo-prism-plugin</a> 的 Hexo 插件来做代码高亮，安装命令如下：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> i -S hexo-prism-plugin</code></pre><p>然后，修改 Hexo 根目录下 <code>_config.yml</code> 文件中 <code>highlight.enable</code> 的值为 <code>false</code>，并新增 <code>prism</code> 插件相关的配置，主要配置如下：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">highlight</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">false</span><span class="token key atrule">prism_plugin</span><span class="token punctuation">:</span>  <span class="token key atrule">mode</span><span class="token punctuation">:</span> <span class="token string">'preprocess'</span>    <span class="token comment" spellcheck="true"># realtime/preprocess</span>  <span class="token key atrule">theme</span><span class="token punctuation">:</span> <span class="token string">'tomorrow'</span>  <span class="token key atrule">line_number</span><span class="token punctuation">:</span> <span class="token boolean important">false    </span><span class="token comment" spellcheck="true"># default false</span>  custom_css<span class="token punctuation">:</span></code></pre><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>本主题中还使用到了 <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener">hexo-generator-search</a> 的 Hexo 插件来做内容搜索，安装命令如下：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-generator-search --save</code></pre><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">search</span><span class="token punctuation">:</span>  <span class="token key atrule">path</span><span class="token punctuation">:</span> search.xml  <span class="token key atrule">field</span><span class="token punctuation">:</span> post</code></pre><h3 id="中文链接转拼音（可选的）"><a href="#中文链接转拼音（可选的）" class="headerlink" title="中文链接转拼音（可选的）"></a>中文链接转拼音（可选的）</h3><p>如果你的文章名称是中文的，那么 Hexo 默认生成的永久链接也会有中文，这样不利于 <code>SEO</code>，且 <code>gitment</code> 评论对中文链接也不支持。我们可以用 <a href="https://github.com/viko16/hexo-permalink-pinyin" target="_blank" rel="noopener">hexo-permalink-pinyin</a> Hexo 插件使在生成文章时生成中文拼音的永久链接。</p><p>安装命令如下：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> i hexo-permalink-pinyin --save</code></pre><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">permalink_pinyin</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">separator</span><span class="token punctuation">:</span> <span class="token string">'-'</span> <span class="token comment" spellcheck="true"># default: '-'</span></code></pre><blockquote><p><strong>注</strong>：除了此插件外，<a href="https://github.com/rozbo/hexo-abbrlink" target="_blank" rel="noopener">hexo-abbrlink</a> 插件也可以生成非中文的链接。</p></blockquote><h3 id="文章字数统计插件（可选的）"><a href="#文章字数统计插件（可选的）" class="headerlink" title="文章字数统计插件（可选的）"></a>文章字数统计插件（可选的）</h3><p>如果你想要在文章中显示文章字数、阅读时长信息，可以安装 <a href="https://github.com/willin/hexo-wordcount" target="_blank" rel="noopener">hexo-wordcount</a>插件。</p><p>安装命令如下：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> i --save hexo-wordcount</code></pre><p>然后只需在本主题下的 <code>_config.yml</code> 文件中，激活以下配置项即可：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">wordCount</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 将这个值设置为 true 即可.</span>  <span class="token key atrule">postWordCount</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">min2read</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">totalCount</span><span class="token punctuation">:</span> <span class="token boolean important">true</span></code></pre><h3 id="添加-RSS-订阅支持（可选的）"><a href="#添加-RSS-订阅支持（可选的）" class="headerlink" title="添加 RSS 订阅支持（可选的）"></a>添加 RSS 订阅支持（可选的）</h3><p>本主题中还使用到了 <a href="https://github.com/hexojs/hexo-generator-feed" target="_blank" rel="noopener">hexo-generator-feed</a> 的 Hexo 插件来做 <code>RSS</code>，安装命令如下：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-generator-feed --save</code></pre><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">feed</span><span class="token punctuation">:</span>  <span class="token key atrule">type</span><span class="token punctuation">:</span> atom  <span class="token key atrule">path</span><span class="token punctuation">:</span> atom.xml  <span class="token key atrule">limit</span><span class="token punctuation">:</span> <span class="token number">20</span>  <span class="token key atrule">hub</span><span class="token punctuation">:</span>  <span class="token key atrule">content</span><span class="token punctuation">:</span>  <span class="token key atrule">content_limit</span><span class="token punctuation">:</span> <span class="token number">140</span>  <span class="token key atrule">content_limit_delim</span><span class="token punctuation">:</span> <span class="token string">' '</span>  <span class="token key atrule">order_by</span><span class="token punctuation">:</span> <span class="token punctuation">-</span>date</code></pre><p>执行 <code>hexo clean &amp;&amp; hexo g</code> 重新生成博客文件，然后在 <code>public</code> 文件夹中即可看到 <code>atom.xml</code> 文件，说明你已经安装成功了。</p><h3 id="修改页脚"><a href="#修改页脚" class="headerlink" title="修改页脚"></a>修改页脚</h3><p>页脚信息可能需要做定制化修改，而且它不便于做成配置信息，所以可能需要你自己去再修改和加工。修改的地方在主题文件的 <code>/layout/_partial/footer.ejs</code> 文件中，包括站点、使用的主题、访问量等。</p><h3 id="修改社交链接"><a href="#修改社交链接" class="headerlink" title="修改社交链接"></a>修改社交链接</h3><p>在主题的 <code>_config.yml</code> 文件中，默认支持 <code>QQ</code>、<code>GitHub</code> 和邮箱的配置，你可以在主题文件的 <code>/layout/_partial/social-link.ejs</code> 文件中，新增、修改你需要的社交链接地址，增加链接可参考如下代码：</p><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>https://github.com/blinkfox<span class="token punctuation">"</span></span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>tooltipped<span class="token punctuation">"</span></span> <span class="token attr-name">target</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>_blank<span class="token punctuation">"</span></span> <span class="token attr-name">data-tooltip</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>访问我的GitHub<span class="token punctuation">"</span></span> <span class="token attr-name">data-position</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>top<span class="token punctuation">"</span></span> <span class="token attr-name">data-delay</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>50<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>i</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>fa fa-github<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>i</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span></code></pre><p>其中，社交图标（如：<code>fa-github</code>）你可以在 <a href="https://fontawesome.com/icons" target="_blank" rel="noopener">Font Awesome</a> 中搜索找到。以下是常用社交图标的标识，供你参考：</p><ul><li>Facebook: <code>fa-facebook</code></li><li>Twitter: <code>fa-twitter</code></li><li>Google-plus: <code>fa-google-plus</code></li><li>Linkedin: <code>fa-linkedin</code></li><li>Tumblr: <code>fa-tumblr</code></li><li>Medium: <code>fa-medium</code></li><li>Slack: <code>fa-slack</code></li><li>新浪微博: <code>fa-weibo</code></li><li>微信: <code>fa-wechat</code></li><li>QQ: <code>fa-qq</code></li></ul><blockquote><p><strong>注意</strong>: 本主题中使用的 <code>Font Awesome</code> 版本为 <code>4.7.0</code>。</p></blockquote><h3 id="修改打赏的二维码图片"><a href="#修改打赏的二维码图片" class="headerlink" title="修改打赏的二维码图片"></a>修改打赏的二维码图片</h3><p>在主题文件的 <code>source/medias/reward</code> 文件中，你可以替换成你的的微信和支付宝的打赏二维码图片。</p><h3 id="配置音乐播放器（可选的）"><a href="#配置音乐播放器（可选的）" class="headerlink" title="配置音乐播放器（可选的）"></a>配置音乐播放器（可选的）</h3><p>要支持音乐播放，就必须开启音乐的播放配置和音乐数据的文件。</p><p>首先，在你的博客 <code>source</code> 目录下的 <code>_data</code> 目录（没有的话就新建一个）中新建 <code>musics.json</code> 文件，文件内容如下所示：</p><pre class=" language-json"><code class="language-json"><span class="token punctuation">[</span><span class="token punctuation">{</span>    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"五月雨变奏电音"</span><span class="token punctuation">,</span>    <span class="token property">"artist"</span><span class="token operator">:</span> <span class="token string">"AnimeVibe"</span><span class="token punctuation">,</span>    <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"http://xxx.com/music1.mp3"</span><span class="token punctuation">,</span>    <span class="token property">"cover"</span><span class="token operator">:</span> <span class="token string">"http://xxx.com/music-cover1.png"</span><span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"Take me hand"</span><span class="token punctuation">,</span>    <span class="token property">"artist"</span><span class="token operator">:</span> <span class="token string">"DAISHI DANCE,Cecile Corbel"</span><span class="token punctuation">,</span>    <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"/medias/music/music2.mp3"</span><span class="token punctuation">,</span>    <span class="token property">"cover"</span><span class="token operator">:</span> <span class="token string">"/medias/music/cover2.png"</span><span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"Shape of You"</span><span class="token punctuation">,</span>    <span class="token property">"artist"</span><span class="token operator">:</span> <span class="token string">"J.Fla"</span><span class="token punctuation">,</span>    <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"http://xxx.com/music3.mp3"</span><span class="token punctuation">,</span>    <span class="token property">"cover"</span><span class="token operator">:</span> <span class="token string">"http://xxx.com/music-cover3.png"</span><span class="token punctuation">}</span><span class="token punctuation">]</span></code></pre><blockquote><p><strong>注</strong>：以上 JSON 中的属性：<code>name</code>、<code>artist</code>、<code>url</code>、<code>cover</code> 分别表示音乐的名称、作者、音乐文件地址、音乐封面。</p></blockquote><p>然后，在主题的 <code>_config.yml</code> 配置文件中激活配置即可：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># 是否在首页显示音乐.</span><span class="token key atrule">music</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">showTitle</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>  <span class="token key atrule">title</span><span class="token punctuation">:</span> 听听音乐  <span class="token key atrule">fixed</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 是否开启吸底模式</span>  <span class="token key atrule">autoplay</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 是否自动播放</span>  <span class="token key atrule">theme</span><span class="token punctuation">:</span> '<span class="token comment" spellcheck="true">#42b983'</span>  <span class="token key atrule">loop</span><span class="token punctuation">:</span> <span class="token string">'all'</span> <span class="token comment" spellcheck="true"># 音频循环播放, 可选值: 'all', 'one', 'none'</span>  <span class="token key atrule">order</span><span class="token punctuation">:</span> <span class="token string">'list'</span> <span class="token comment" spellcheck="true"># 音频循环顺序, 可选值: 'list', 'random'</span>  <span class="token key atrule">preload</span><span class="token punctuation">:</span> <span class="token string">'auto'</span> <span class="token comment" spellcheck="true"># 预加载，可选值: 'none', 'metadata', 'auto'</span>  <span class="token key atrule">volume</span><span class="token punctuation">:</span> <span class="token number">0.7 </span><span class="token comment" spellcheck="true"># 默认音量，请注意播放器会记忆用户设置，用户手动设置音量后默认音量即失效</span>  <span class="token key atrule">listFolded</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 列表默认折叠</span>  <span class="token key atrule">listMaxHeight</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 列表最大高度</span></code></pre><h2 id="文章-Front-matter-介绍"><a href="#文章-Front-matter-介绍" class="headerlink" title="文章 Front-matter 介绍"></a>文章 Front-matter 介绍</h2><h3 id="Front-matter-选项详解"><a href="#Front-matter-选项详解" class="headerlink" title="Front-matter 选项详解"></a>Front-matter 选项详解</h3><p><code>Front-matter</code> 选项中的所有内容均为<strong>非必填</strong>的。但我仍然建议至少填写 <code>title</code> 和 <code>date</code> 的值。</p><table><thead><tr><th>配置选项</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>title</td><td><code>Markdown</code> 的文件标题</td><td>文章标题，强烈建议填写此选项</td></tr><tr><td>date</td><td>文件创建时的日期时间</td><td>发布时间，强烈建议填写此选项，且最好保证全局唯一</td></tr><tr><td>author</td><td>根 <code>_config.yml</code> 中的 <code>author</code></td><td>文章作者</td></tr><tr><td>img</td><td><code>featureImages</code> 中的某个值</td><td>文章特征图，推荐使用图床(腾讯云、七牛云、又拍云等)来做图片的路径.如: <code>http://xxx.com/xxx.jpg</code></td></tr><tr><td>top</td><td><code>true</code></td><td>推荐文章（文章是否置顶），如果 <code>top</code> 值为 <code>true</code>，则会作为首页推荐文章</td></tr><tr><td>cover</td><td><code>false</code></td><td><code>v1.0.2</code>版本新增，表示该文章是否需要加入到首页轮播封面中</td></tr><tr><td>coverImg</td><td>无</td><td><code>v1.0.2</code>版本新增，表示该文章在首页轮播封面需要显示的图片路径，如果没有，则默认使用文章的特色图片</td></tr><tr><td>password</td><td>无</td><td>文章阅读密码，如果要对文章设置阅读验证密码的话，就可以设置 <code>password</code> 的值，该值必须是用 <code>SHA256</code> 加密后的密码，防止被他人识破。前提是在主题的 <code>config.yml</code> 中激活了 <code>verifyPassword</code> 选项</td></tr><tr><td>toc</td><td><code>true</code></td><td>是否开启 TOC，可以针对某篇文章单独关闭 TOC 的功能。前提是在主题的 <code>config.yml</code> 中激活了 <code>toc</code> 选项</td></tr><tr><td>mathjax</td><td><code>false</code></td><td>是否开启数学公式支持 ，本文章是否开启 <code>mathjax</code>，且需要在主题的 <code>_config.yml</code> 文件中也需要开启才行</td></tr><tr><td>summary</td><td>无</td><td>文章摘要，自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</td></tr><tr><td>categories</td><td>无</td><td>文章分类，本主题的分类表示宏观上大的分类，只建议一篇文章一个分类</td></tr><tr><td>tags</td><td>无</td><td>文章标签，一篇文章可以多个标签</td></tr></tbody></table><blockquote><p><strong>注意</strong>:</p><ol><li>如果 <code>img</code> 属性不填写的话，文章特色图会根据文章标题的 <code>hashcode</code> 的值取余，然后选取主题中对应的特色图片，从而达到让所有文章都的特色图<strong>各有特色</strong>。</li><li><code>date</code> 的值尽量保证每篇文章是唯一的，因为本主题中 <code>Gitalk</code> 和 <code>Gitment</code> 识别 <code>id</code> 是通过 <code>date</code> 的值来作为唯一标识的。</li><li>如果要对文章设置阅读验证密码的功能，不仅要在 Front-matter 中设置采用了 SHA256 加密的 password 的值，还需要在主题的 <code>_config.yml</code> 中激活了配置。有些在线的 SHA256 加密的地址，可供你使用：<a href="http://tool.oschina.net/encrypt?type=2" target="_blank" rel="noopener">开源中国在线工具</a>、<a href="http://encode.chahuo.com/" target="_blank" rel="noopener">chahuo</a>、<a href="http://tool.chinaz.com/tools/hash.aspx" target="_blank" rel="noopener">站长工具</a>。</li></ol></blockquote><p>以下为文章的 <code>Front-matter</code> 示例。</p><h3 id="最简示例"><a href="#最简示例" class="headerlink" title="最简示例"></a>最简示例</h3><pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> typora<span class="token punctuation">-</span>vue<span class="token punctuation">-</span>theme主题介绍<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-07 09:25:00</span><span class="token punctuation">---</span></code></pre><h3 id="最全示例"><a href="#最全示例" class="headerlink" title="最全示例"></a>最全示例</h3><pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> typora<span class="token punctuation">-</span>vue<span class="token punctuation">-</span>theme主题介绍<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-07 09:25:00</span><span class="token key atrule">author</span><span class="token punctuation">:</span> 赵奇<span class="token key atrule">img</span><span class="token punctuation">:</span> /source/images/xxx.jpg<span class="token key atrule">top</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span class="token key atrule">cover</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span class="token key atrule">coverImg</span><span class="token punctuation">:</span> /images/1.jpg<span class="token key atrule">password</span><span class="token punctuation">:</span> 8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92<span class="token key atrule">toc</span><span class="token punctuation">:</span> <span class="token boolean important">false</span><span class="token key atrule">mathjax</span><span class="token punctuation">:</span> <span class="token boolean important">false</span><span class="token key atrule">summary</span><span class="token punctuation">:</span> 这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要<span class="token key atrule">categories</span><span class="token punctuation">:</span> Markdown<span class="token key atrule">tags</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> Typora  <span class="token punctuation">-</span> Markdown<span class="token punctuation">---</span></code></pre><h2 id="效果截图"><a href="#效果截图" class="headerlink" title="效果截图"></a>效果截图</h2><p><img src="http://static.blinkfox.com/matery-20181202-1.png" alt="首页"></p><p><img src="http://static.blinkfox.com/matery-20181202-2.png" alt="首页推荐文章"></p><p><img src="http://static.blinkfox.com/matery-20181202-3.png" alt="首页文章列表"></p><p><img src="http://static.blinkfox.com/matery-20181202-7.png" alt="首页文章列表"></p><p><img src="http://static.blinkfox.com/matery-20181202-8.png" alt="首页文章列表"></p><h2 id="自定制修改"><a href="#自定制修改" class="headerlink" title="自定制修改"></a>自定制修改</h2><p>在本主题的 <code>_config.yml</code> 中可以修改部分自定义信息，有以下几个部分：</p><ul><li>菜单</li><li>我的梦想</li><li>首页的音乐播放器和视频播放器配置</li><li>是否显示推荐文章名称和按钮配置</li><li><code>favicon</code> 和 <code>Logo</code></li><li>个人信息</li><li>TOC 目录</li><li>文章打赏信息</li><li>复制文章内容时追加版权信息</li><li>MathJax</li><li>文章字数统计、阅读时长</li><li>点击页面的’爱心’效果</li><li>我的项目</li><li>我的技能</li><li>我的相册</li><li><code>Gitalk</code>、<code>Gitment</code>、<code>Valine</code> 和 <code>disqus</code> 评论配置</li><li><a href="http://busuanzi.ibruce.info/" target="_blank" rel="noopener">不蒜子统计</a>和谷歌分析（<code>Google Analytics</code>）</li><li>默认特色图的集合。当文章没有设置特色图时，本主题会根据文章标题的 <code>hashcode</code> 值取余，来选择展示对应的特色图</li></ul><p><strong>我认为个人博客应该都有自己的风格和特色</strong>。如果本主题中的诸多功能和主题色彩你不满意，可以在主题中自定义修改，很多更自由的功能和细节点的修改难以在主题的 <code>_config.yml</code> 中完成，需要修改源代码才来完成。以下列出了可能对你有用的地方：</p><h3 id="修改主题颜色"><a href="#修改主题颜色" class="headerlink" title="修改主题颜色"></a>修改主题颜色</h3><p>在主题文件的 <code>/source/css/matery.css</code> 文件中，搜索 <code>.bg-color</code> 来修改背景颜色：</p><pre class=" language-css"><code class="language-css"><span class="token comment" spellcheck="true">/* 整体背景颜色，包括导航、移动端的导航、页尾、标签页等的背景颜色. */</span><span class="token selector"><span class="token class">.bg-color</span> </span><span class="token punctuation">{</span>    <span class="token property">background-image</span><span class="token punctuation">:</span> <span class="token function">linear-gradient</span><span class="token punctuation">(</span>to right, <span class="token hexcode">#4cbf30</span> <span class="token number">0%</span>, <span class="token hexcode">#0f9d58</span> <span class="token number">100%</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token atrule"><span class="token rule">@-webkit-keyframes</span> rainbow</span> <span class="token punctuation">{</span>   <span class="token comment" spellcheck="true">/* 动态切换背景颜色. */</span><span class="token punctuation">}</span><span class="token atrule"><span class="token rule">@keyframes</span> rainbow</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">/* 动态切换背景颜色. */</span><span class="token punctuation">}</span></code></pre><h3 id="修改-banner-图和文章特色图"><a href="#修改-banner-图和文章特色图" class="headerlink" title="修改 banner 图和文章特色图"></a>修改 banner 图和文章特色图</h3><p>你可以直接在 <code>/source/medias/banner</code> 文件夹中更换你喜欢的 <code>banner</code> 图片，主题代码中是每天动态切换一张，只需 <code>7</code> 张即可。如果你会 <code>JavaScript</code> 代码，可以修改成你自己喜欢切换逻辑，如：随机切换等，<code>banner</code> 切换的代码位置在 <code>/layout/_partial/bg-cover-content.ejs</code> 文件的 <code>&lt;script&gt;&lt;/script&gt;</code> 代码中：</p><pre class=" language-javascript"><code class="language-javascript"><span class="token function">$</span><span class="token punctuation">(</span><span class="token string">'.bg-cover'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">css</span><span class="token punctuation">(</span><span class="token string">'background-image'</span><span class="token punctuation">,</span> <span class="token string">'url(/medias/banner/'</span> <span class="token operator">+</span> <span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getDay</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'.jpg)'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>在 <code>/source/medias/featureimages</code> 文件夹中默认有 24 张特色图片，你可以再增加或者减少，并需要在 <code>_config.yml</code> 做同步修改。</p><h2 id="版本记录"><a href="#版本记录" class="headerlink" title="版本记录"></a>版本记录</h2><ul><li>v1.0.0<ul><li>新增了所有基础功能；</li></ul></li><li>v1.0.1<ul><li>调整 <code>css</code>、<code>js</code> 的文件请求路径在主题的<code>_config.yml</code>中配置，便于你更快捷的配置自己的 CDN；</li><li>新增代码是否折行为可配置，默认为折行；</li><li>默认激活 <code>TOC</code> 功能，并新增为某篇文章关闭 <code>TOC</code> 的 <code>Front-matter</code> 配置选项；</li><li>修复文章滚动时，高亮的目录选项不准确的问题；</li><li><code>IOS</code>下移除搜索框自动获得焦点属性，防止自动获得焦点后导致视图上移；</li></ul></li><li>v1.0.2<ul><li>升级了 <a href="https://materializecss.com/" target="_blank" rel="noopener">Materialize</a> 框架版本为<code>1.0.0</code>，重构和修改了升级过程中的部分文件或问题；</li><li>新增了首页封面的全屏轮播特效，可以将更重要的文章设置到首页轮播中；</li><li>修复首页第一个按钮是中文的问题</li><li>修复了 iPhone 上点击搜索输入获取焦点的问题；</li><li>修复了 iPhone 上输入框获取焦点后页面放大的问题；</li><li>修复一些文章或 UI 显示问题；</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> hexo-theme-matery </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
